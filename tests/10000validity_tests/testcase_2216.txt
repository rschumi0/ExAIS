import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Mul5600 = tf.keras.layers.Input(shape=([2, 2, 2, 2]))
in1Mul5600 = tf.keras.layers.Input(shape=([2, 2, 2, 2]))
in0Sub25437 = tf.keras.layers.Input(shape=([2]))
in1Sub25437 = tf.keras.layers.Input(shape=([2]))
in0Con57015 = tf.keras.layers.Input(shape=([14]))

Mul5600 = keras.layers.Multiply(name = 'Mul5600', )([in0Mul5600,in1Mul5600])
Res24032 = keras.layers.Reshape((2, 2, 4), name = 'Res24032', )(Mul5600)
Res39018 = keras.layers.Reshape((2, 8), name = 'Res39018', )(Res24032)
Fla55885 = keras.layers.Flatten(name = 'Fla55885', )(Res39018)
Sub25437 = keras.layers.Subtract(name = 'Sub25437', )([in0Sub25437,in1Sub25437])
Con57015 = keras.layers.Concatenate(axis=1, name = 'Con57015', )([Sub25437,in0Con57015])
Add62168 = keras.layers.Add(name = 'Add62168', )([Fla55885,Con57015])
Lea86966 = keras.layers.LeakyReLU(alpha=7.4138908785430075, name = 'Lea86966', )(Add62168)
model = tf.keras.models.Model(inputs=[in0Mul5600,in1Mul5600,in0Sub25437,in1Sub25437,in0Con57015], outputs=Lea86966)
in0Mul5600 = tf.constant([[[[[0.5528, 0.468], [0.8701, 0.0398]], [[0.6592, 0.4675], [0.3843, 0.0499]]], [[[0.8851, 0.4522], [0.9133, 0.14]], [[0.406, 0.6666], [0.6433, 0.1924]]]]])
in1Mul5600 = tf.constant([[[[[0.0401, 0.6125], [0.8685, 0.841]], [[0.4423, 0.4588], [0.3877, 0.9509]]], [[[0.4731, 0.8432], [0.9148, 0.9983]], [[0.1527, 0.2382], [0.3053, 0.042]]]]])
in0Sub25437 = tf.constant([[0.0616, 0.8228]])
in1Sub25437 = tf.constant([[0.8059, 0.8314]])
in0Con57015 = tf.constant([[0.1709, 0.6875, 0.1012, 0.0339, 0.0513, 0.7191, 0.3173, 0.7809, 0.9644, 0.5941, 0.6602, 0.7023, 0.8021, 0.1389]])
print (np.array2string(model.predict([in0Mul5600,in1Mul5600,in0Sub25437,in1Sub25437,in0Con57015],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Lea86966.png')

LMul5600 = multiply_layer([[[[[[0.5528, 0.468], [0.8701, 0.0398]], [[0.6592, 0.4675], [0.3843, 0.0499]]], [[[0.8851, 0.4522], [0.9133, 0.14]], [[0.406, 0.6666], [0.6433, 0.1924]]]]], [[[[[0.0401, 0.6125], [0.8685, 0.841]], [[0.4423, 0.4588], [0.3877, 0.9509]]], [[[0.4731, 0.8432], [0.9148, 0.9983]], [[0.1527, 0.2382], [0.3053, 0.042]]]]]], Mul5600), 
LRes24032 = reshape_layer(Mul5600, [2, 2, 4], Res24032), 
LRes39018 = reshape_layer(Res24032, [2, 8], Res39018), 
LFla55885 = flatten_layer(Res39018, Fla55885), 
LSub25437 = subtract_layer([[0.0616, 0.8228]], [[0.8059, 0.8314]], Sub25437), 
LCon57015 = concatenate_layer([Sub25437,[[0.1709, 0.6875, 0.1012, 0.0339, 0.0513, 0.7191, 0.3173, 0.7809, 0.9644, 0.5941, 0.6602, 0.7023, 0.8021, 0.1389]]], 1, Con57015), 
LAdd62168 = add_layer([Fla55885,Con57015], Add62168), 
LLea86966 = leaky_relu_layer(Add62168, 7.4138908785430075, Lea86966), 
exec_layers([LMul5600,LRes24032,LRes39018,LFla55885,LSub25437,LCon57015,LAdd62168,LLea86966],["Mul5600","Res24032","Res39018","Fla55885","Sub25437","Con57015","Add62168","Lea86966"],Lea86966,"Lea86966")

Actual (Unparsed): [[-5.3538130, 0.2780500, 0.9265819, 0.7209718, 0.3927642, 0.2483890, 0.2002931, 0.7665499, 0.7360408, 1.1621951, 1.7998868, 0.7338620, 0.7221962, 0.8610841, 0.9984995, 0.1469808]]

Expected (Unparsed): [[-5.353813185905452,0.27804999999999996,0.92658185,0.7209718,0.39276416000000003,0.24838900000000003,0.20029311,0.76654991,0.73604081,1.16219504,1.79988684,0.733862,0.7221962000000001,0.86108412,0.99849949,0.1469808]]

Actual:   [[-5.3538, 0.2781, 0.9266, 0.721, 0.3928, 0.2484, 0.2003, 0.7666, 0.7361, 1.1622, 1.7999, 0.7339, 0.7222, 0.8611, 0.9985, 0.147]]

Expected: [[-5.3538, 0.2781, 0.9266, 0.721, 0.3928, 0.2484, 0.2003, 0.7666, 0.7361, 1.1622, 1.7999, 0.7339, 0.7222, 0.8611, 0.9985, 0.147]]