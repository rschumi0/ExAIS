import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Max7486 = tf.keras.layers.Input(shape=([1, 1, 1, 2]))
in1Max7486 = tf.keras.layers.Input(shape=([1, 1, 1, 2]))
in0Con29778 = tf.keras.layers.Input(shape=([1]))
in0GRU39463 = tf.keras.layers.Input(shape=([1, 2]))

Max7486 = keras.layers.Maximum(name = 'Max7486', )([in0Max7486,in1Max7486])
Res2574 = keras.layers.Reshape((1, 1, 2), name = 'Res2574', )(Max7486)
Res6140 = keras.layers.Reshape((1, 2), name = 'Res6140', )(Res2574)
Fla39608 = keras.layers.Flatten(name = 'Fla39608', )(Res6140)
Con29778 = keras.layers.Concatenate(axis=1, name = 'Con29778', )([Fla39608,in0Con29778])
GRU39463 = keras.layers.GRU(3,reset_after=False, recurrent_activation='sigmoid', name = 'GRU39463', )(in0GRU39463)
Add69078 = keras.layers.Add(name = 'Add69078', )([Con29778,GRU39463])
ReL3629 = keras.layers.ReLU(max_value=6.604664089898668, negative_slope=7.174994174827523, threshold=0.962621159502054, name = 'ReL3629', )(Add69078)
Bat72861 = keras.layers.BatchNormalization(axis=1, epsilon=0.37060191404167186,  name = 'Bat72861', )(ReL3629)
model = tf.keras.models.Model(inputs=[in0Max7486,in1Max7486,in0Con29778,in0GRU39463], outputs=Bat72861)
w = model.get_layer('GRU39463').get_weights() 
w[0] = np.array([[9, 9, 2, 9, 10, 6, 7, 9, 10], [6, 9, 8, 9, 1, 6, 8, 2, 2]])
w[1] = np.array([[1, 6, 1, 3, 2, 6, 4, 8, 3], [8, 3, 3, 2, 7, 3, 3, 4, 1], [5, 7, 5, 2, 6, 7, 8, 9, 1]])
w[2] = np.array([6, 8, 9, 1, 3, 3, 3, 1, 2])
model.get_layer('GRU39463').set_weights(w) 
w = model.get_layer('Bat72861').get_weights() 
w[0] = np.array([0.2894, 0.4892, 0.3812])
w[1] = np.array([0.1705, 0.3326, 0.627])
w[2] = np.array([0.9355, 0.6702, 0.5251])
w[3] = np.array([0.6593, 0.9707, 0.5772])
model.get_layer('Bat72861').set_weights(w) 
in0Max7486 = tf.constant([[[[[0.8162, 0.721]]]]])
in1Max7486 = tf.constant([[[[[0.1665, 0.73]]]]])
in0Con29778 = tf.constant([[0.5803]])
in0GRU39463 = tf.constant([[[5, 8]]])
print (np.array2string(model.predict([in0Max7486,in1Max7486,in0Con29778,in0GRU39463],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Bat72861.png')

LMax7486 = maximum_layer([[[[[[0.8162, 0.721]]]]], [[[[[0.1665, 0.73]]]]]], Max7486), 
LRes2574 = reshape_layer(Max7486, [1, 1, 2], Res2574), 
LRes6140 = reshape_layer(Res2574, [1, 2], Res6140), 
LFla39608 = flatten_layer(Res6140, Fla39608), 
LCon29778 = concatenate_layer([Fla39608,[[0.5803]]], 1, Con29778), 
LGRU39463 = gru_layer([[[5, 8]]],[[9, 9, 2, 9, 10, 6, 7, 9, 10], [6, 9, 8, 9, 1, 6, 8, 2, 2]],[[1, 6, 1, 3, 2, 6, 4, 8, 3], [8, 3, 3, 2, 7, 3, 3, 4, 1], [5, 7, 5, 2, 6, 7, 8, 9, 1]],[6, 8, 9, 1, 3, 3, 3, 1, 2], false, GRU39463), 
LAdd69078 = add_layer([Con29778,GRU39463], Add69078), 
LReL3629 = relu_layer(Add69078, 6.604664089898668, 7.174994174827523, 0.962621159502054, ReL3629), 
LBat72861 = batch_normalization_layer(ReL3629, 1, 0.37060191404167186, [0.2894, 0.4892, 0.3812], [0.1705, 0.3326, 0.627], [0.9355, 0.6702, 0.5251], [0.6593, 0.9707, 0.5772], Bat72861), 
exec_layers([LMax7486,LRes2574,LRes6140,LFla39608,LCon29778,LGRU39463,LAdd69078,LReL3629,LBat72861],["Max7486","Res2574","Res6140","Fla39608","Con29778","GRU39463","Add69078","ReL3629","Bat72861"],Bat72861,"Bat72861")

Actual (Unparsed): [[-0.3958636, -0.6554998, -0.6527044]]

Expected (Unparsed): [[-0.3958636302872647,-0.6554998396980128,-0.6527043586591015]]

Actual:   [[-0.3958, -0.6554, -0.6527]]

Expected: [[-0.3958, -0.6554, -0.6527]]