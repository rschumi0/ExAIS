import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Sub82789 = tf.keras.layers.Input(shape=([2, 2]))
in1Sub82789 = tf.keras.layers.Input(shape=([2, 2]))
in0Max42619 = tf.keras.layers.Input(shape=([1, 1, 2]))
in1Max42619 = tf.keras.layers.Input(shape=([1, 1, 2]))
in0Con92325 = tf.keras.layers.Input(shape=([2]))

Sub82789 = keras.layers.Subtract(name = 'Sub82789', )([in0Sub82789,in1Sub82789])
ReL20335 = keras.layers.ReLU(max_value=4.088685958224706, negative_slope=5.6495794933473436, threshold=5.7292165134358966, name = 'ReL20335', )(Sub82789)
Lea28456 = keras.layers.LeakyReLU(alpha=9.560089509602928, name = 'Lea28456', )(ReL20335)
Fla94494 = keras.layers.Flatten(name = 'Fla94494', )(Lea28456)
Max42619 = keras.layers.Maximum(name = 'Max42619', )([in0Max42619,in1Max42619])
Res35440 = keras.layers.Reshape((1, 2), name = 'Res35440', )(Max42619)
Sim99258 = keras.layers.SimpleRNN(2,name = 'Sim99258', )(Res35440)
Con92325 = keras.layers.Concatenate(axis=1, name = 'Con92325', )([Sim99258,in0Con92325])
Add89712 = keras.layers.Add(name = 'Add89712', )([Fla94494,Con92325])
model = tf.keras.models.Model(inputs=[in0Sub82789,in1Sub82789,in0Max42619,in1Max42619,in0Con92325], outputs=Add89712)
w = model.get_layer('Sim99258').get_weights() 
w[0] = np.array([[10, 5], [7, 6]])
w[1] = np.array([[7, 4], [9, 2]])
w[2] = np.array([9, 3])
model.get_layer('Sim99258').set_weights(w) 
in0Sub82789 = tf.constant([[[0.443, 0.4349], [0.4834, 0.0551]]])
in1Sub82789 = tf.constant([[[0.7261, 0.63], [0.3359, 0.9906]]])
in0Max42619 = tf.constant([[[[0.6919, 0.0485]]]])
in1Max42619 = tf.constant([[[[0.7731, 0.3225]]]])
in0Con92325 = tf.constant([[0.8255, 0.9732]])
print (np.array2string(model.predict([in0Sub82789,in1Sub82789,in0Max42619,in1Max42619,in0Con92325],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Add89712.png')

LSub82789 = subtract_layer([[[0.443, 0.4349], [0.4834, 0.0551]]], [[[0.7261, 0.63], [0.3359, 0.9906]]], Sub82789), 
LReL20335 = relu_layer(Sub82789, 4.088685958224706, 5.6495794933473436, 5.7292165134358966, ReL20335), 
LLea28456 = leaky_relu_layer(ReL20335, 9.560089509602928, Lea28456), 
LFla94494 = flatten_layer(Lea28456, Fla94494), 
LMax42619 = maximum_layer([[[[[0.6919, 0.0485]]]], [[[[0.7731, 0.3225]]]]], Max42619), 
LRes35440 = reshape_layer(Max42619, [1, 2], Res35440), 
LSim99258 = simple_rnn_layer(Res35440,[[10, 5], [7, 6]],[[7, 4], [9, 2]],[9, 3], Sim99258), 
LCon92325 = concatenate_layer([Sim99258,[[0.8255, 0.9732]]], 1, Con92325), 
LAdd89712 = add_layer([Fla94494,Con92325], Add89712), 
exec_layers([LSub82789,LReL20335,LLea28456,LFla94494,LMax42619,LRes35440,LSim99258,LCon92325,LAdd89712],["Sub82789","ReL20335","Lea28456","Fla94494","Max42619","Res35440","Sim99258","Con92325","Add89712"],Add89712,"Add89712")

Actual (Unparsed): [[-323.7281233, -318.9751992, -300.6457082, -358.9913600]]

Expected (Unparsed): [[-323.7281347602697,-318.9752120686397,-300.64571964023327,-358.9913755970365]]

Actual:   [[-323.7281, -318.9751, -300.6457, -358.9913]]

Expected: [[-323.7281, -318.9752, -300.6457, -358.9913]]