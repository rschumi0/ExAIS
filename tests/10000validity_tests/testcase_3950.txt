import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Sub9661 = tf.keras.layers.Input(shape=([2, 3, 3, 3]))
in1Sub9661 = tf.keras.layers.Input(shape=([2, 3, 3, 3]))

Sub9661 = keras.layers.Subtract(name = 'Sub9661', )([in0Sub9661,in1Sub9661])
Con47293 = keras.layers.Conv3DTranspose(2, (1, 3, 3),strides=(1, 2, 1), padding='valid', name = 'Con47293', )(Sub9661)
Fla71259 = keras.layers.Flatten(name = 'Fla71259', )(Con47293)
model = tf.keras.models.Model(inputs=[in0Sub9661,in1Sub9661], outputs=Fla71259)
w = model.get_layer('Con47293').get_weights() 
w[0] = np.array([[[[[0.8786, 0.8606, 0.7939], [0.0839, 0.5005, 0.248]], [[0.6657, 0.8818, 0.4819], [0.3447, 0.6954, 0.1106]], [[0.7658, 0.1476, 0.8257], [0.5969, 0.2696, 0.0261]]], [[[0.5215, 0.4663, 0.4089], [0.6003, 0.5292, 0.6492]], [[0.5428, 0.2348, 0.7974], [0.2251, 0.4886, 0.9834]], [[0.4367, 0.6467, 0.8187], [0.8962, 0.9956, 0.9124]]], [[[0.1262, 0.83, 0.1251], [0.4418, 0.8267, 0.2933]], [[0.9192, 0.75, 0.3487], [0.3988, 0.8279, 0.0104]], [[0.7583, 0.6985, 0.0618], [0.2538, 0.9614, 0.5304]]]]])
w[1] = np.array([0, 0])
model.get_layer('Con47293').set_weights(w) 
in0Sub9661 = tf.constant([[[[[0.7928, 0.369, 0.5206], [0.7389, 0.591, 0.5811], [0.6123, 0.6011, 0.9476]], [[0.3388, 0.4435, 0.1312], [0.4518, 0.0023, 0.0409], [0.7249, 0.6577, 0.4124]], [[0.0746, 0.922, 0.6857], [0.4348, 0.4349, 0.7051], [0.3501, 0.2802, 0.6831]]], [[[0.1435, 0.5293, 0.6422], [0.92, 0.6774, 0.2842], [0.1517, 0.7986, 0.8119]], [[0.0534, 0.3813, 0.1181], [0.2391, 0.1048, 0.6485], [0.6796, 0.4401, 0.3081]], [[0.1663, 0.8601, 0.6298], [0.2059, 0.6673, 0.663], [0.5059, 0.3113, 0.4884]]]]])
in1Sub9661 = tf.constant([[[[[0.8452, 0.6351, 0.5319], [0.2458, 0.086, 0.4711], [0.6343, 0.6502, 0.0615]], [[0.9337, 0.6988, 0.7188], [0.7232, 0.2005, 0.5889], [0.6258, 0.036, 0.8624]], [[0.5603, 0.6973, 0.9285], [0.8776, 0.2688, 0.607], [0.2298, 0.8199, 0.6011]]], [[[0.6582, 0.9245, 0.7654], [0.9506, 0.1444, 0.1439], [0.7836, 0.7839, 0.178]], [[0.9296, 0.4099, 0.6077], [0.0446, 0.9542, 0.6616], [0.7306, 0.0965, 0.0665]], [[0.133, 0.5403, 0.4346], [0.7394, 0.4515, 0.3653], [0.1447, 0.24, 0.9138]]]]])
print (np.array2string(model.predict([in0Sub9661,in1Sub9661],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Fla71259.png')

LSub9661 = subtract_layer([[[[[0.7928, 0.369, 0.5206], [0.7389, 0.591, 0.5811], [0.6123, 0.6011, 0.9476]], [[0.3388, 0.4435, 0.1312], [0.4518, 0.0023, 0.0409], [0.7249, 0.6577, 0.4124]], [[0.0746, 0.922, 0.6857], [0.4348, 0.4349, 0.7051], [0.3501, 0.2802, 0.6831]]], [[[0.1435, 0.5293, 0.6422], [0.92, 0.6774, 0.2842], [0.1517, 0.7986, 0.8119]], [[0.0534, 0.3813, 0.1181], [0.2391, 0.1048, 0.6485], [0.6796, 0.4401, 0.3081]], [[0.1663, 0.8601, 0.6298], [0.2059, 0.6673, 0.663], [0.5059, 0.3113, 0.4884]]]]], [[[[[0.8452, 0.6351, 0.5319], [0.2458, 0.086, 0.4711], [0.6343, 0.6502, 0.0615]], [[0.9337, 0.6988, 0.7188], [0.7232, 0.2005, 0.5889], [0.6258, 0.036, 0.8624]], [[0.5603, 0.6973, 0.9285], [0.8776, 0.2688, 0.607], [0.2298, 0.8199, 0.6011]]], [[[0.6582, 0.9245, 0.7654], [0.9506, 0.1444, 0.1439], [0.7836, 0.7839, 0.178]], [[0.9296, 0.4099, 0.6077], [0.0446, 0.9542, 0.6616], [0.7306, 0.0965, 0.0665]], [[0.133, 0.5403, 0.4346], [0.7394, 0.4515, 0.3653], [0.1447, 0.24, 0.9138]]]]], Sub9661), 
LCon47293 = conv3D_transpose_layer(Sub9661, 1, 3, 3,[[[[[0.8786, 0.8606, 0.7939], [0.0839, 0.5005, 0.248]], [[0.6657, 0.8818, 0.4819], [0.3447, 0.6954, 0.1106]], [[0.7658, 0.1476, 0.8257], [0.5969, 0.2696, 0.0261]]], [[[0.5215, 0.4663, 0.4089], [0.6003, 0.5292, 0.6492]], [[0.5428, 0.2348, 0.7974], [0.2251, 0.4886, 0.9834]], [[0.4367, 0.6467, 0.8187], [0.8962, 0.9956, 0.9124]]], [[[0.1262, 0.83, 0.1251], [0.4418, 0.8267, 0.2933]], [[0.9192, 0.75, 0.3487], [0.3988, 0.8279, 0.0104]], [[0.7583, 0.6985, 0.0618], [0.2538, 0.9614, 0.5304]]]]],[0, 0], 1, 2, 1, false, Con47293), 
LFla71259 = flatten_layer(Con47293, Fla71259), 
exec_layers([LSub9661,LCon47293,LFla71259],["Sub9661","Con47293","Fla71259"],Fla71259,"Fla71259")

Actual (Unparsed): [[-0.2840154, -0.1403818, 0.6801945, 0.1170456, 1.3797302, 0.6233340, 0.9120508, 0.4896255, 0.7075580, -0.0032420, -0.1560296, -0.1796118, 0.4376786, 0.4817418, 0.5976794, 0.6797795, 1.3150831, 1.8875065, 0.6840897, 0.7398773, -1.4377755, -0.5698641, -1.5049343, -0.2791805, -0.6217316, 0.0269688, 0.6931488, 0.8157755, -0.2001293, 0.6322172, -0.6695564, -0.8736932, -1.3094414, -1.4600679, -1.3792176, -1.9239349, -0.8544073, -1.0570118, 0.0769154, 0.2971979, -0.7866023, -0.6347287, -1.6209269, -0.8669121, -1.7167039, -1.0620010, -0.5675457, -0.5422806, 0.5617684, 0.3126181, -0.2477959, -0.3302802, -0.5178418, -0.3525384, -0.5440946, -0.5153088, -0.0016754, -0.2979409, -0.2293556, -0.3546957, 0.0948314, -0.1000360, -0.2683409, -0.0397346, -0.8971086, -0.4430503, -0.4792936, -0.2986510, -0.2806893, -0.4448426, -0.8901331, -0.2715345, -0.2072945, -0.1668716, -0.0762588, 0.0701798, 0.0688661, -0.0083907, 0.0416720, -0.3566732, -0.5030743, -0.5980957, -0.1804632, -0.0753314, -0.4242748, -0.5356586, 0.6121198, 1.1195598, 0.2525297, 0.0266969, -1.5915194, -0.7994892, -1.7713424, -0.8535870, -1.5042702, -1.0674536, 0.4072134, 0.4806736, -0.2185781, 0.2584754, -0.6704719, -0.8589663, -1.1727237, -1.0339279, -0.6738637, -1.3364885, -0.2294475, -0.2893119, 0.3977323, 0.5168178, 0.2638831, -0.3430851, -1.3281088, -0.6057761, -0.6150450, -0.7321876, -0.1841278, -0.6330151, 0.1521403, 0.6692577, 0.2463060, 0.3159520, 0.1929536, 0.3429179, 0.4273059, 0.7828347, 0.0238920, -0.2938458, -0.1444292, 0.0065588, 0.2940559, 0.3363427, 0.4875541, 0.3100894, 0.0875047, 0.4821905, 0.0017342, 0.4286196, 0.2974113, -0.0654118]]

Expected (Unparsed): [[-0.2840153700000001,-0.14038181,0.68019453,0.11704558999999996,1.3797301099999997,0.62333397,0.9120507899999999,0.48962550999999993,0.70755801,-0.0032419500000000177,-0.15602960000000005,-0.17961180000000004,0.4376785299999999,0.4817418099999998,0.5976793799999999,0.6797794499999998,1.31508313,1.8875065000000002,0.6840896999999999,0.73987728,-1.43777547,-0.56986404,-1.5049342399999999,-0.27918051999999993,-0.6217315600000003,0.026968789999999923,0.69314879,0.8157755000000001,-0.20012927000000008,0.6322172099999999,-0.66955638,-0.87369315,-1.3094413599999999,-1.4600678699999998,-1.3792175800000002,-1.92393479,-0.85440728,-1.0570117700000001,0.07691535999999988,0.29719793999999977,-0.7866022600000001,-0.63472869,-1.62092686,-0.8669120499999999,-1.7167037900000002,-1.0620009400000001,-0.5675456600000002,-0.54228061,0.5617684,0.31261810999999995,-0.24779585999999998,-0.33028022999999995,-0.5178418,-0.3525383700000001,-0.54409454,-0.51530877,-0.001675339999999928,-0.2979408499999999,-0.22935557999999995,-0.3546956599999999,0.09483137999999999,-0.10003601000000004,-0.26834084999999996,-0.03973459000000002,-0.89710863,-0.4430502599999999,-0.47929364999999996,-0.29865105000000003,-0.28068935999999994,-0.44484263999999996,-0.89013302,-0.27153453,-0.20729441999999992,-0.16687153000000005,-0.07625877999999997,0.07017983000000001,0.06886607000000008,-0.00839071999999999,0.04167192999999991,-0.35667319999999997,-0.5030742899999999,-0.59809569,-0.18046312999999994,-0.07533138999999993,-0.4242747599999998,-0.5356586299999999,0.6121197899999999,1.11955979,0.25252969,0.0266968999999998,-1.5915193799999998,-0.7994891399999999,-1.7713423600000002,-0.8535870000000002,-1.5042701699999999,-1.0674536100000007,0.40721332000000005,0.4806736100000001,-0.21857812000000001,0.2584753399999999,-0.67047192,-0.8589663000000001,-1.1727237400000001,-1.03392787,-0.67386372,-1.33648853,-0.22944748000000012,-0.28931188,0.39773234,0.5168178,0.26388314,-0.3430850900000001,-1.3281087299999998,-0.60577605,-0.6150450299999997,-0.7321875500000001,-0.18412782000000008,-0.6330151000000002,0.15214024000000015,0.6692577000000001,0.24630596999999999,0.31595199,0.19295358000000007,0.3429179400000001,0.42730596000000015,0.7828346700000004,0.02389204000000006,-0.2938457999999998,-0.14442922999999994,0.006558760000000108,0.29405598,0.33634275999999996,0.48755417,0.31008950999999996,0.08750474000000005,0.48219048999999997,0.0017341700000000904,0.42861957,0.29741129,-0.06541177999999992]]

Actual:   [[-0.284, -0.1403, 0.6802, 0.1171, 1.3798, 0.6234, 0.9121, 0.4897, 0.7076, -0.0032, -0.156, -0.1796, 0.4377, 0.4818, 0.5977, 0.6798, 1.3151, 1.8876, 0.6841, 0.7399, -1.4377, -0.5698, -1.5049, -0.2791, -0.6217, 0.027, 0.6932, 0.8158, -0.2001, 0.6323, -0.6695, -0.8736, -1.3094, -1.46, -1.3792, -1.9239, -0.8544, -1.057, 0.077, 0.2972, -0.7866, -0.6347, -1.6209, -0.8669, -1.7167, -1.062, -0.5675, -0.5422, 0.5618, 0.3127, -0.2477, -0.3302, -0.5178, -0.3525, -0.544, -0.5153, -0.0016, -0.2979, -0.2293, -0.3546, 0.0949, -0.1, -0.2683, -0.0397, -0.8971, -0.443, -0.4792, -0.2986, -0.2806, -0.4448, -0.8901, -0.2715, -0.2072, -0.1668, -0.0762, 0.0702, 0.0689, -0.0083, 0.0417, -0.3566, -0.503, -0.598, -0.1804, -0.0753, -0.4242, -0.5356, 0.6122, 1.1196, 0.2526, 0.0267, -1.5915, -0.7994, -1.7713, -0.8535, -1.5042, -1.0674, 0.4073, 0.4807, -0.2185, 0.2585, -0.6704, -0.8589, -1.1727, -1.0339, -0.6738, -1.3364, -0.2294, -0.2893, 0.3978, 0.5169, 0.2639, -0.343, -1.3281, -0.6057, -0.615, -0.7321, -0.1841, -0.633, 0.1522, 0.6693, 0.2464, 0.316, 0.193, 0.343, 0.4274, 0.7829, 0.0239, -0.2938, -0.1444, 0.0066, 0.2941, 0.3364, 0.4876, 0.3101, 0.0876, 0.4822, 0.0018, 0.4287, 0.2975, -0.0654]]

Expected: [[-0.284, -0.1403, 0.6802, 0.1171, 1.3798, 0.6234, 0.9121, 0.4897, 0.7076, -0.0032, -0.156, -0.1796, 0.4377, 0.4818, 0.5977, 0.6798, 1.3151, 1.8876, 0.6841, 0.7399, -1.4377, -0.5698, -1.5049, -0.2791, -0.6217, 0.027, 0.6932, 0.8158, -0.2001, 0.6323, -0.6695, -0.8736, -1.3094, -1.46, -1.3792, -1.9239, -0.8544, -1.057, 0.077, 0.2972, -0.7866, -0.6347, -1.6209, -0.8669, -1.7167, -1.062, -0.5675, -0.5422, 0.5618, 0.3127, -0.2477, -0.3302, -0.5178, -0.3525, -0.544, -0.5153, -0.0016, -0.2979, -0.2293, -0.3546, 0.0949, -0.1, -0.2683, -0.0397, -0.8971, -0.443, -0.4792, -0.2986, -0.2806, -0.4448, -0.8901, -0.2715, -0.2072, -0.1668, -0.0762, 0.0702, 0.0689, -0.0083, 0.0417, -0.3566, -0.503, -0.598, -0.1804, -0.0753, -0.4242, -0.5356, 0.6122, 1.1196, 0.2526, 0.0267, -1.5915, -0.7994, -1.7713, -0.8535, -1.5042, -1.0674, 0.4073, 0.4807, -0.2185, 0.2585, -0.6704, -0.8589, -1.1727, -1.0339, -0.6738, -1.3364, -0.2294, -0.2893, 0.3978, 0.5169, 0.2639, -0.343, -1.3281, -0.6057, -0.615, -0.7321, -0.1841, -0.633, 0.1522, 0.6693, 0.2464, 0.316, 0.193, 0.343, 0.4274, 0.7829, 0.0239, -0.2938, -0.1444, 0.0066, 0.2941, 0.3364, 0.4876, 0.3101, 0.0876, 0.4822, 0.0018, 0.4287, 0.2975, -0.0654]]