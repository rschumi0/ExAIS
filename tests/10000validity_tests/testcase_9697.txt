import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Max70618 = tf.keras.layers.Input(shape=([2, 1, 1, 2]))
in1Max70618 = tf.keras.layers.Input(shape=([2, 1, 1, 2]))
in0Sub87827 = tf.keras.layers.Input(shape=([2]))
in1Sub87827 = tf.keras.layers.Input(shape=([2]))
in0Con68079 = tf.keras.layers.Input(shape=([130]))

Max70618 = keras.layers.Maximum(name = 'Max70618', )([in0Max70618,in1Max70618])
Res2986 = keras.layers.Reshape((2, 1, 2), name = 'Res2986', )(Max70618)
Con93827 = keras.layers.Conv2DTranspose(3, (2, 1),strides=(2, 11), padding='valid', name = 'Con93827', )(Res2986)
Thr31612 = keras.layers.ThresholdedReLU(theta=8.60323069102734, name = 'Thr31612', )(Con93827)
Res63144 = keras.layers.Reshape((4, 33), name = 'Res63144', )(Thr31612)
Fla90872 = keras.layers.Flatten(name = 'Fla90872', )(Res63144)
Sub87827 = keras.layers.Subtract(name = 'Sub87827', )([in0Sub87827,in1Sub87827])
Con68079 = keras.layers.Concatenate(axis=1, name = 'Con68079', )([Sub87827,in0Con68079])
Sub45109 = keras.layers.Subtract(name = 'Sub45109', )([Fla90872,Con68079])
model = tf.keras.models.Model(inputs=[in0Max70618,in1Max70618,in0Sub87827,in1Sub87827,in0Con68079], outputs=Sub45109)
w = model.get_layer('Con93827').get_weights() 
w[0] = np.array([[[[0.4104, 0.9534], [0.3958, 0.8181], [0.9335, 0.7578]]], [[[0.3335, 0.4417], [0.541, 0.1994], [0.4557, 0.3191]]]])
w[1] = np.array([0, 0, 0])
model.get_layer('Con93827').set_weights(w) 
in0Max70618 = tf.constant([[[[[0.4908, 0.9912]]], [[[0.8983, 0.6054]]]]])
in1Max70618 = tf.constant([[[[[0.3655, 0.4152]]], [[[0.5071, 0.4887]]]]])
in0Sub87827 = tf.constant([[0.2176, 0.395]])
in1Sub87827 = tf.constant([[0.0369, 0.6425]])
in0Con68079 = tf.constant([[0.8204, 0.8177, 0.7212, 0.9659, 0.5374, 0.7389, 0.0742, 0.7497, 0.7505, 0.3433, 0.5625, 0.1271, 0.8548, 0.5483, 0.8407, 0.774, 0.9608, 0.1046, 0.7336, 0.1288, 0.7339, 0.004, 0.4334, 0.6271, 0.5037, 0.1633, 0.6751, 0.8369, 0.6738, 0.0726, 0.1347, 0.834, 0.6787, 0.2572, 0.7153, 0.6523, 0.609, 0.4692, 0.9586, 0.535, 0.1498, 0.4994, 0.5534, 0.0162, 0.6675, 0.3786, 0.7978, 0.5703, 0.1149, 0.5252, 0.9184, 0.6804, 0.9278, 0.0868, 0.3476, 0.6981, 0.2336, 0.3236, 0.4192, 0.7925, 0.5071, 0.3847, 0.4513, 0.6217, 0.2708, 0.2417, 0.8592, 0.0959, 0.5619, 0.8582, 0.7621, 0.7506, 0.6632, 0.9011, 0.5317, 0.7964, 0.0341, 0.4271, 0.8659, 0.8729, 0.4645, 0.0134, 0.2681, 0.8965, 0.2234, 0.7473, 0.2588, 0.1635, 0.3786, 0.3339, 0.2567, 0.0466, 0.5194, 0.3386, 0.8251, 0.2043, 0.8359, 0.2421, 0.9028, 0.2155, 0.8741, 0.5514, 0.7091, 0.9423, 0.3202, 0.6859, 0.2488, 0.7304, 0.8478, 0.736, 0.7262, 0.7613, 0.4042, 0.4791, 0.7246, 0.4864, 0.3103, 0.2282, 0.7069, 0.0929, 0.1988, 0.1903, 0.247, 0.3782, 0.1285, 0.0129, 0.4195, 0.2431, 0.1753, 0.1107]])
print (np.array2string(model.predict([in0Max70618,in1Max70618,in0Sub87827,in1Sub87827,in0Con68079],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Sub45109.png')

LMax70618 = maximum_layer([[[[[[0.4908, 0.9912]]], [[[0.8983, 0.6054]]]]], [[[[[0.3655, 0.4152]]], [[[0.5071, 0.4887]]]]]], Max70618), 
LRes2986 = reshape_layer(Max70618, [2, 1, 2], Res2986), 
LCon93827 = conv2D_transpose_layer(Res2986, 2, 1,[[[[0.4104, 0.9534], [0.3958, 0.8181], [0.9335, 0.7578]]], [[[0.3335, 0.4417], [0.541, 0.1994], [0.4557, 0.3191]]]],[0, 0, 0], 2, 11, false, Con93827), 
LThr31612 = thresholded_relu_layer(Con93827, 8.60323069102734, Thr31612), 
LRes63144 = reshape_layer(Thr31612, [4, 33], Res63144), 
LFla90872 = flatten_layer(Res63144, Fla90872), 
LSub87827 = subtract_layer([[0.2176, 0.395]], [[0.0369, 0.6425]], Sub87827), 
LCon68079 = concatenate_layer([Sub87827,[[0.8204, 0.8177, 0.7212, 0.9659, 0.5374, 0.7389, 0.0742, 0.7497, 0.7505, 0.3433, 0.5625, 0.1271, 0.8548, 0.5483, 0.8407, 0.774, 0.9608, 0.1046, 0.7336, 0.1288, 0.7339, 0.004, 0.4334, 0.6271, 0.5037, 0.1633, 0.6751, 0.8369, 0.6738, 0.0726, 0.1347, 0.834, 0.6787, 0.2572, 0.7153, 0.6523, 0.609, 0.4692, 0.9586, 0.535, 0.1498, 0.4994, 0.5534, 0.0162, 0.6675, 0.3786, 0.7978, 0.5703, 0.1149, 0.5252, 0.9184, 0.6804, 0.9278, 0.0868, 0.3476, 0.6981, 0.2336, 0.3236, 0.4192, 0.7925, 0.5071, 0.3847, 0.4513, 0.6217, 0.2708, 0.2417, 0.8592, 0.0959, 0.5619, 0.8582, 0.7621, 0.7506, 0.6632, 0.9011, 0.5317, 0.7964, 0.0341, 0.4271, 0.8659, 0.8729, 0.4645, 0.0134, 0.2681, 0.8965, 0.2234, 0.7473, 0.2588, 0.1635, 0.3786, 0.3339, 0.2567, 0.0466, 0.5194, 0.3386, 0.8251, 0.2043, 0.8359, 0.2421, 0.9028, 0.2155, 0.8741, 0.5514, 0.7091, 0.9423, 0.3202, 0.6859, 0.2488, 0.7304, 0.8478, 0.736, 0.7262, 0.7613, 0.4042, 0.4791, 0.7246, 0.4864, 0.3103, 0.2282, 0.7069, 0.0929, 0.1988, 0.1903, 0.247, 0.3782, 0.1285, 0.0129, 0.4195, 0.2431, 0.1753, 0.1107]]], 1, Con68079), 
LSub45109 = subtract_layer(Fla90872,Con68079, Sub45109), 
exec_layers([LMax70618,LRes2986,LCon93827,LThr31612,LRes63144,LFla90872,LSub87827,LCon68079,LSub45109],["Max70618","Res2986","Con93827","Thr31612","Res63144","Fla90872","Sub87827","Con68079","Sub45109"],Sub45109,"Sub45109")

Actual (Unparsed): [[-0.1807000, 0.2475000, -0.8204000, -0.8177000, -0.7212000, -0.9659000, -0.5374000, -0.7389000, -0.0742000, -0.7497000, -0.7505000, -0.3433000, -0.5625000, -0.1271000, -0.8548000, -0.5483000, -0.8407000, -0.7740000, -0.9608000, -0.1046000, -0.7336000, -0.1288000, -0.7339000, -0.0040000, -0.4334000, -0.6271000, -0.5037000, -0.1633000, -0.6751000, -0.8369000, -0.6738000, -0.0726000, -0.1347000, -0.8340000, -0.6787000, -0.2572000, -0.7153000, -0.6523000, -0.6090000, -0.4692000, -0.9586000, -0.5350000, -0.1498000, -0.4994000, -0.5534000, -0.0162000, -0.6675000, -0.3786000, -0.7978000, -0.5703000, -0.1149000, -0.5252000, -0.9184000, -0.6804000, -0.9278000, -0.0868000, -0.3476000, -0.6981000, -0.2336000, -0.3236000, -0.4192000, -0.7925000, -0.5071000, -0.3847000, -0.4513000, -0.6217000, -0.2708000, -0.2417000, -0.8592000, -0.0959000, -0.5619000, -0.8582000, -0.7621000, -0.7506000, -0.6632000, -0.9011000, -0.5317000, -0.7964000, -0.0341000, -0.4271000, -0.8659000, -0.8729000, -0.4645000, -0.0134000, -0.2681000, -0.8965000, -0.2234000, -0.7473000, -0.2588000, -0.1635000, -0.3786000, -0.3339000, -0.2567000, -0.0466000, -0.5194000, -0.3386000, -0.8251000, -0.2043000, -0.8359000, -0.2421000, -0.9028000, -0.2155000, -0.8741000, -0.5514000, -0.7091000, -0.9423000, -0.3202000, -0.6859000, -0.2488000, -0.7304000, -0.8478000, -0.7360000, -0.7262000, -0.7613000, -0.4042000, -0.4791000, -0.7246000, -0.4864000, -0.3103000, -0.2282000, -0.7069000, -0.0929000, -0.1988000, -0.1903000, -0.2470000, -0.3782000, -0.1285000, -0.0129000, -0.4195000, -0.2431000, -0.1753000, -0.1107000]]

Expected (Unparsed): [[-0.18069999999999997,0.24749999999999994,-0.8204,-0.8177,-0.7212,-0.9659,-0.5374,-0.7389,-0.0742,-0.7497,-0.7505,-0.3433,-0.5625,-0.1271,-0.8548,-0.5483,-0.8407,-0.774,-0.9608,-0.1046,-0.7336,-0.1288,-0.7339,-0.004,-0.4334,-0.6271,-0.5037,-0.1633,-0.6751,-0.8369,-0.6738,-0.0726,-0.1347,-0.834,-0.6787,-0.2572,-0.7153,-0.6523,-0.609,-0.4692,-0.9586,-0.535,-0.1498,-0.4994,-0.5534,-0.0162,-0.6675,-0.3786,-0.7978,-0.5703,-0.1149,-0.5252,-0.9184,-0.6804,-0.9278,-0.0868,-0.3476,-0.6981,-0.2336,-0.3236,-0.4192,-0.7925,-0.5071,-0.3847,-0.4513,-0.6217,-0.2708,-0.2417,-0.8592,-0.0959,-0.5619,-0.8582,-0.7621,-0.7506,-0.6632,-0.9011,-0.5317,-0.7964,-0.0341,-0.4271,-0.8659,-0.8729,-0.4645,-0.0134,-0.2681,-0.8965,-0.2234,-0.7473,-0.2588,-0.1635,-0.3786,-0.3339,-0.2567,-0.0466,-0.5194,-0.3386,-0.8251,-0.2043,-0.8359,-0.2421,-0.9028,-0.2155,-0.8741,-0.5514,-0.7091,-0.9423,-0.3202,-0.6859,-0.2488,-0.7304,-0.8478,-0.736,-0.7262,-0.7613,-0.4042,-0.4791,-0.7246,-0.4864,-0.3103,-0.2282,-0.7069,-0.0929,-0.1988,-0.1903,-0.247,-0.3782,-0.1285,-0.0129,-0.4195,-0.2431,-0.1753,-0.1107]]

Actual:   [[-0.1807, 0.2475, -0.8204, -0.8177, -0.7212, -0.9659, -0.5374, -0.7389, -0.0742, -0.7497, -0.7505, -0.3433, -0.5625, -0.1271, -0.8548, -0.5483, -0.8407, -0.774, -0.9608, -0.1046, -0.7336, -0.1288, -0.7339, -0.004, -0.4334, -0.6271, -0.5037, -0.1633, -0.6751, -0.8369, -0.6738, -0.0726, -0.1347, -0.834, -0.6787, -0.2572, -0.7153, -0.6523, -0.609, -0.4692, -0.9586, -0.535, -0.1498, -0.4994, -0.5534, -0.0162, -0.6675, -0.3786, -0.7978, -0.5703, -0.1149, -0.5252, -0.9184, -0.6804, -0.9278, -0.0868, -0.3476, -0.6981, -0.2336, -0.3236, -0.4192, -0.7925, -0.5071, -0.3847, -0.4513, -0.6217, -0.2708, -0.2417, -0.8592, -0.0959, -0.5619, -0.8582, -0.7621, -0.7506, -0.6632, -0.9011, -0.5317, -0.7964, -0.0341, -0.4271, -0.8659, -0.8729, -0.4645, -0.0134, -0.2681, -0.8965, -0.2234, -0.7473, -0.2588, -0.1635, -0.3786, -0.3339, -0.2567, -0.0466, -0.5194, -0.3386, -0.8251, -0.2043, -0.8359, -0.2421, -0.9028, -0.2155, -0.8741, -0.5514, -0.7091, -0.9423, -0.3202, -0.6859, -0.2488, -0.7304, -0.8478, -0.736, -0.7262, -0.7613, -0.4042, -0.4791, -0.7246, -0.4864, -0.3103, -0.2282, -0.7069, -0.0929, -0.1988, -0.1903, -0.247, -0.3782, -0.1285, -0.0129, -0.4195, -0.2431, -0.1753, -0.1107]]

Expected: [[-0.1806, 0.2475, -0.8204, -0.8177, -0.7212, -0.9659, -0.5374, -0.7389, -0.0742, -0.7497, -0.7505, -0.3433, -0.5625, -0.1271, -0.8548, -0.5483, -0.8407, -0.774, -0.9608, -0.1046, -0.7336, -0.1288, -0.7339, -0.004, -0.4334, -0.6271, -0.5037, -0.1633, -0.6751, -0.8369, -0.6738, -0.0726, -0.1347, -0.834, -0.6787, -0.2572, -0.7153, -0.6523, -0.609, -0.4692, -0.9586, -0.535, -0.1498, -0.4994, -0.5534, -0.0162, -0.6675, -0.3786, -0.7978, -0.5703, -0.1149, -0.5252, -0.9184, -0.6804, -0.9278, -0.0868, -0.3476, -0.6981, -0.2336, -0.3236, -0.4192, -0.7925, -0.5071, -0.3847, -0.4513, -0.6217, -0.2708, -0.2417, -0.8592, -0.0959, -0.5619, -0.8582, -0.7621, -0.7506, -0.6632, -0.9011, -0.5317, -0.7964, -0.0341, -0.4271, -0.8659, -0.8729, -0.4645, -0.0134, -0.2681, -0.8965, -0.2234, -0.7473, -0.2588, -0.1635, -0.3786, -0.3339, -0.2567, -0.0466, -0.5194, -0.3386, -0.8251, -0.2043, -0.8359, -0.2421, -0.9028, -0.2155, -0.8741, -0.5514, -0.7091, -0.9423, -0.3202, -0.6859, -0.2488, -0.7304, -0.8478, -0.736, -0.7262, -0.7613, -0.4042, -0.4791, -0.7246, -0.4864, -0.3103, -0.2282, -0.7069, -0.0929, -0.1988, -0.1903, -0.247, -0.3782, -0.1285, -0.0129, -0.4195, -0.2431, -0.1753, -0.1107]]