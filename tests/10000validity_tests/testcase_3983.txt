import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Add40573 = tf.keras.layers.Input(shape=([1, 1, 2]))
in1Add40573 = tf.keras.layers.Input(shape=([1, 1, 2]))
in0Con53562 = tf.keras.layers.Input(shape=([3, 3, 3, 1]))
in0Max89045 = tf.keras.layers.Input(shape=([1, 2, 2, 2]))
in1Max89045 = tf.keras.layers.Input(shape=([1, 2, 2, 2]))

Add40573 = keras.layers.Add(name = 'Add40573', )([in0Add40573,in1Add40573])
Den19754 = keras.layers.Dense(1,name = 'Den19754', )(Add40573)
Zer71837 = keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name = 'Zer71837', )(Den19754)
Res82759 = keras.layers.Reshape((3, 3, 1, 1), name = 'Res82759', )(Zer71837)
Zer98308 = keras.layers.ZeroPadding3D(padding=((0, 0), (0, 0), (2, 0)), name = 'Zer98308', )(Res82759)
Con53562 = keras.layers.Concatenate(axis=4, name = 'Con53562', )([Zer98308,in0Con53562])
Max89045 = keras.layers.Maximum(name = 'Max89045', )([in0Max89045,in1Max89045])
Con91042 = keras.layers.Conv3D(2, (2, 1, 1),strides=(1, 1, 1), padding='same', dilation_rate=(1, 1, 1), name = 'Con91042', )(Max89045)
Zer85517 = keras.layers.ZeroPadding3D(padding=((2, 0), (1, 0), (1, 0)), name = 'Zer85517', )(Con91042)
Ave66048 = keras.layers.Average(name = 'Ave66048', )([Con53562,Zer85517])
Lea50128 = keras.layers.LeakyReLU(alpha=9.528086698168096, name = 'Lea50128', )(Ave66048)
model = tf.keras.models.Model(inputs=[in0Add40573,in1Add40573,in0Con53562,in0Max89045,in1Max89045], outputs=Lea50128)
w = model.get_layer('Den19754').get_weights() 
w[0] = np.array([[0.128], [0.5836]])
w[1] = np.array([0.8322])
model.get_layer('Den19754').set_weights(w) 
w = model.get_layer('Con91042').get_weights() 
w[0] = np.array([[[[[0.8065, 0.0506], [0.4681, 0.3723]]]], [[[[0.5884, 0.0577], [0.2924, 0.6138]]]]])
w[1] = np.array([0, 0])
model.get_layer('Con91042').set_weights(w) 
in0Add40573 = tf.constant([[[[0.7015, 0.7376]]]])
in1Add40573 = tf.constant([[[[0.9886, 0.5624]]]])
in0Con53562 = tf.constant([[[[[0.7983], [0.4162], [0.3686]], [[0.0158], [0.1342], [0.815]], [[0.497], [0.1854], [0.5134]]], [[[0.823], [0.0345], [0.7139]], [[0.1185], [0.3675], [0.8351]], [[0.3533], [0.0576], [0.2764]]], [[[0.2064], [0.9106], [0.2058]], [[0.1564], [0.4312], [0.596]], [[0.9839], [0.5653], [0.7993]]]]])
in0Max89045 = tf.constant([[[[[0.9462, 0.2754], [0.155, 0.4189]], [[0.3298, 0.0063], [0.5444, 0.0892]]]]])
in1Max89045 = tf.constant([[[[[0.3, 0.0178], [0.3894, 0.6023]], [[0.2107, 0.3257], [0.7056, 0.4014]]]]])
print (np.array2string(model.predict([in0Add40573,in1Add40573,in0Con53562,in0Max89045,in1Max89045],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Lea50128.png')

LAdd40573 = add_layer([[[[[0.7015, 0.7376]]]], [[[[0.9886, 0.5624]]]]], Add40573), 
LDen19754 = dense_layer(Add40573, [[0.128], [0.5836]],[0.8322], Den19754), 
LZer71837 = zero_padding2D_layer(Den19754, 1, 1, 1, 1, Zer71837), 
LRes82759 = reshape_layer(Zer71837, [3, 3, 1, 1], Res82759), 
LZer98308 = zero_padding3D_layer(Res82759, 0, 0, 0, 0, 2, 0, Zer98308), 
LCon53562 = concatenate_layer([Zer98308,[[[[[0.7983], [0.4162], [0.3686]], [[0.0158], [0.1342], [0.815]], [[0.497], [0.1854], [0.5134]]], [[[0.823], [0.0345], [0.7139]], [[0.1185], [0.3675], [0.8351]], [[0.3533], [0.0576], [0.2764]]], [[[0.2064], [0.9106], [0.2058]], [[0.1564], [0.4312], [0.596]], [[0.9839], [0.5653], [0.7993]]]]]], 4, Con53562), 
LMax89045 = maximum_layer([[[[[[0.9462, 0.2754], [0.155, 0.4189]], [[0.3298, 0.0063], [0.5444, 0.0892]]]]], [[[[[0.3, 0.0178], [0.3894, 0.6023]], [[0.2107, 0.3257], [0.7056, 0.4014]]]]]], Max89045), 
LCon91042 = conv3D_layer(Max89045, 2, 1, 1,[[[[[0.8065, 0.0506], [0.4681, 0.3723]]]], [[[[0.5884, 0.0577], [0.2924, 0.6138]]]]],[0, 0], 1, 1, 1, true, 1, 1, 1, Con91042), 
LZer85517 = zero_padding3D_layer(Con91042, 2, 0, 1, 0, 1, 0, Zer85517), 
LAve66048 = average_layer([Con53562,Zer85517], Ave66048), 
LLea50128 = leaky_relu_layer(Ave66048, 9.528086698168096, Lea50128), 
exec_layers([LAdd40573,LDen19754,LZer71837,LRes82759,LZer98308,LCon53562,LMax89045,LCon91042,LZer85517,LAve66048,LLea50128],["Add40573","Den19754","Zer71837","Res82759","Zer98308","Con53562","Max89045","Con91042","Zer85517","Ave66048","Lea50128"],Lea50128,"Lea50128")

Actual (Unparsed): [[[[[0.0000000, 0.3991500], [0.0000000, 0.2081000], [0.0000000, 0.1843000]], [[0.0000000, 0.0079000], [0.0000000, 0.0671000], [0.0000000, 0.4075000]], [[0.0000000, 0.2485000], [0.0000000, 0.0927000], [0.0000000, 0.2567000]]], [[[0.0000000, 0.4115000], [0.0000000, 0.0172500], [0.0000000, 0.3569500]], [[0.0000000, 0.0592500], [0.0000000, 0.1837500], [0.9036064, 0.4175500]], [[0.0000000, 0.1766500], [0.0000000, 0.0288000], [0.0000000, 0.1382000]]], [[[0.0000000, 0.1032000], [0.0000000, 0.4553000], [0.0000000, 0.1029000]], [[0.0000000, 0.0782000], [0.4460125, 0.2908046], [0.2979939, 0.4199700]], [[0.0000000, 0.4919500], [0.2092219, 0.3516230], [0.3784809, 0.4922223]]]]]

Expected (Unparsed): [[[[[0,0.39915],[0,0.2081],[0,0.1843]],[[0,0.0079],[0,0.0671],[0,0.4075]],[[0,0.2485],[0,0.0927],[0,0.2567]]],[[[0,0.4115],[0,0.01725],[0,0.35695]],[[0,0.05925],[0,0.18375],[0.9036064,0.41755]],[[0,0.17665],[0,0.0288],[0,0.1382]]],[[[0,0.1032],[0,0.4553],[0,0.1029]],[[0,0.0782],[0.44601252,0.29080457000000004],[0.297993865,0.419969965]],[[0,0.49195],[0.209221935,0.351622995],[0.37848086999999997,0.49222229]]]]]

Actual:   [[[[[0, 0.3992], [0, 0.2081], [0, 0.1843]], [[0, 0.0079], [0, 0.0671], [0, 0.4075]], [[0, 0.2485], [0, 0.0927], [0, 0.2567]]], [[[0, 0.4115], [0, 0.0173], [0, 0.357]], [[0, 0.0593], [0, 0.1838], [0.9037, 0.4176]], [[0, 0.1767], [0, 0.0288], [0, 0.1382]]], [[[0, 0.1032], [0, 0.4553], [0, 0.1029]], [[0, 0.0782], [0.4461, 0.2909], [0.298, 0.42]], [[0, 0.492], [0.2093, 0.3517], [0.3785, 0.4923]]]]]

Expected: [[[[[0, 0.3992], [0, 0.2081], [0, 0.1843]], [[0, 0.0079], [0, 0.0671], [0, 0.4075]], [[0, 0.2485], [0, 0.0927], [0, 0.2567]]], [[[0, 0.4115], [0, 0.0173], [0, 0.357]], [[0, 0.0593], [0, 0.1838], [0.9037, 0.4176]], [[0, 0.1767], [0, 0.0288], [0, 0.1382]]], [[[0, 0.1032], [0, 0.4553], [0, 0.1029]], [[0, 0.0782], [0.4461, 0.2909], [0.298, 0.42]], [[0, 0.492], [0.2093, 0.3517], [0.3785, 0.4923]]]]]