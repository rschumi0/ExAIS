import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Mul62610 = tf.keras.layers.Input(shape=([1, 1]))
in1Mul62610 = tf.keras.layers.Input(shape=([1, 1]))
in0Con11795 = tf.keras.layers.Input(shape=([47]))
in0Add41317 = tf.keras.layers.Input(shape=([1, 1]))
in1Add41317 = tf.keras.layers.Input(shape=([1, 1]))
in0Con90556 = tf.keras.layers.Input(shape=([3, 4, 3]))
in0Zer19712 = tf.keras.layers.Input(shape=([1, 2, 4]))
in0Bat89940 = tf.keras.layers.Input(shape=([4, 2, 4]))
in0Con83505 = tf.keras.layers.Input(shape=([40]))

Mul62610 = keras.layers.Multiply(name = 'Mul62610', )([in0Mul62610,in1Mul62610])
Sep88545 = keras.layers.SeparableConv1D(3, (1),strides=(1), padding='valid', name = 'Sep88545', )(Mul62610)
Res36444 = keras.layers.Reshape((1, 3, 1), name = 'Res36444', )(Sep88545)
Res51143 = keras.layers.Reshape((1, 3, 1, 1), name = 'Res51143', )(Res36444)
Glo45284 = keras.layers.GlobalAveragePooling3D(name = 'Glo45284', )(Res51143)
Con11795 = keras.layers.Concatenate(axis=1, name = 'Con11795', )([Glo45284,in0Con11795])
Add41317 = keras.layers.Add(name = 'Add41317', )([in0Add41317,in1Add41317])
Res66902 = keras.layers.Reshape((1, 1, 1), name = 'Res66902', )(Add41317)
Zer12369 = keras.layers.ZeroPadding2D(padding=((2, 0), (3, 0)), name = 'Zer12369', )(Res66902)
Con90556 = keras.layers.Concatenate(axis=3, name = 'Con90556', )([Zer12369,in0Con90556])
Zer19712 = keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name = 'Zer19712', )(in0Zer19712)
Ave18175 = keras.layers.Average(name = 'Ave18175', )([Con90556,Zer19712])
Res11028 = keras.layers.Reshape((3, 16), name = 'Res11028', )(Ave18175)
Fla88248 = keras.layers.Flatten(name = 'Fla88248', )(Res11028)
Bat89940 = keras.layers.BatchNormalization(axis=1, epsilon=0.6880799022133912,  name = 'Bat89940', )(in0Bat89940)
Res69686 = keras.layers.Reshape((4, 8), name = 'Res69686', )(Bat89940)
Glo4402 = keras.layers.GlobalMaxPool1D(name = 'Glo4402', )(Res69686)
Con83505 = keras.layers.Concatenate(axis=1, name = 'Con83505', )([Glo4402,in0Con83505])
Ave49888 = keras.layers.Average(name = 'Ave49888', )([Fla88248,Con83505])
Mul97883 = keras.layers.Multiply(name = 'Mul97883', )([Con11795,Ave49888])
model = tf.keras.models.Model(inputs=[in0Mul62610,in1Mul62610,in0Con11795,in0Add41317,in1Add41317,in0Con90556,in0Zer19712,in0Bat89940,in0Con83505], outputs=Mul97883)
w = model.get_layer('Sep88545').get_weights() 
w[0] = np.array([[[0.2781]]])
w[1] = np.array([[[0.844, 0.1531, 0.2122]]])
w[2] = np.array([0, 0, 0])
model.get_layer('Sep88545').set_weights(w) 
w = model.get_layer('Bat89940').get_weights() 
w[0] = np.array([0.1684, 0.2755, 0.3838, 0.2751])
w[1] = np.array([0.8644, 0.1327, 0.7644, 0.8918])
w[2] = np.array([0.8615, 0.8343, 0.7249, 0.795])
w[3] = np.array([0.285, 0.7388, 0.2363, 0.4813])
model.get_layer('Bat89940').set_weights(w) 
in0Mul62610 = tf.constant([[[0.1122]]])
in1Mul62610 = tf.constant([[[0.4994]]])
in0Con11795 = tf.constant([[0.6015, 0.2206, 0.0463, 0.5181, 0.259, 0.6569, 0.8057, 0.9689, 0.6327, 0.1276, 0.1472, 0.9806, 0.6052, 0.7192, 0.1976, 0.1107, 0.9258, 0.9125, 0.1247, 0.7162, 0.8036, 0.7752, 0.8354, 0.473, 0.2042, 0.9715, 0.7933, 0.8781, 0.6186, 0.01, 0.5392, 0.1838, 0.6477, 0.4618, 0.0901, 0.1578, 0.6097, 0.7135, 0.7743, 0.0392, 0.5165, 0.191, 0.2106, 0.0735, 0.1915, 0.8873, 0.0833]])
in0Add41317 = tf.constant([[[0.6874]]])
in1Add41317 = tf.constant([[[0.1832]]])
in0Con90556 = tf.constant([[[[0.0186, 0.6362, 0.0651], [0.8958, 0.2988, 0.2947], [0.4891, 0.8944, 0.184], [0.9709, 0.0612, 0.694]], [[0.9658, 0.1644, 0.851], [0.1236, 0.9649, 0.0386], [0.5532, 0.2975, 0.6038], [0.3046, 0.5507, 0.6234]], [[0.6818, 0.3857, 0.3535], [0.7811, 0.0841, 0.9984], [0.5466, 0.8571, 0.2853], [0.0155, 0.2772, 0.9338]]]])
in0Zer19712 = tf.constant([[[[1.9926, 1.1345, 1.8202, 1.2378], [1.9323, 1.5866, 1.1618, 1.6619]]]])
in0Bat89940 = tf.constant([[[[1.394, 1.8912, 1.2174, 1.063], [1.0781, 1.0533, 1.836, 1.9092]], [[1.738, 1.4224, 1.2864, 1.6793], [1.1083, 1.6136, 1.298, 1.6159]], [[1.411, 1.7203, 1.1842, 1.6249], [1.5335, 1.792, 1.5629, 1.4822]], [[1.3791, 1.8081, 1.8562, 1.6588], [1.0779, 1.6712, 1.7086, 1.7543]]]])
in0Con83505 = tf.constant([[0.4775, 0.5755, 0.3302, 0.591, 0.5092, 0.2473, 0.9018, 0.4779, 0.5823, 0.1887, 0.9058, 0.8284, 0.4374, 0.6328, 0.9946, 0.15, 0.4383, 0.1402, 0.0948, 0.0564, 0.0496, 0.9313, 0.0707, 0.8419, 0.6015, 0.0915, 0.4221, 0.6121, 0.0288, 0.7631, 0.0894, 0.9869, 0.111, 0.6298, 0.5476, 0.4848, 0.3717, 0.2307, 0.7635, 0.3089]])
print (np.array2string(model.predict([in0Mul62610,in1Mul62610,in0Con11795,in0Add41317,in1Add41317,in0Con90556,in0Zer19712,in0Bat89940,in0Con83505],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Mul97883.png')

LMul62610 = multiply_layer([[[[0.1122]]], [[[0.4994]]]], Mul62610), 
LSep88545 = separable_conv1D_layer(Mul62610, 1,[[[[0.2781]]],[[[0.844, 0.1531, 0.2122]]]],[0, 0, 0], 1, false, Sep88545), 
LRes36444 = reshape_layer(Sep88545, [1, 3, 1], Res36444), 
LRes51143 = reshape_layer(Res36444, [1, 3, 1, 1], Res51143), 
LGlo45284 = global_average_pooling3D_layer(Res51143, Glo45284), 
LCon11795 = concatenate_layer([Glo45284,[[0.6015, 0.2206, 0.0463, 0.5181, 0.259, 0.6569, 0.8057, 0.9689, 0.6327, 0.1276, 0.1472, 0.9806, 0.6052, 0.7192, 0.1976, 0.1107, 0.9258, 0.9125, 0.1247, 0.7162, 0.8036, 0.7752, 0.8354, 0.473, 0.2042, 0.9715, 0.7933, 0.8781, 0.6186, 0.01, 0.5392, 0.1838, 0.6477, 0.4618, 0.0901, 0.1578, 0.6097, 0.7135, 0.7743, 0.0392, 0.5165, 0.191, 0.2106, 0.0735, 0.1915, 0.8873, 0.0833]]], 1, Con11795), 
LAdd41317 = add_layer([[[[0.6874]]], [[[0.1832]]]], Add41317), 
LRes66902 = reshape_layer(Add41317, [1, 1, 1], Res66902), 
LZer12369 = zero_padding2D_layer(Res66902, 2, 0, 3, 0, Zer12369), 
LCon90556 = concatenate_layer([Zer12369,[[[[0.0186, 0.6362, 0.0651], [0.8958, 0.2988, 0.2947], [0.4891, 0.8944, 0.184], [0.9709, 0.0612, 0.694]], [[0.9658, 0.1644, 0.851], [0.1236, 0.9649, 0.0386], [0.5532, 0.2975, 0.6038], [0.3046, 0.5507, 0.6234]], [[0.6818, 0.3857, 0.3535], [0.7811, 0.0841, 0.9984], [0.5466, 0.8571, 0.2853], [0.0155, 0.2772, 0.9338]]]]], 3, Con90556), 
LZer19712 = zero_padding2D_layer([[[[1.9926, 1.1345, 1.8202, 1.2378], [1.9323, 1.5866, 1.1618, 1.6619]]]], 1, 1, 1, 1, Zer19712), 
LAve18175 = average_layer([Con90556,Zer19712], Ave18175), 
LRes11028 = reshape_layer(Ave18175, [3, 16], Res11028), 
LFla88248 = flatten_layer(Res11028, Fla88248), 
LBat89940 = batch_normalization_layer([[[[1.394, 1.8912, 1.2174, 1.063], [1.0781, 1.0533, 1.836, 1.9092]], [[1.738, 1.4224, 1.2864, 1.6793], [1.1083, 1.6136, 1.298, 1.6159]], [[1.411, 1.7203, 1.1842, 1.6249], [1.5335, 1.792, 1.5629, 1.4822]], [[1.3791, 1.8081, 1.8562, 1.6588], [1.0779, 1.6712, 1.7086, 1.7543]]]], 1, 0.6880799022133912, [0.1684, 0.2755, 0.3838, 0.2751], [0.8644, 0.1327, 0.7644, 0.8918], [0.8615, 0.8343, 0.7249, 0.795], [0.285, 0.7388, 0.2363, 0.4813], Bat89940), 
LRes69686 = reshape_layer(Bat89940, [4, 8], Res69686), 
LGlo4402 = global_max_pool1D_layer(Res69686, Glo4402), 
LCon83505 = concatenate_layer([Glo4402,[[0.4775, 0.5755, 0.3302, 0.591, 0.5092, 0.2473, 0.9018, 0.4779, 0.5823, 0.1887, 0.9058, 0.8284, 0.4374, 0.6328, 0.9946, 0.15, 0.4383, 0.1402, 0.0948, 0.0564, 0.0496, 0.9313, 0.0707, 0.8419, 0.6015, 0.0915, 0.4221, 0.6121, 0.0288, 0.7631, 0.0894, 0.9869, 0.111, 0.6298, 0.5476, 0.4848, 0.3717, 0.2307, 0.7635, 0.3089]]], 1, Con83505), 
LAve49888 = average_layer([Fla88248,Con83505], Ave49888), 
LMul97883 = multiply_layer([Con11795,Ave49888], Mul97883), 
exec_layers([LMul62610,LSep88545,LRes36444,LRes51143,LGlo45284,LCon11795,LAdd41317,LRes66902,LZer12369,LCon90556,LZer19712,LAve18175,LRes11028,LFla88248,LBat89940,LRes69686,LGlo4402,LCon83505,LAve49888,LMul97883],["Mul62610","Sep88545","Res36444","Res51143","Glo45284","Con11795","Add41317","Res66902","Zer12369","Con90556","Zer19712","Ave18175","Res11028","Fla88248","Bat89940","Res69686","Glo4402","Con83505","Ave49888","Mul97883"],Mul97883,"Mul97883")

Actual (Unparsed): [[0.0032676, 0.3521944, 0.1632293, 0.0267665, 0.2816353, 0.2121567, 0.4183197, 0.5169345, 0.2313249, 0.2594228, 0.0495981, 0.0502688, 0.2496607, 0.2217301, 0.3352910, 0.0815001, 0.0322303, 0.3108836, 0.4507750, 0.0781807, 0.5134080, 0.5070113, 0.9252593, 0.3292311, 0.3321524, 0.1235512, 0.4004766, 0.4717160, 0.0217769, 0.3351575, 0.0017302, 0.3110106, 0.0552778, 0.1400327, 0.1419920, 0.0355377, 0.0022723, 0.3516902, 0.0468948, 0.5753436, 0.0021756, 0.2332256, 0.0932223, 0.0660705, 0.0296572, 0.0228316, 0.4002167, 0.0323121]]

Expected (Unparsed): [[0.0032675545078706793,0.3521944272963175,0.1632292847432836,0.026766517486388006,0.28163529649327934,0.21215670540917764,0.41831967546303855,0.5169344963213304,0.23132487499999999,0.2594228175,0.049598119999999996,0.050268799999999995,0.24966076,0.22173015,0.33529104,0.08150012,0.032230305,0.31088363999999996,0.450775,0.07818066500000001,0.5134079699999999,0.50701133,0.92525934,0.32923114000000003,0.332152425,0.12355121000000001,0.4004765875,0.47171601249999995,0.02177688,0.33515748000000006,0.0017302499999999998,0.31101056,0.05527785,0.14003274000000002,0.141991955,0.035537692499999995,0.00227232,0.35169020250000005,0.04689478749999999,0.575343615,0.0021755999999999998,0.233225575,0.093222325,0.06607048500000001,0.029657249999999996,0.0228315875,0.40021666499999997,0.03231207]]

Actual:   [[0.0033, 0.3522, 0.1633, 0.0268, 0.2817, 0.2122, 0.4184, 0.517, 0.2314, 0.2595, 0.0496, 0.0503, 0.2497, 0.2218, 0.3353, 0.0816, 0.0323, 0.3109, 0.4508, 0.0782, 0.5135, 0.5071, 0.9253, 0.3293, 0.3322, 0.1236, 0.4005, 0.4718, 0.0218, 0.3352, 0.0018, 0.3111, 0.0553, 0.1401, 0.142, 0.0356, 0.0023, 0.3517, 0.0469, 0.5754, 0.0022, 0.2333, 0.0933, 0.0661, 0.0297, 0.0229, 0.4003, 0.0324]]

Expected: [[0.0033, 0.3522, 0.1633, 0.0268, 0.2817, 0.2122, 0.4184, 0.517, 0.2314, 0.2595, 0.0496, 0.0503, 0.2497, 0.2218, 0.3353, 0.0816, 0.0323, 0.3109, 0.4508, 0.0782, 0.5135, 0.5071, 0.9253, 0.3293, 0.3322, 0.1236, 0.4005, 0.4718, 0.0218, 0.3352, 0.0018, 0.3111, 0.0553, 0.1401, 0.142, 0.0356, 0.0023, 0.3517, 0.0469, 0.5754, 0.0022, 0.2333, 0.0933, 0.0661, 0.0297, 0.0229, 0.4003, 0.0324]]