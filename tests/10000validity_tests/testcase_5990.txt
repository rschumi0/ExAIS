import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Min42356 = tf.keras.layers.Input(shape=([1, 2, 2]))
in1Min42356 = tf.keras.layers.Input(shape=([1, 2, 2]))
in0Sep68181 = tf.keras.layers.Input(shape=([2, 2]))
in0Con51641 = tf.keras.layers.Input(shape=([2, 1]))
in0Up_75847 = tf.keras.layers.Input(shape=([2, 1, 1]))
in0Con11766 = tf.keras.layers.Input(shape=([2, 3]))
in0Lay95920 = tf.keras.layers.Input(shape=([1, 4]))

Min42356 = keras.layers.Minimum(name = 'Min42356', )([in0Min42356,in1Min42356])
Res54815 = keras.layers.Reshape((1, 2, 2, 1), name = 'Res54815', )(Min42356)
Con10229 = keras.layers.Conv3D(4, (1, 2, 1),strides=(3, 1, 1), padding='same', dilation_rate=(1, 1, 1), name = 'Con10229', )(Res54815)
Res69398 = keras.layers.Reshape((1, 2, 8), name = 'Res69398', )(Con10229)
Res36593 = keras.layers.Reshape((1, 16), name = 'Res36593', )(Res69398)
Zer70875 = keras.layers.ZeroPadding1D(padding=((1, 0)), name = 'Zer70875', )(Res36593)
Sep68181 = keras.layers.SeparableConv1D(3, (1),strides=(2), padding='same', name = 'Sep68181', )(in0Sep68181)
Zer3467 = keras.layers.ZeroPadding1D(padding=((1, 0)), name = 'Zer3467', )(Sep68181)
Con51641 = keras.layers.Concatenate(axis=2, name = 'Con51641', )([Zer3467,in0Con51641])
Up_75847 = keras.layers.UpSampling2D(size=(1, 1), name = 'Up_75847', )(in0Up_75847)
Res79418 = keras.layers.Reshape((2, 1), name = 'Res79418', )(Up_75847)
Con11766 = keras.layers.Concatenate(axis=2, name = 'Con11766', )([Res79418,in0Con11766])
Lay95920 = keras.layers.LayerNormalization(axis=2, epsilon=1.0403780260617277, name = 'Lay95920', )(in0Lay95920)
Zer60195 = keras.layers.ZeroPadding1D(padding=((1, 0)), name = 'Zer60195', )(Lay95920)
Mul7216 = keras.layers.Multiply(name = 'Mul7216', )([Con11766,Zer60195])
Max18337 = keras.layers.Maximum(name = 'Max18337', )([Con51641,Mul7216])
Dot22046 = keras.layers.Dot(axes=(1, 1), name = 'Dot22046', )([Zer70875,Max18337])
model = tf.keras.models.Model(inputs=[in0Min42356,in1Min42356,in0Sep68181,in0Con51641,in0Up_75847,in0Con11766,in0Lay95920], outputs=Dot22046)
w = model.get_layer('Con10229').get_weights() 
w[0] = np.array([[[[[0.4935, 0.1762, 0.8854, 0.5077]]], [[[0.708, 0.7937, 0.0135, 0.2398]]]]])
w[1] = np.array([0, 0, 0, 0])
model.get_layer('Con10229').set_weights(w) 
w = model.get_layer('Sep68181').get_weights() 
w[0] = np.array([[[0.9209], [0.7288]]])
w[1] = np.array([[[0.1814, 0.0881, 0.3338], [0.1982, 0.4473, 0.7613]]])
w[2] = np.array([0, 0, 0])
model.get_layer('Sep68181').set_weights(w) 
in0Min42356 = tf.constant([[[[0.5593, 0.9198], [0.8482, 0.7787]]]])
in1Min42356 = tf.constant([[[[0.9808, 0.136], [0.6621, 0.466]]]])
in0Sep68181 = tf.constant([[[0.7132, 0.8084], [0.6991, 0.9012]]])
in0Con51641 = tf.constant([[[0.9607], [0.487]]])
in0Up_75847 = tf.constant([[[[1.3786]], [[1.8346]]]])
in0Con11766 = tf.constant([[[0.7526, 0.9102, 0.7165], [0.3105, 0.004, 0.5843]]])
in0Lay95920 = tf.constant([[[1.2458, 1.4556, 1.3377, 1.1765]]])
print (np.array2string(model.predict([in0Min42356,in1Min42356,in0Sep68181,in0Con51641,in0Up_75847,in0Con11766,in0Lay95920],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Dot22046.png')

LMin42356 = minimum_layer([[[[[0.5593, 0.9198], [0.8482, 0.7787]]]], [[[[0.9808, 0.136], [0.6621, 0.466]]]]], Min42356), 
LRes54815 = reshape_layer(Min42356, [1, 2, 2, 1], Res54815), 
LCon10229 = conv3D_layer(Res54815, 1, 2, 1,[[[[[0.4935, 0.1762, 0.8854, 0.5077]]], [[[0.708, 0.7937, 0.0135, 0.2398]]]]],[0, 0, 0, 0], 3, 1, 1, true, 1, 1, 1, Con10229), 
LRes69398 = reshape_layer(Con10229, [1, 2, 8], Res69398), 
LRes36593 = reshape_layer(Res69398, [1, 16], Res36593), 
LZer70875 = zero_padding1D_layer(Res36593, 1, 0, Zer70875), 
LSep68181 = separable_conv1D_layer([[[0.7132, 0.8084], [0.6991, 0.9012]]], 1,[[[[0.9209], [0.7288]]],[[[0.1814, 0.0881, 0.3338], [0.1982, 0.4473, 0.7613]]]],[0, 0, 0], 2, true, Sep68181), 
LZer3467 = zero_padding1D_layer(Sep68181, 1, 0, Zer3467), 
LCon51641 = concatenate_layer([Zer3467,[[[0.9607], [0.487]]]], 2, Con51641), 
LUp_75847 = up_sampling2D_layer([[[[1.3786]], [[1.8346]]]], 1, 1, Up_75847), 
LRes79418 = reshape_layer(Up_75847, [2, 1], Res79418), 
LCon11766 = concatenate_layer([Res79418,[[[0.7526, 0.9102, 0.7165], [0.3105, 0.004, 0.5843]]]], 2, Con11766), 
LLay95920 = layer_normalization_layer([[[1.2458, 1.4556, 1.3377, 1.1765]]], 2, 1.0403780260617277, Lay95920), 
LZer60195 = zero_padding1D_layer(Lay95920, 1, 0, Zer60195), 
LMul7216 = multiply_layer([Con11766,Zer60195], Mul7216), 
LMax18337 = maximum_layer([Con51641,Mul7216], Max18337), 
LDot22046 = dot_layer(Zer70875,Max18337, 1, 1, Dot22046), 
exec_layers([LMin42356,LRes54815,LCon10229,LRes69398,LRes36593,LZer70875,LSep68181,LZer3467,LCon51641,LUp_75847,LRes79418,LCon11766,LLay95920,LZer60195,LMul7216,LMax18337,LDot22046],["Min42356","Res54815","Con10229","Res69398","Res36593","Zer70875","Sep68181","Zer3467","Con51641","Up_75847","Res79418","Con11766","Lay95920","Zer60195","Mul7216","Max18337","Dot22046"],Dot22046,"Dot22046")

Actual (Unparsed): [[[0.1757035, 0.2393690, 0.4973382, 0.3627085], [0.1472232, 0.2005689, 0.4167231, 0.3039160], [0.1189337, 0.1620289, 0.3366483, 0.2455174], [0.1044453, 0.1422906, 0.2956380, 0.2156086], [0.0936678, 0.1276079, 0.2651317, 0.1933604], [0.0929089, 0.1265741, 0.2629838, 0.1917939], [0.0298914, 0.0407225, 0.0846093, 0.0617055], [0.0426516, 0.0581063, 0.1207277, 0.0880467], [0.0770837, 0.1050146, 0.2181895, 0.1591255], [0.0275221, 0.0374946, 0.0779027, 0.0568144], [0.1382976, 0.1884092, 0.3914589, 0.2854908], [0.0793017, 0.1080363, 0.2244677, 0.1637042], [0.0542531, 0.0739115, 0.1535664, 0.1119959], [0.0193706, 0.0263895, 0.0548296, 0.0399872], [0.0973368, 0.1326064, 0.2755170, 0.2009344], [0.0558142, 0.0760383, 0.1579851, 0.1152184]]]

Expected (Unparsed): [[[0.17570349178121034,0.2393689743101541,0.49733824522811343,0.36270851745],[0.147223167608867,0.20056891452737205,0.4167231458706185,0.30391596840999996],[0.11893371108789613,0.16202888255322861,0.3366483077329894,0.24551743158999997],[0.10444526959888986,0.14229061017504135,0.2956379897638667,0.21560862853000004],[0.09366778208232374,0.1276079416274331,0.26513172790692335,0.193360428],[0.09290894480523101,0.1265741425899491,0.2629837979143145,0.1917939438],[0.029891432173395547,0.04072247732513415,0.08460931694506878,0.0617055298],[0.04265162801551374,0.05810628091241813,0.12072774205177336,0.088046678],[0.0770836630398512,0.1050146310176626,0.218189481172818,0.15912547245],[0.027522069762151533,0.03749458558320598,0.07790270837416521,0.05681440374],[0.13829761956531764,0.18840922857758555,0.3914588989471389,0.28549076658],[0.07930167320229474,0.10803632860722857,0.22446767901000952,0.16370415879],[0.05425311429779589,0.07391152100019752,0.15356637702240325,0.11199587700000001],[0.019370615479780418,0.02638948328315056,0.054829575747411256,0.0399871804],[0.09733679310895337,0.13260640464756815,0.27551706224039685,0.2009344468],[0.05581419681659772,0.07603825574832883,0.15798510560136603,0.1152184534]]]

Actual:   [[[0.1758, 0.2394, 0.4974, 0.3628], [0.1473, 0.2006, 0.4168, 0.304], [0.119, 0.1621, 0.3367, 0.2456], [0.1045, 0.1423, 0.2957, 0.2157], [0.0937, 0.1277, 0.2652, 0.1934], [0.093, 0.1266, 0.263, 0.1918], [0.0299, 0.0408, 0.0847, 0.0618], [0.0427, 0.0582, 0.1208, 0.0881], [0.0771, 0.1051, 0.2182, 0.1592], [0.0276, 0.0375, 0.078, 0.0569], [0.1383, 0.1885, 0.3915, 0.2855], [0.0794, 0.1081, 0.2245, 0.1638], [0.0543, 0.074, 0.1536, 0.112], [0.0194, 0.0264, 0.0549, 0.04], [0.0974, 0.1327, 0.2756, 0.201], [0.0559, 0.0761, 0.158, 0.1153]]]

Expected: [[[0.1758, 0.2394, 0.4974, 0.3628], [0.1473, 0.2006, 0.4168, 0.304], [0.119, 0.1621, 0.3367, 0.2456], [0.1045, 0.1423, 0.2957, 0.2157], [0.0937, 0.1277, 0.2652, 0.1934], [0.093, 0.1266, 0.263, 0.1918], [0.0299, 0.0408, 0.0847, 0.0618], [0.0427, 0.0582, 0.1208, 0.0881], [0.0771, 0.1051, 0.2182, 0.1592], [0.0276, 0.0375, 0.078, 0.0569], [0.1383, 0.1885, 0.3915, 0.2855], [0.0794, 0.1081, 0.2245, 0.1638], [0.0543, 0.074, 0.1536, 0.112], [0.0194, 0.0264, 0.0549, 0.04], [0.0974, 0.1327, 0.2756, 0.201], [0.0559, 0.0761, 0.158, 0.1153]]]