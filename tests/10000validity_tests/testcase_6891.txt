import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Zer48736 = tf.keras.layers.Input(shape=([1, 1, 4]))
in0Ave51834 = tf.keras.layers.Input(shape=([2, 2, 2]))
in1Ave51834 = tf.keras.layers.Input(shape=([2, 2, 2]))
in0Con39732 = tf.keras.layers.Input(shape=([3, 3, 2]))

Zer48736 = keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name = 'Zer48736', )(in0Zer48736)
Ave51834 = keras.layers.Average(name = 'Ave51834', )([in0Ave51834,in1Ave51834])
Bat33861 = keras.layers.BatchNormalization(axis=3, epsilon=0.358871567947012,  name = 'Bat33861', )(Ave51834)
Zer48794 = keras.layers.ZeroPadding2D(padding=((1, 0), (1, 0)), name = 'Zer48794', )(Bat33861)
Con39732 = keras.layers.Concatenate(axis=3, name = 'Con39732', )([Zer48794,in0Con39732])
Max29146 = keras.layers.Maximum(name = 'Max29146', )([Zer48736,Con39732])
model = tf.keras.models.Model(inputs=[in0Zer48736,in0Ave51834,in1Ave51834,in0Con39732], outputs=Max29146)
w = model.get_layer('Bat33861').get_weights() 
w[0] = np.array([0.2244, 0.2609])
w[1] = np.array([0.5753, 0.1518])
w[2] = np.array([0.2251, 0.2972])
w[3] = np.array([0.1218, 0.8067])
model.get_layer('Bat33861').set_weights(w) 
in0Zer48736 = tf.constant([[[[1.4188, 1.3781, 1.2464, 1.135]]]])
in0Ave51834 = tf.constant([[[[0.7523, 0.2243], [0.5916, 0.6667]], [[0.7527, 0.18], [0.4918, 0.2877]]]])
in1Ave51834 = tf.constant([[[[0.1411, 0.2994], [0.2632, 0.4784]], [[0.3903, 0.1414], [0.2299, 0.1803]]]])
in0Con39732 = tf.constant([[[[0.2667, 0.8864], [0.4727, 0.7345], [0.2038, 0.9385]], [[0.3469, 0.1774], [0.1852, 0.2868], [0.1413, 0.4918]], [[0.5049, 0.3134], [0.0868, 0.5713], [0.0486, 0.3668]]]])
print (np.array2string(model.predict([in0Zer48736,in0Ave51834,in1Ave51834,in0Con39732],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Max29146.png')

LZer48736 = zero_padding2D_layer([[[[1.4188, 1.3781, 1.2464, 1.135]]]], 1, 1, 1, 1, Zer48736), 
LAve51834 = average_layer([[[[[0.7523, 0.2243], [0.5916, 0.6667]], [[0.7527, 0.18], [0.4918, 0.2877]]]], [[[[0.1411, 0.2994], [0.2632, 0.4784]], [[0.3903, 0.1414], [0.2299, 0.1803]]]]], Ave51834), 
LBat33861 = batch_normalization_layer(Ave51834, 3, 0.358871567947012, [0.2244, 0.2609], [0.5753, 0.1518], [0.2251, 0.2972], [0.1218, 0.8067], Bat33861), 
LZer48794 = zero_padding2D_layer(Bat33861, 1, 0, 1, 0, Zer48794), 
LCon39732 = concatenate_layer([Zer48794,[[[[0.2667, 0.8864], [0.4727, 0.7345], [0.2038, 0.9385]], [[0.3469, 0.1774], [0.1852, 0.2868], [0.1413, 0.4918]], [[0.5049, 0.3134], [0.0868, 0.5713], [0.0486, 0.3668]]]]], 3, Con39732), 
LMax29146 = maximum_layer([Zer48736,Con39732], Max29146), 
exec_layers([LZer48736,LAve51834,LBat33861,LZer48794,LCon39732,LMax29146],["Zer48736","Ave51834","Bat33861","Zer48794","Con39732","Max29146"],Max29146,"Max29146")

Actual (Unparsed): [[[[0.0000000, 0.0000000, 0.2667000, 0.8864000], [0.0000000, 0.0000000, 0.4727000, 0.7345000], [0.0000000, 0.0000000, 0.2038000, 0.9385000]], [[0.0000000, 0.0000000, 0.3469000, 0.1774000], [1.4188000, 1.3781000, 1.2464000, 1.1350000], [0.6407779, 0.2183411, 0.1413000, 0.4918000]], [[0.0000000, 0.0000000, 0.5049000, 0.3134000], [0.6874183, 0.1188134, 0.0868000, 0.5713000], [0.6192378, 0.1365271, 0.0486000, 0.3668000]]]]

Expected (Unparsed): [[[[0,0,0.2667,0.8864],[0,0,0.4727,0.7345],[0,0,0.2038,0.9385]],[[0,0,0.3469,0.1774],[1.4188,1.3781,1.2464,1.135],[0.6407778662279635,0.21834105578488283,0.1413,0.4918]],[[0,0,0.5049,0.3134],[0.6874183038129835,0.11881342249995819,0.0868,0.5713],[0.6192378168089276,0.13652709378752642,0.0486,0.3668]]]]

Actual:   [[[[0, 0, 0.2667, 0.8864], [0, 0, 0.4727, 0.7345], [0, 0, 0.2038, 0.9385]], [[0, 0, 0.3469, 0.1774], [1.4188, 1.3781, 1.2464, 1.135], [0.6408, 0.2184, 0.1413, 0.4918]], [[0, 0, 0.5049, 0.3134], [0.6875, 0.1189, 0.0868, 0.5713], [0.6193, 0.1366, 0.0486, 0.3668]]]]

Expected: [[[[0, 0, 0.2667, 0.8864], [0, 0, 0.4727, 0.7345], [0, 0, 0.2038, 0.9385]], [[0, 0, 0.3469, 0.1774], [1.4188, 1.3781, 1.2464, 1.135], [0.6408, 0.2184, 0.1413, 0.4918]], [[0, 0, 0.5049, 0.3134], [0.6875, 0.1189, 0.0868, 0.5713], [0.6193, 0.1366, 0.0486, 0.3668]]]]