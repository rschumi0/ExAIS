import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Lay68612 = tf.keras.layers.Input(shape=([3, 4]))
in0Glo27113 = tf.keras.layers.Input(shape=([1, 1, 1]))
in0PRe46952 = tf.keras.layers.Input(shape=([1, 1]))
in0Con70650 = tf.keras.layers.Input(shape=([11]))

Lay68612 = keras.layers.LayerNormalization(axis=2, epsilon=2.6706980307255024, name = 'Lay68612', )(in0Lay68612)
Fla42676 = keras.layers.Flatten(name = 'Fla42676', )(Lay68612)
Glo27113 = keras.layers.GlobalMaxPool2D(name = 'Glo27113', )(in0Glo27113)
Sof24016 = keras.layers.Softmax(axis=1, name = 'Sof24016', )(Glo27113)
PRe46952 = keras.layers.PReLU(name = 'PRe46952', input_shape=(1, 1))(in0PRe46952)
Fla70587 = keras.layers.Flatten(name = 'Fla70587', )(PRe46952)
Sub77752 = keras.layers.Subtract(name = 'Sub77752', )([Sof24016,Fla70587])
Lea75652 = keras.layers.LeakyReLU(alpha=7.322821443695608, name = 'Lea75652', )(Sub77752)
Con70650 = keras.layers.Concatenate(axis=1, name = 'Con70650', )([Lea75652,in0Con70650])
Max34802 = keras.layers.Maximum(name = 'Max34802', )([Fla42676,Con70650])
model = tf.keras.models.Model(inputs=[in0Lay68612,in0Glo27113,in0PRe46952,in0Con70650], outputs=Max34802)
w = model.get_layer('PRe46952').get_weights() 
w[0] = np.array([[0.8271]])
model.get_layer('PRe46952').set_weights(w) 
in0Lay68612 = tf.constant([[[1.7911, 1.1015, 1.4214, 1.3383], [1.7214, 1.2422, 1.2274, 1.6937], [1.5047, 1.3211, 1.7404, 1.2199]]])
in0Glo27113 = tf.constant([[[[1.3625]]]])
in0PRe46952 = tf.constant([[[0.5841]]])
in0Con70650 = tf.constant([[0.5364, 0.1723, 0.0621, 0.1714, 0.4507, 0.3336, 0.5675, 0.6069, 0.0396, 0.8165, 0.9557]])
print (np.array2string(model.predict([in0Lay68612,in0Glo27113,in0PRe46952,in0Con70650],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Max34802.png')

LLay68612 = layer_normalization_layer([[[1.7911, 1.1015, 1.4214, 1.3383], [1.7214, 1.2422, 1.2274, 1.6937], [1.5047, 1.3211, 1.7404, 1.2199]]], 2, 2.6706980307255024, Lay68612), 
LFla42676 = flatten_layer(Lay68612, Fla42676), 
LGlo27113 = global_max_pool2D_layer([[[[1.3625]]]], Glo27113), 
LSof24016 = softmax_layer(Glo27113, 1, Sof24016), 
LPRe46952 = prelu_layer([[[0.5841]]], [[0.8271]], PRe46952), 
LFla70587 = flatten_layer(PRe46952, Fla70587), 
LSub77752 = subtract_layer(Sof24016,Fla70587, Sub77752), 
LLea75652 = leaky_relu_layer(Sub77752, 7.322821443695608, Lea75652), 
LCon70650 = concatenate_layer([Lea75652,[[0.5364, 0.1723, 0.0621, 0.1714, 0.4507, 0.3336, 0.5675, 0.6069, 0.0396, 0.8165, 0.9557]]], 1, Con70650), 
LMax34802 = maximum_layer([Fla42676,Con70650], Max34802), 
exec_layers([LLay68612,LFla42676,LGlo27113,LSof24016,LPRe46952,LFla70587,LSub77752,LLea75652,LCon70650,LMax34802],["Lay68612","Fla42676","Glo27113","Sof24016","PRe46952","Fla70587","Sub77752","Lea75652","Con70650","Max34802"],Max34802,"Max34802")

Actual (Unparsed): [[0.4159000, 0.5364000, 0.1723000, 0.0621000, 0.1714000, 0.4507000, 0.3336000, 0.5675000, 0.6069000, 0.0396000, 0.8165000, 0.9557000]]

Expected (Unparsed): [[0.41590000000000005,0.5364,0.1723,0.0621,0.1714,0.4507,0.3336,0.5675,0.6069,0.0396,0.8165,0.9557]]

Actual:   [[0.4159, 0.5364, 0.1723, 0.0621, 0.1714, 0.4507, 0.3336, 0.5675, 0.6069, 0.0396, 0.8165, 0.9557]]

Expected: [[0.416, 0.5364, 0.1723, 0.0621, 0.1714, 0.4507, 0.3336, 0.5675, 0.6069, 0.0396, 0.8165, 0.9557]]