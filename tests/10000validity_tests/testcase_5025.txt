import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Lea64755 = tf.keras.layers.Input(shape=([1, 2, 2]))
in0Con35824 = tf.keras.layers.Input(shape=([1, 2]))
in0Max26323 = tf.keras.layers.Input(shape=([1, 2, 2]))
in1Max26323 = tf.keras.layers.Input(shape=([1, 2, 2]))
in0Con76084 = tf.keras.layers.Input(shape=([4, 4]))
in0Con37368 = tf.keras.layers.Input(shape=([2, 1, 2]))

Lea64755 = keras.layers.LeakyReLU(alpha=0.654322330705731, name = 'Lea64755', input_shape=(1, 2, 2))(in0Lea64755)
Dep52690 = keras.layers.DepthwiseConv2D((1, 2),strides=(2, 2), padding='valid', name = 'Dep52690', )(Lea64755)
Res12996 = keras.layers.Reshape((1, 2), name = 'Res12996', )(Dep52690)
Con35824 = keras.layers.Concatenate(axis=2, name = 'Con35824', )([Res12996,in0Con35824])
Max26323 = keras.layers.Maximum(name = 'Max26323', )([in0Max26323,in1Max26323])
Res52434 = keras.layers.Reshape((1, 4), name = 'Res52434', )(Max26323)
Dot33664 = keras.layers.Dot(axes=(1, 1), name = 'Dot33664', )([Con35824,Res52434])
Con76084 = keras.layers.Concatenate(axis=2, name = 'Con76084', )([Dot33664,in0Con76084])
Con37368 = keras.layers.Conv2DTranspose(4, (2, 1),strides=(1, 2), padding='same', name = 'Con37368', )(in0Con37368)
Res63345 = keras.layers.Reshape((2, 8), name = 'Res63345', )(Con37368)
Zer34741 = keras.layers.ZeroPadding1D(padding=((2, 0)), name = 'Zer34741', )(Res63345)
Dot14497 = keras.layers.Dot(axes=(2, 2), name = 'Dot14497', )([Con76084,Zer34741])
Fla71251 = keras.layers.Flatten(name = 'Fla71251', )(Dot14497)
model = tf.keras.models.Model(inputs=[in0Lea64755,in0Con35824,in0Max26323,in1Max26323,in0Con76084,in0Con37368], outputs=Fla71251)
w = model.get_layer('Dep52690').get_weights() 
w[0] = np.array([[[[0.0909], [0.9376]], [[0.3388], [0.2605]]]])
w[1] = np.array([0, 0])
model.get_layer('Dep52690').set_weights(w) 
w = model.get_layer('Con37368').get_weights() 
w[0] = np.array([[[[0.624, 0.1024], [0.6103, 0.7694], [0.5272, 0.1091], [0.5083, 0.6972]]], [[[0.912, 0.9621], [0.8226, 0.5887], [0.5698, 0.1472], [0.4302, 0.0734]]]])
w[1] = np.array([0, 0, 0, 0])
model.get_layer('Con37368').set_weights(w) 
in0Lea64755 = tf.constant([[[[0.6376, 0.4991], [0.9652, 0.2372]]]])
in0Con35824 = tf.constant([[[0.7958, 0.7731]]])
in0Max26323 = tf.constant([[[[0.0835, 0.4059], [0.9712, 0.3943]]]])
in1Max26323 = tf.constant([[[[0.5357, 0.9678], [0.7087, 0.0687]]]])
in0Con76084 = tf.constant([[[0.0076, 0.0309, 0.4986, 0.1437], [0.0508, 0.7638, 0.9069, 0.3627], [0.1235, 0.3271, 0.4049, 0.969], [0.4882, 0.3028, 0.2378, 0.3505]]])
in0Con37368 = tf.constant([[[[0.5885, 0.5245]], [[0.907, 0.418]]]])
print (np.array2string(model.predict([in0Lea64755,in0Con35824,in0Max26323,in1Max26323,in0Con76084,in0Con37368],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Fla71251.png')

LLea64755 = leaky_relu_layer([[[[0.6376, 0.4991], [0.9652, 0.2372]]]], 0.654322330705731, Lea64755), 
LDep52690 = depthwise_conv2D_layer(Lea64755, 1, 2,[[[[0.0909], [0.9376]], [[0.3388], [0.2605]]]],[0, 0], 2, 2, false, Dep52690), 
LRes12996 = reshape_layer(Dep52690, [1, 2], Res12996), 
LCon35824 = concatenate_layer([Res12996,[[[0.7958, 0.7731]]]], 2, Con35824), 
LMax26323 = maximum_layer([[[[[0.0835, 0.4059], [0.9712, 0.3943]]]], [[[[0.5357, 0.9678], [0.7087, 0.0687]]]]], Max26323), 
LRes52434 = reshape_layer(Max26323, [1, 4], Res52434), 
LDot33664 = dot_layer(Con35824,Res52434, 1, 1, Dot33664), 
LCon76084 = concatenate_layer([Dot33664,[[[0.0076, 0.0309, 0.4986, 0.1437], [0.0508, 0.7638, 0.9069, 0.3627], [0.1235, 0.3271, 0.4049, 0.969], [0.4882, 0.3028, 0.2378, 0.3505]]]], 2, Con76084), 
LCon37368 = conv2D_transpose_layer([[[[0.5885, 0.5245]], [[0.907, 0.418]]]], 2, 1,[[[[0.624, 0.1024], [0.6103, 0.7694], [0.5272, 0.1091], [0.5083, 0.6972]]], [[[0.912, 0.9621], [0.8226, 0.5887], [0.5698, 0.1472], [0.4302, 0.0734]]]],[0, 0, 0, 0], 1, 2, true, Con37368), 
LRes63345 = reshape_layer(Con37368, [2, 8], Res63345), 
LZer34741 = zero_padding1D_layer(Res63345, 2, 0, Zer34741), 
LDot14497 = dot_layer(Con76084,Zer34741, 2, 2, Dot14497), 
LFla71251 = flatten_layer(Dot14497, Fla71251), 
exec_layers([LLea64755,LDep52690,LRes12996,LCon35824,LMax26323,LRes52434,LDot33664,LCon76084,LCon37368,LRes63345,LZer34741,LDot14497,LFla71251],["Lea64755","Dep52690","Res12996","Con35824","Max26323","Res52434","Dot33664","Con76084","Con37368","Res63345","Zer34741","Dot14497","Fla71251"],Fla71251,"Fla71251")

Actual (Unparsed): [[0.0000000, 0.0000000, 0.6092805, 1.4703135, 0.0000000, 0.0000000, 0.8384196, 2.0232711, 0.0000000, 0.0000000, 1.2594967, 3.0394128, 0.0000000, 0.0000000, 1.2235699, 2.9527145]]

Expected (Unparsed): [[0.0,0.0,0.6092804750315579,1.4703134528794848,0.0,0.0,0.8384195386292994,2.0232710177358295,0.0,0.0,1.259496648627349,3.039412786430583,0.0,0.0,1.2235698153478305,2.9527142814645435]]

Actual:   [[0, 0, 0.6093, 1.4704, 0, 0, 0.8385, 2.0233, 0, 0, 1.2595, 3.0395, 0, 0, 1.2236, 2.9528]]

Expected: [[0, 0, 0.6093, 1.4704, 0, 0, 0.8385, 2.0233, 0, 0, 1.2595, 3.0395, 0, 0, 1.2236, 2.9528]]