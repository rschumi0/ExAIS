import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Min87997 = tf.keras.layers.Input(shape=([1, 1]))
in1Min87997 = tf.keras.layers.Input(shape=([1, 1]))
in0Con65999 = tf.keras.layers.Input(shape=([3]))
in0Con93983 = tf.keras.layers.Input(shape=([2, 1]))

Min87997 = keras.layers.Minimum(name = 'Min87997', )([in0Min87997,in1Min87997])
Bat74146 = keras.layers.BatchNormalization(axis=1, epsilon=0.816784940097824,  name = 'Bat74146', )(Min87997)
Thr67541 = keras.layers.ThresholdedReLU(theta=2.670773490570701, name = 'Thr67541', )(Bat74146)
Fla32095 = keras.layers.Flatten(name = 'Fla32095', )(Thr67541)
Con65999 = keras.layers.Concatenate(axis=1, name = 'Con65999', )([Fla32095,in0Con65999])
Con93983 = keras.layers.Conv1D(4, (2),strides=(1), padding='valid', dilation_rate=(1), name = 'Con93983', )(in0Con93983)
Glo44015 = keras.layers.GlobalAveragePooling1D(name = 'Glo44015', )(Con93983)
Dot62564 = keras.layers.Dot(axes=(1, 1), name = 'Dot62564', )([Con65999,Glo44015])
model = tf.keras.models.Model(inputs=[in0Min87997,in1Min87997,in0Con65999,in0Con93983], outputs=Dot62564)
w = model.get_layer('Bat74146').get_weights() 
w[0] = np.array([0.0364])
w[1] = np.array([0.7458])
w[2] = np.array([0.2806])
w[3] = np.array([0.3093])
model.get_layer('Bat74146').set_weights(w) 
w = model.get_layer('Con93983').get_weights() 
w[0] = np.array([[[0.3002, 0.9581, 0.3099, 0.4034]], [[0.6322, 0.3742, 0.5269, 0.5536]]])
w[1] = np.array([0, 0, 0, 0])
model.get_layer('Con93983').set_weights(w) 
in0Min87997 = tf.constant([[[0.7767]]])
in1Min87997 = tf.constant([[[0.7866]]])
in0Con65999 = tf.constant([[0.6436, 0.8526, 0.0687]])
in0Con93983 = tf.constant([[[0.3925], [0.0948]]])
print (np.array2string(model.predict([in0Min87997,in1Min87997,in0Con65999,in0Con93983],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Dot62564.png')

LMin87997 = minimum_layer([[[[0.7767]]], [[[0.7866]]]], Min87997), 
LBat74146 = batch_normalization_layer(Min87997, 1, 0.816784940097824, [0.0364], [0.7458], [0.2806], [0.3093], Bat74146), 
LThr67541 = thresholded_relu_layer(Bat74146, 2.670773490570701, Thr67541), 
LFla32095 = flatten_layer(Thr67541, Fla32095), 
LCon65999 = concatenate_layer([Fla32095,[[0.6436, 0.8526, 0.0687]]], 1, Con65999), 
LCon93983 = conv1D_layer([[[0.3925], [0.0948]]], 2,[[[0.3002, 0.9581, 0.3099, 0.4034]], [[0.6322, 0.3742, 0.5269, 0.5536]]],[0, 0, 0, 0], 1, false, 1, Con93983), 
LGlo44015 = global_average_pooling1D_layer(Con93983, Glo44015), 
LDot62564 = dot_layer(Con65999,Glo44015, 1, 1, Dot62564), 
exec_layers([LMin87997,LBat74146,LThr67541,LFla32095,LCon65999,LCon93983,LGlo44015,LDot62564],["Min87997","Bat74146","Thr67541","Fla32095","Con65999","Con93983","Glo44015","Dot62564"],Dot62564,"Dot62564")

Actual (Unparsed): [[0.4256368]]

Expected (Unparsed): [[0.42563684152399994]]

Actual:   [[0.4257]]

Expected: [[0.4257]]