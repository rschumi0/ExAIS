import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Fla27898 = tf.keras.layers.Input(shape=([3, 3, 3]))
in0Ave28379 = tf.keras.layers.Input(shape=([2, 2]))
in1Ave28379 = tf.keras.layers.Input(shape=([2, 2]))

Fla27898 = keras.layers.Flatten(name = 'Fla27898', )(in0Fla27898)
Res205 = keras.layers.Reshape((27, 1), name = 'Res205', )(Fla27898)
PRe41601 = keras.layers.PReLU(name = 'PRe41601', )(Res205)
Res88397 = keras.layers.Reshape((27, 1, 1), name = 'Res88397', )(PRe41601)
Con71024 = keras.layers.Conv2D(2, (3, 1),strides=(1, 1), padding='valid', dilation_rate=(1, 1), name = 'Con71024', )(Res88397)
Res46839 = keras.layers.Reshape((25, 2), name = 'Res46839', )(Con71024)
Ave28379 = keras.layers.Average(name = 'Ave28379', )([in0Ave28379,in1Ave28379])
Thr87776 = keras.layers.ThresholdedReLU(theta=6.401557795363782, name = 'Thr87776', )(Ave28379)
Zer53663 = keras.layers.ZeroPadding1D(padding=((23, 0)), name = 'Zer53663', )(Thr87776)
Sub31394 = keras.layers.Subtract(name = 'Sub31394', )([Res46839,Zer53663])
model = tf.keras.models.Model(inputs=[in0Fla27898,in0Ave28379,in1Ave28379], outputs=Sub31394)
w = model.get_layer('PRe41601').get_weights() 
w[0] = np.array([[0.7182], [0.3898], [0.1167], [0.8895], [0.7225], [0.0487], [0.7518], [0.4846], [0.0294], [0.8791], [0.3456], [0.0346], [0.2988], [0.2432], [0.5007], [0.7311], [0.3718], [0.4336], [0.1469], [0.6425], [0.5092], [0.6478], [0.204], [0.9111], [0.4491], [0.3519], [0.2205]])
model.get_layer('PRe41601').set_weights(w) 
w = model.get_layer('Con71024').get_weights() 
w[0] = np.array([[[[0.8902, 0.2363]]], [[[0.0539, 0.7326]]], [[[0.9565, 0.5544]]]])
w[1] = np.array([0, 0])
model.get_layer('Con71024').set_weights(w) 
in0Fla27898 = tf.constant([[[[1.1794, 1.765, 1.1765], [1.3276, 1.8606, 1.4698], [1.3285, 1.4533, 1.2486]], [[1.4495, 1.9791, 1.3094], [1.756, 1.5186, 1.3135], [1.3975, 1.967, 1.3472]], [[1.9263, 1.1201, 1.8344], [1.4247, 1.3339, 1.6504], [1.7903, 1.6165, 1.271]]]])
in0Ave28379 = tf.constant([[[0.2735, 0.9673], [0.5054, 0.6021]]])
in1Ave28379 = tf.constant([[[0.1386, 0.0007], [0.0222, 0.0906]]])
print (np.array2string(model.predict([in0Fla27898,in0Ave28379,in1Ave28379],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Sub31394.png')

LFla27898 = flatten_layer([[[[1.1794, 1.765, 1.1765], [1.3276, 1.8606, 1.4698], [1.3285, 1.4533, 1.2486]], [[1.4495, 1.9791, 1.3094], [1.756, 1.5186, 1.3135], [1.3975, 1.967, 1.3472]], [[1.9263, 1.1201, 1.8344], [1.4247, 1.3339, 1.6504], [1.7903, 1.6165, 1.271]]]], Fla27898), 
LRes205 = reshape_layer(Fla27898, [27, 1], Res205), 
LPRe41601 = prelu_layer(Res205, [[0.7182], [0.3898], [0.1167], [0.8895], [0.7225], [0.0487], [0.7518], [0.4846], [0.0294], [0.8791], [0.3456], [0.0346], [0.2988], [0.2432], [0.5007], [0.7311], [0.3718], [0.4336], [0.1469], [0.6425], [0.5092], [0.6478], [0.204], [0.9111], [0.4491], [0.3519], [0.2205]], PRe41601), 
LRes88397 = reshape_layer(PRe41601, [27, 1, 1], Res88397), 
LCon71024 = conv2D_layer(Res88397, 3, 1,[[[[0.8902, 0.2363]]], [[[0.0539, 0.7326]]], [[[0.9565, 0.5544]]]],[0, 0], 1, 1, false, 1, 1, Con71024), 
LRes46839 = reshape_layer(Con71024, [25, 2], Res46839), 
LAve28379 = average_layer([[[[0.2735, 0.9673], [0.5054, 0.6021]]], [[[0.1386, 0.0007], [0.0222, 0.0906]]]], Ave28379), 
LThr87776 = thresholded_relu_layer(Ave28379, 6.401557795363782, Thr87776), 
LZer53663 = zero_padding1D_layer(Thr87776, 23, 0, Zer53663), 
LSub31394 = subtract_layer(Res46839,Zer53663, Sub31394), 
exec_layers([LFla27898,LRes205,LPRe41601,LRes88397,LCon71024,LRes46839,LAve28379,LThr87776,LZer53663,LSub31394],["Fla27898","Res205","PRe41601","Res88397","Con71024","Res46839","Ave28379","Thr87776","Zer53663","Sub31394"],Sub31394,"Sub31394")

Actual (Unparsed): [[[2.2703576, 2.2239828], [2.9044657, 2.0149948], [2.8985418, 2.2821233], [2.6879796, 2.4916446], [3.0062386, 2.2529557], [2.7701036, 2.1262824], [2.4552495, 2.0708360], [2.7474739, 2.0617419], [3.0826409, 2.4541609], [2.6494594, 2.5183368], [3.5119855, 2.4004542], [2.7128171, 2.4377687], [2.9014066, 2.2556736], [2.7593641, 2.0958893], [3.1260385, 2.4246934], [2.6386727, 2.5181412], [3.6661435, 2.5197016], [2.3744807, 2.3505342], [3.5297693, 2.2927614], [2.4587128, 2.3984148], [2.9856496, 2.2167181], [2.9187728, 2.2288535], [2.9888163, 2.5168260], [3.1118656, 2.5977509], [2.8965659, 2.3119382]]]

Expected (Unparsed): [[[2.2703576300000003,2.22398282],[2.90446575,2.01499484],[2.89854184,2.28212335],[2.68797956,2.49164456],[3.00623859,2.25295566],[2.77010356,2.1262823600000003],[2.45524947,2.07083597],[2.74747395,2.06174195],[3.0826409200000002,2.4541609199999996],[2.64945949,2.5183368699999997],[3.51198548,2.40045417],[2.71281718,2.4377686599999997],[2.90140649,2.25567356],[2.75936412,2.0958892799999997],[3.12603845,2.42469335],[2.6386726,2.51814113],[3.66614343,2.51970154],[2.3744806599999997,2.3505341800000004],[3.52976925,2.2927613100000004],[2.4587127300000002,2.39841475],[2.98564956,2.2167181],[2.9187727500000005,2.2288535100000004],[2.98881629,2.51682593],[3.1118655,2.5977509000000003],[2.8965659099999996,2.3119381900000002]]]

Actual:   [[[2.2704, 2.224], [2.9045, 2.015], [2.8986, 2.2822], [2.688, 2.4917], [3.0063, 2.253], [2.7702, 2.1263], [2.4553, 2.0709], [2.7475, 2.0618], [3.0827, 2.4542], [2.6495, 2.5184], [3.512, 2.4005], [2.7129, 2.4378], [2.9015, 2.2557], [2.7594, 2.0959], [3.1261, 2.4247], [2.6387, 2.5182], [3.6662, 2.5198], [2.3745, 2.3506], [3.5298, 2.2928], [2.4588, 2.3985], [2.9857, 2.2168], [2.9188, 2.2289], [2.9889, 2.5169], [3.1119, 2.5978], [2.8966, 2.312]]]

Expected: [[[2.2704, 2.224], [2.9045, 2.015], [2.8986, 2.2822], [2.688, 2.4917], [3.0063, 2.253], [2.7702, 2.1263], [2.4553, 2.0709], [2.7475, 2.0618], [3.0827, 2.4542], [2.6495, 2.5184], [3.512, 2.4005], [2.7129, 2.4378], [2.9015, 2.2557], [2.7594, 2.0959], [3.1261, 2.4247], [2.6387, 2.5182], [3.6662, 2.5198], [2.3745, 2.3506], [3.5298, 2.2928], [2.4588, 2.3985], [2.9857, 2.2168], [2.9188, 2.2289], [2.9889, 2.5169], [3.1119, 2.5978], [2.8966, 2.312]]]