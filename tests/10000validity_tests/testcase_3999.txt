import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Per61721 = tf.keras.layers.Input(shape=([3, 3]))
in0Con87027 = tf.keras.layers.Input(shape=([3, 3, 2]))
in0Max77972 = tf.keras.layers.Input(shape=([2, 1, 2]))
in0Glo48264 = tf.keras.layers.Input(shape=([1, 2, 1]))
in0Con75437 = tf.keras.layers.Input(shape=([26]))

Per61721 = keras.layers.Permute((2,1), name = 'Per61721',)(in0Per61721)
Res8322 = keras.layers.Reshape((3, 3, 1), name = 'Res8322', )(Per61721)
Con87027 = keras.layers.Concatenate(axis=3, name = 'Con87027', )([Res8322,in0Con87027])
Max77972 = keras.layers.MaxPool2D(pool_size=(1, 1), strides=(1, 1), padding='valid', name = 'Max77972', )(in0Max77972)
Sep33261 = keras.layers.SeparableConv2D(3, (1, 1),strides=(1, 1), padding='valid', name = 'Sep33261', )(Max77972)
Zer27755 = keras.layers.ZeroPadding2D(padding=((1, 0), (2, 0)), name = 'Zer27755', )(Sep33261)
Ave76768 = keras.layers.Average(name = 'Ave76768', )([Con87027,Zer27755])
Res60717 = keras.layers.Reshape((3, 9), name = 'Res60717', )(Ave76768)
Fla68992 = keras.layers.Flatten(name = 'Fla68992', )(Res60717)
Glo48264 = keras.layers.GlobalAveragePooling2D(name = 'Glo48264', )(in0Glo48264)
Con75437 = keras.layers.Concatenate(axis=1, name = 'Con75437', )([Glo48264,in0Con75437])
Ave23432 = keras.layers.Average(name = 'Ave23432', )([Fla68992,Con75437])
Res37268 = keras.layers.Reshape((27, 1), name = 'Res37268', )(Ave23432)
Res60266 = keras.layers.Reshape((27, 1, 1), name = 'Res60266', )(Res37268)
Up_92970 = keras.layers.UpSampling2D(size=(2, 2), name = 'Up_92970', )(Res60266)
model = tf.keras.models.Model(inputs=[in0Per61721,in0Con87027,in0Max77972,in0Glo48264,in0Con75437], outputs=Up_92970)
w = model.get_layer('Sep33261').get_weights() 
w[0] = np.array([[[[0.9645], [0.119]]]])
w[1] = np.array([[[[0.2755, 0.2521, 0.2554], [0.8066, 0.1665, 0.1584]]]])
w[2] = np.array([0, 0, 0])
model.get_layer('Sep33261').set_weights(w) 
in0Per61721 = tf.constant([[[1.3222, 1.2088, 1.9249], [1.999, 1.9998, 1.396], [1.6076, 1.3186, 1.685]]])
in0Con87027 = tf.constant([[[[0.7145, 0.241], [0.4477, 0.8315], [0.7317, 0.83]], [[0.969, 0.5072], [0.211, 0.1333], [0.7315, 0.3186]], [[0.8644, 0.5251], [0.6748, 0.4515], [0.9233, 0.2764]]]])
in0Max77972 = tf.constant([[[[1.3545, 1.9489]], [[1.2003, 1.1997]]]])
in0Glo48264 = tf.constant([[[[1.8795], [1.1987]]]])
in0Con75437 = tf.constant([[0.523, 0.2765, 0.5787, 0.4555, 0.6835, 0.4332, 0.3806, 0.7961, 0.2676, 0.215, 0.7416, 0.933, 0.0215, 0.3396, 0.2486, 0.1973, 0.2037, 0.6238, 0.6274, 0.029, 0.4401, 0.4548, 0.6865, 0.1062, 0.5927, 0.7775]])
print (np.array2string(model.predict([in0Per61721,in0Con87027,in0Max77972,in0Glo48264,in0Con75437],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Up_92970.png')

LPer61721 = permute_layer([[[1.3222, 1.2088, 1.9249], [1.999, 1.9998, 1.396], [1.6076, 1.3186, 1.685]]], 2,1, Per61721), 
LRes8322 = reshape_layer(Per61721, [3, 3, 1], Res8322), 
LCon87027 = concatenate_layer([Res8322,[[[[0.7145, 0.241], [0.4477, 0.8315], [0.7317, 0.83]], [[0.969, 0.5072], [0.211, 0.1333], [0.7315, 0.3186]], [[0.8644, 0.5251], [0.6748, 0.4515], [0.9233, 0.2764]]]]], 3, Con87027), 
LMax77972 = max_pool2D_layer([[[[1.3545, 1.9489]], [[1.2003, 1.1997]]]], 1, 1, 1, 1, false, Max77972), 
LSep33261 = separable_conv2D_layer(Max77972, 1, 1,[[[[[0.9645], [0.119]]]],[[[[0.2755, 0.2521, 0.2554], [0.8066, 0.1665, 0.1584]]]]],[0, 0, 0], 1, 1, false, Sep33261), 
LZer27755 = zero_padding2D_layer(Sep33261, 1, 0, 2, 0, Zer27755), 
LAve76768 = average_layer([Con87027,Zer27755], Ave76768), 
LRes60717 = reshape_layer(Ave76768, [3, 9], Res60717), 
LFla68992 = flatten_layer(Res60717, Fla68992), 
LGlo48264 = global_average_pooling2D_layer([[[[1.8795], [1.1987]]]], Glo48264), 
LCon75437 = concatenate_layer([Glo48264,[[0.523, 0.2765, 0.5787, 0.4555, 0.6835, 0.4332, 0.3806, 0.7961, 0.2676, 0.215, 0.7416, 0.933, 0.0215, 0.3396, 0.2486, 0.1973, 0.2037, 0.6238, 0.6274, 0.029, 0.4401, 0.4548, 0.6865, 0.1062, 0.5927, 0.7775]]], 1, Con75437), 
LAve23432 = average_layer([Fla68992,Con75437], Ave23432), 
LRes37268 = reshape_layer(Ave23432, [27, 1], Res37268), 
LRes60266 = reshape_layer(Res37268, [27, 1, 1], Res60266), 
LUp_92970 = up_sampling2D_layer(Res60266, 2, 2, Up_92970), 
exec_layers([LPer61721,LRes8322,LCon87027,LMax77972,LSep33261,LZer27755,LAve76768,LRes60717,LFla68992,LGlo48264,LCon75437,LAve23432,LRes37268,LRes60266,LUp_92970],["Per61721","Res8322","Con87027","Max77972","Sep33261","Zer27755","Ave76768","Res60717","Fla68992","Glo48264","Con75437","Ave23432","Res37268","Res60266","Up_92970"],Up_92970,"Up_92970")

Actual (Unparsed): [[[[1.1001000], [1.1001000]], [[1.1001000], [1.1001000]], [[0.4401250], [0.4401250]], [[0.4401250], [0.4401250]], [[0.1985000], [0.1985000]], [[0.1985000], [0.1985000]], [[0.7891000], [0.7891000]], [[0.7891000], [0.7891000]], [[0.3396750], [0.3396750]], [[0.3396750], [0.3396750]], [[0.5496250], [0.5496250]], [[0.5496250], [0.5496250]], [[0.6185000], [0.6185000]], [[0.6185000], [0.6185000]], [[0.3732250], [0.3732250]], [[0.3732250], [0.3732250]], [[0.6055500], [0.6055500]], [[0.6055500], [0.6055500]], [[0.4360000], [0.4360000]], [[0.4360000], [0.4360000]], [[0.3497500], [0.3497500]], [[0.3497500], [0.3497500]], [[0.4976000], [0.4976000]], [[0.4976000], [0.4976000]], [[0.9664500], [0.9664500]], [[0.9664500], [0.9664500]], [[0.0635000], [0.0635000]], [[0.0635000], [0.0635000]], [[0.2031250], [0.2031250]], [[0.2031250], [0.2031250]], [[0.5906959], [0.5906959]], [[0.5906959], [0.5906959]], [[0.3735155], [0.3735155]], [[0.3735155], [0.3735155]], [[0.2740986], [0.2740986]], [[0.2740986], [0.2740986]], [[0.7931250], [0.7931250]], [[0.7931250], [0.7931250]], [[0.5298000], [0.5298000]], [[0.5298000], [0.5298000]], [[0.1457750], [0.1457750]], [[0.1457750], [0.1457750]], [[0.5690500], [0.5690500]], [[0.5690500], [0.5690500]], [[0.3961000], [0.3961000]], [[0.3961000], [0.3961000]], [[0.4561250], [0.4561250]], [[0.4561250], [0.4561250]], [[0.5828743], [0.5828743]], [[0.5828743], [0.5828743]], [[0.6060809], [0.6060809]], [[0.6060809], [0.6060809]], [[0.5374219], [0.5374219]], [[0.5374219], [0.5374219]]]]

Expected (Unparsed): [[[[1.1000999999999999],[1.1000999999999999]],[[1.1000999999999999],[1.1000999999999999]],[[0.440125],[0.440125]],[[0.440125],[0.440125]],[[0.1985],[0.1985]],[[0.1985],[0.1985]],[[0.7891],[0.7891]],[[0.7891],[0.7891]],[[0.339675],[0.339675]],[[0.339675],[0.339675]],[[0.549625],[0.549625]],[[0.549625],[0.549625]],[[0.6184999999999999],[0.6184999999999999]],[[0.6184999999999999],[0.6184999999999999]],[[0.37322500000000003],[0.37322500000000003]],[[0.37322500000000003],[0.37322500000000003]],[[0.60555],[0.60555]],[[0.60555],[0.60555]],[[0.43600000000000005],[0.43600000000000005]],[[0.43600000000000005],[0.43600000000000005]],[[0.34975],[0.34975]],[[0.34975],[0.34975]],[[0.49760000000000004],[0.49760000000000004]],[[0.49760000000000004],[0.49760000000000004]],[[0.96645],[0.96645]],[[0.96645],[0.96645]],[[0.0635],[0.0635]],[[0.0635],[0.0635]],[[0.203125],[0.203125]],[[0.203125],[0.203125]],[[0.59069583685875],[0.59069583685875]],[[0.59069583685875],[0.59069583685875]],[[0.37351545366875005],[0.37351545366875005]],[[0.37351545366875005],[0.37351545366875005]],[[0.2740986100725],[0.2740986100725]],[[0.2740986100725],[0.2740986100725]],[[0.7931250000000001],[0.7931250000000001]],[[0.7931250000000001],[0.7931250000000001]],[[0.5297999999999999],[0.5297999999999999]],[[0.5297999999999999],[0.5297999999999999]],[[0.14577500000000002],[0.14577500000000002]],[[0.14577500000000002],[0.14577500000000002]],[[0.56905],[0.56905]],[[0.56905],[0.56905]],[[0.3961],[0.3961]],[[0.3961],[0.3961]],[[0.456125],[0.456125]],[[0.456125],[0.456125]],[[0.58287427507625],[0.58287427507625]],[[0.58287427507625],[0.58287427507625]],[[0.60608093527125],[0.60608093527125]],[[0.60608093527125],[0.60608093527125]],[[0.5374219312774999],[0.5374219312774999]],[[0.5374219312774999],[0.5374219312774999]]]]

Actual:   [[[[1.1001], [1.1001]], [[1.1001], [1.1001]], [[0.4402], [0.4402]], [[0.4402], [0.4402]], [[0.1985], [0.1985]], [[0.1985], [0.1985]], [[0.7891], [0.7891]], [[0.7891], [0.7891]], [[0.3397], [0.3397]], [[0.3397], [0.3397]], [[0.5497], [0.5497]], [[0.5497], [0.5497]], [[0.6185], [0.6185]], [[0.6185], [0.6185]], [[0.3733], [0.3733]], [[0.3733], [0.3733]], [[0.6056], [0.6056]], [[0.6056], [0.6056]], [[0.436], [0.436]], [[0.436], [0.436]], [[0.3498], [0.3498]], [[0.3498], [0.3498]], [[0.4976], [0.4976]], [[0.4976], [0.4976]], [[0.9665], [0.9665]], [[0.9665], [0.9665]], [[0.0635], [0.0635]], [[0.0635], [0.0635]], [[0.2032], [0.2032]], [[0.2032], [0.2032]], [[0.5907], [0.5907]], [[0.5907], [0.5907]], [[0.3736], [0.3736]], [[0.3736], [0.3736]], [[0.2741], [0.2741]], [[0.2741], [0.2741]], [[0.7932], [0.7932]], [[0.7932], [0.7932]], [[0.5298], [0.5298]], [[0.5298], [0.5298]], [[0.1458], [0.1458]], [[0.1458], [0.1458]], [[0.5691], [0.5691]], [[0.5691], [0.5691]], [[0.3961], [0.3961]], [[0.3961], [0.3961]], [[0.4562], [0.4562]], [[0.4562], [0.4562]], [[0.5829], [0.5829]], [[0.5829], [0.5829]], [[0.6061], [0.6061]], [[0.6061], [0.6061]], [[0.5375], [0.5375]], [[0.5375], [0.5375]]]]

Expected: [[[[1.1001], [1.1001]], [[1.1001], [1.1001]], [[0.4402], [0.4402]], [[0.4402], [0.4402]], [[0.1985], [0.1985]], [[0.1985], [0.1985]], [[0.7891], [0.7891]], [[0.7891], [0.7891]], [[0.3397], [0.3397]], [[0.3397], [0.3397]], [[0.5497], [0.5497]], [[0.5497], [0.5497]], [[0.6185], [0.6185]], [[0.6185], [0.6185]], [[0.3733], [0.3733]], [[0.3733], [0.3733]], [[0.6056], [0.6056]], [[0.6056], [0.6056]], [[0.4361], [0.4361]], [[0.4361], [0.4361]], [[0.3498], [0.3498]], [[0.3498], [0.3498]], [[0.4977], [0.4977]], [[0.4977], [0.4977]], [[0.9665], [0.9665]], [[0.9665], [0.9665]], [[0.0635], [0.0635]], [[0.0635], [0.0635]], [[0.2032], [0.2032]], [[0.2032], [0.2032]], [[0.5907], [0.5907]], [[0.5907], [0.5907]], [[0.3736], [0.3736]], [[0.3736], [0.3736]], [[0.2741], [0.2741]], [[0.2741], [0.2741]], [[0.7932], [0.7932]], [[0.7932], [0.7932]], [[0.5298], [0.5298]], [[0.5298], [0.5298]], [[0.1458], [0.1458]], [[0.1458], [0.1458]], [[0.5691], [0.5691]], [[0.5691], [0.5691]], [[0.3961], [0.3961]], [[0.3961], [0.3961]], [[0.4562], [0.4562]], [[0.4562], [0.4562]], [[0.5829], [0.5829]], [[0.5829], [0.5829]], [[0.6061], [0.6061]], [[0.6061], [0.6061]], [[0.5375], [0.5375]], [[0.5375], [0.5375]]]]