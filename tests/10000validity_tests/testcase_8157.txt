import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Con14858 = tf.keras.layers.Input(shape=([1, 1, 2, 1]))
in0Sub82973 = tf.keras.layers.Input(shape=([2, 3]))
in1Sub82973 = tf.keras.layers.Input(shape=([2, 3]))
in0Glo24224 = tf.keras.layers.Input(shape=([2, 1, 2]))
in0Con20521 = tf.keras.layers.Input(shape=([4]))
in0Con53981 = tf.keras.layers.Input(shape=([2]))

Con14858 = keras.layers.Conv3D(4, (1, 1, 1),strides=(1, 1, 1), padding='same', dilation_rate=(1, 1, 1), name = 'Con14858', )(in0Con14858)
Res6567 = keras.layers.Reshape((1, 1, 8), name = 'Res6567', )(Con14858)
Res21 = keras.layers.Reshape((1, 8), name = 'Res21', )(Res6567)
Fla58491 = keras.layers.Flatten(name = 'Fla58491', )(Res21)
Sub82973 = keras.layers.Subtract(name = 'Sub82973', )([in0Sub82973,in1Sub82973])
Bat63415 = keras.layers.BatchNormalization(axis=1, epsilon=0.8380487694964077,  name = 'Bat63415', )(Sub82973)
Fla9003 = keras.layers.Flatten(name = 'Fla9003', )(Bat63415)
Glo24224 = keras.layers.GlobalAveragePooling2D(name = 'Glo24224', )(in0Glo24224)
Con20521 = keras.layers.Concatenate(axis=1, name = 'Con20521', )([Glo24224,in0Con20521])
Ave78633 = keras.layers.Average(name = 'Ave78633', )([Fla9003,Con20521])
Con53981 = keras.layers.Concatenate(axis=1, name = 'Con53981', )([Ave78633,in0Con53981])
Min24862 = keras.layers.Minimum(name = 'Min24862', )([Fla58491,Con53981])
model = tf.keras.models.Model(inputs=[in0Con14858,in0Sub82973,in1Sub82973,in0Glo24224,in0Con20521,in0Con53981], outputs=Min24862)
w = model.get_layer('Con14858').get_weights() 
w[0] = np.array([[[[[0.9661, 0.4808, 0.5464, 0.8149]]]]])
w[1] = np.array([0, 0, 0, 0])
model.get_layer('Con14858').set_weights(w) 
w = model.get_layer('Bat63415').get_weights() 
w[0] = np.array([0.9591, 0.6104])
w[1] = np.array([0.3378, 0.8158])
w[2] = np.array([0.2604, 0.6492])
w[3] = np.array([0.6686, 0.0612])
model.get_layer('Bat63415').set_weights(w) 
in0Con14858 = tf.constant([[[[[0.1096], [0.5274]]]]])
in0Sub82973 = tf.constant([[[0.7869, 0.1229, 0.679], [0.7618, 0.3308, 0.8207]]])
in1Sub82973 = tf.constant([[[0.7983, 0.3116, 0.2408], [0.3698, 0.8207, 0.1142]]])
in0Glo24224 = tf.constant([[[[1.1276, 1.8564]], [[1.7277, 1.9944]]]])
in0Con20521 = tf.constant([[0.2379, 0.6412, 0.3316, 0.7628]])
in0Con53981 = tf.constant([[0.3548, 0.0753]])
print (np.array2string(model.predict([in0Con14858,in0Sub82973,in1Sub82973,in0Glo24224,in0Con20521,in0Con53981],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Min24862.png')

LCon14858 = conv3D_layer([[[[[0.1096], [0.5274]]]]], 1, 1, 1,[[[[[0.9661, 0.4808, 0.5464, 0.8149]]]]],[0, 0, 0, 0], 1, 1, 1, true, 1, 1, 1, Con14858), 
LRes6567 = reshape_layer(Con14858, [1, 1, 8], Res6567), 
LRes21 = reshape_layer(Res6567, [1, 8], Res21), 
LFla58491 = flatten_layer(Res21, Fla58491), 
LSub82973 = subtract_layer([[[0.7869, 0.1229, 0.679], [0.7618, 0.3308, 0.8207]]], [[[0.7983, 0.3116, 0.2408], [0.3698, 0.8207, 0.1142]]], Sub82973), 
LBat63415 = batch_normalization_layer(Sub82973, 1, 0.8380487694964077, [0.9591, 0.6104], [0.3378, 0.8158], [0.2604, 0.6492], [0.6686, 0.0612], Bat63415), 
LFla9003 = flatten_layer(Bat63415, Fla9003), 
LGlo24224 = global_average_pooling2D_layer([[[[1.1276, 1.8564]], [[1.7277, 1.9944]]]], Glo24224), 
LCon20521 = concatenate_layer([Glo24224,[[0.2379, 0.6412, 0.3316, 0.7628]]], 1, Con20521), 
LAve78633 = average_layer([Fla9003,Con20521], Ave78633), 
LCon53981 = concatenate_layer([Ave78633,[[0.3548, 0.0753]]], 1, Con53981), 
LMin24862 = minimum_layer([Fla58491,Con53981], Min24862), 
exec_layers([LCon14858,LRes6567,LRes21,LFla58491,LSub82973,LBat63415,LFla9003,LGlo24224,LCon20521,LAve78633,LCon53981,LMin24862],["Con14858","Res6567","Res21","Fla58491","Sub82973","Bat63415","Fla9003","Glo24224","Con20521","Ave78633","Con53981","Min24862"],Min24862,"Min24862")

Actual (Unparsed): [[0.1058846, 0.0526957, 0.0598854, 0.0893130, 0.2070882, 0.2535739, 0.2881714, 0.0753000]]

Expected (Unparsed): [[0.10588456,0.05269568,0.05988544,0.08931304,0.20708818674226026,0.25357392,0.28817136,0.0753]]

Actual:   [[0.1059, 0.0527, 0.0599, 0.0894, 0.2071, 0.2536, 0.2882, 0.0753]]

Expected: [[0.1059, 0.0527, 0.0599, 0.0894, 0.2071, 0.2536, 0.2882, 0.0753]]