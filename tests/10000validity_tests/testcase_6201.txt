import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Glo98497 = tf.keras.layers.Input(shape=([2, 1]))
in0Con14503 = tf.keras.layers.Input(shape=([7]))
in0Dot61894 = tf.keras.layers.Input(shape=([3, 3]))
in1Dot61894 = tf.keras.layers.Input(shape=([3, 3]))
in0Dot31509 = tf.keras.layers.Input(shape=([3]))
in1Dot31509 = tf.keras.layers.Input(shape=([3]))
in0Con48875 = tf.keras.layers.Input(shape=([8]))

Glo98497 = keras.layers.GlobalAveragePooling1D(name = 'Glo98497', )(in0Glo98497)
Res22489 = keras.layers.Reshape((1, 1), name = 'Res22489', )(Glo98497)
Res64922 = keras.layers.Reshape((1, 1, 1), name = 'Res64922', )(Res22489)
Con48554 = keras.layers.Conv2DTranspose(2, (1, 1),strides=(1, 1), padding='same', name = 'Con48554', )(Res64922)
Res35901 = keras.layers.Reshape((1, 2), name = 'Res35901', )(Con48554)
Fla94795 = keras.layers.Flatten(name = 'Fla94795', )(Res35901)
Con14503 = keras.layers.Concatenate(axis=1, name = 'Con14503', )([Fla94795,in0Con14503])
Dot61894 = keras.layers.Dot(axes=(2, 2), name = 'Dot61894', )([in0Dot61894,in1Dot61894])
Fla4633 = keras.layers.Flatten(name = 'Fla4633', )(Dot61894)
Dot31509 = keras.layers.Dot(axes=(1, 1), name = 'Dot31509', )([in0Dot31509,in1Dot31509])
Thr87803 = keras.layers.ThresholdedReLU(theta=9.456784184528157, name = 'Thr87803', )(Dot31509)
Con48875 = keras.layers.Concatenate(axis=1, name = 'Con48875', )([Thr87803,in0Con48875])
Sub76174 = keras.layers.Subtract(name = 'Sub76174', )([Fla4633,Con48875])
ELU62033 = keras.layers.ELU(alpha=-9.598825546788879, name = 'ELU62033', )(Sub76174)
Add45224 = keras.layers.Add(name = 'Add45224', )([Con14503,ELU62033])
model = tf.keras.models.Model(inputs=[in0Glo98497,in0Con14503,in0Dot61894,in1Dot61894,in0Dot31509,in1Dot31509,in0Con48875], outputs=Add45224)
w = model.get_layer('Con48554').get_weights() 
w[0] = np.array([[[[0.1029], [0.2273]]]])
w[1] = np.array([0, 0])
model.get_layer('Con48554').set_weights(w) 
in0Glo98497 = tf.constant([[[1.5837], [1.4074]]])
in0Con14503 = tf.constant([[0.809, 0.9603, 0.0724, 0.008, 0.8088, 0.0496, 0.6892]])
in0Dot61894 = tf.constant([[[0.568, 0.3432, 0.1203], [0.6145, 0.4351, 0.9168], [0.0063, 0.6648, 0.0293]]])
in1Dot61894 = tf.constant([[[0.4398, 0.791, 0.843], [0.8301, 0.0008, 0.4061], [0.2304, 0.0719, 0.5268]]])
in0Dot31509 = tf.constant([[0.759, 0.7497, 0.8538]])
in1Dot31509 = tf.constant([[0.7798, 0.9075, 0.6934]])
in0Con48875 = tf.constant([[0.07, 0.0628, 0.0823, 0.9099, 0.9513, 0.7904, 0.5515, 0.8935]])
print (np.array2string(model.predict([in0Glo98497,in0Con14503,in0Dot61894,in1Dot61894,in0Dot31509,in1Dot31509,in0Con48875],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Add45224.png')

LGlo98497 = global_average_pooling1D_layer([[[1.5837], [1.4074]]], Glo98497), 
LRes22489 = reshape_layer(Glo98497, [1, 1], Res22489), 
LRes64922 = reshape_layer(Res22489, [1, 1, 1], Res64922), 
LCon48554 = conv2D_transpose_layer(Res64922, 1, 1,[[[[0.1029], [0.2273]]]],[0, 0], 1, 1, true, Con48554), 
LRes35901 = reshape_layer(Con48554, [1, 2], Res35901), 
LFla94795 = flatten_layer(Res35901, Fla94795), 
LCon14503 = concatenate_layer([Fla94795,[[0.809, 0.9603, 0.0724, 0.008, 0.8088, 0.0496, 0.6892]]], 1, Con14503), 
LDot61894 = dot_layer([[[0.568, 0.3432, 0.1203], [0.6145, 0.4351, 0.9168], [0.0063, 0.6648, 0.0293]]], [[[0.4398, 0.791, 0.843], [0.8301, 0.0008, 0.4061], [0.2304, 0.0719, 0.5268]]], 2, 2, Dot61894), 
LFla4633 = flatten_layer(Dot61894, Fla4633), 
LDot31509 = dot_layer([[0.759, 0.7497, 0.8538]], [[0.7798, 0.9075, 0.6934]], 1, 1, Dot31509), 
LThr87803 = thresholded_relu_layer(Dot31509, 9.456784184528157, Thr87803), 
LCon48875 = concatenate_layer([Thr87803,[[0.07, 0.0628, 0.0823, 0.9099, 0.9513, 0.7904, 0.5515, 0.8935]]], 1, Con48875), 
LSub76174 = subtract_layer(Fla4633,Con48875, Sub76174), 
LELU62033 = elu_layer(Sub76174, -9.598825546788879, ELU62033), 
LAdd45224 = add_layer([Con14503,ELU62033], Add45224), 
exec_layers([LGlo98497,LRes22489,LRes64922,LCon48554,LRes35901,LFla94795,LCon14503,LDot61894,LFla4633,LDot31509,LThr87803,LCon48875,LSub76174,LELU62033,LAdd45224],["Glo98497","Res22489","Res64922","Con48554","Res35901","Fla94795","Con14503","Dot61894","Fla4633","Dot31509","Thr87803","Con48875","Sub76174","ELU62033","Add45224"],Add45224,"Add45224")

Actual (Unparsed): [[0.7765826, 0.7905637, 0.9651173, 2.2652836, 0.3294367, 2.4635213, 2.8347855, 4.0201622, 6.0974979]]

Expected (Unparsed): [[0.776582595,0.7905637050000001,0.9651173200000001,2.2652836,0.3294366732882719,2.4635210632044413,2.8347853203942206,4.020162077837656,6.097498000208578]]

Actual:   [[0.7766, 0.7906, 0.9652, 2.2653, 0.3295, 2.4636, 2.8348, 4.0202, 6.0975]]

Expected: [[0.7766, 0.7906, 0.9652, 2.2653, 0.3295, 2.4636, 2.8348, 4.0202, 6.0975]]