import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Bat10879 = tf.keras.layers.Input(shape=([3, 2, 3]))
in0Dot24632 = tf.keras.layers.Input(shape=([2]))
in1Dot24632 = tf.keras.layers.Input(shape=([2]))
in0Con56708 = tf.keras.layers.Input(shape=([17]))

Bat10879 = keras.layers.BatchNormalization(axis=2, epsilon=0.5785672157923122,  name = 'Bat10879', )(in0Bat10879)
Res30179 = keras.layers.Reshape((3, 6), name = 'Res30179', )(Bat10879)
Fla41011 = keras.layers.Flatten(name = 'Fla41011', )(Res30179)
Dot24632 = keras.layers.Dot(axes=(1, 1), name = 'Dot24632', )([in0Dot24632,in1Dot24632])
Con56708 = keras.layers.Concatenate(axis=1, name = 'Con56708', )([Dot24632,in0Con56708])
Min8639 = keras.layers.Minimum(name = 'Min8639', )([Fla41011,Con56708])
model = tf.keras.models.Model(inputs=[in0Bat10879,in0Dot24632,in1Dot24632,in0Con56708], outputs=Min8639)
w = model.get_layer('Bat10879').get_weights() 
w[0] = np.array([0.7333, 0.3792])
w[1] = np.array([0.4691, 0.5935])
w[2] = np.array([0.8963, 0.3619])
w[3] = np.array([0.5745, 0.4826])
model.get_layer('Bat10879').set_weights(w) 
in0Bat10879 = tf.constant([[[[1.3678, 1.7073, 1.4008], [1.1437, 1.3454, 1.6616]], [[1.446, 1.715, 1.859], [1.5038, 1.5165, 1.011]], [[1.7186, 1.979, 1.6237], [1.8539, 1.0639, 1.5429]]]])
in0Dot24632 = tf.constant([[0.63, 0.6614]])
in1Dot24632 = tf.constant([[0.9819, 0.6455]])
in0Con56708 = tf.constant([[0.46, 0.2256, 0.457, 0.7955, 0.3463, 0.9044, 0.1637, 0.4252, 0.2549, 0.0536, 0.3077, 0.663, 0.994, 0.7043, 0.2383, 0.2194, 0.9359]])
print (np.array2string(model.predict([in0Bat10879,in0Dot24632,in1Dot24632,in0Con56708],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Min8639.png')

LBat10879 = batch_normalization_layer([[[[1.3678, 1.7073, 1.4008], [1.1437, 1.3454, 1.6616]], [[1.446, 1.715, 1.859], [1.5038, 1.5165, 1.011]], [[1.7186, 1.979, 1.6237], [1.8539, 1.0639, 1.5429]]]], 2, 0.5785672157923122, [0.7333, 0.3792], [0.4691, 0.5935], [0.8963, 0.3619], [0.5745, 0.4826], Bat10879), 
LRes30179 = reshape_layer(Bat10879, [3, 6], Res30179), 
LFla41011 = flatten_layer(Res30179, Fla41011), 
LDot24632 = dot_layer([[0.63, 0.6614]], [[0.9819, 0.6455]], 1, 1, Dot24632), 
LCon56708 = concatenate_layer([Dot24632,[[0.46, 0.2256, 0.457, 0.7955, 0.3463, 0.9044, 0.1637, 0.4252, 0.2549, 0.0536, 0.3077, 0.663, 0.994, 0.7043, 0.2383, 0.2194, 0.9359]]], 1, Con56708), 
LMin8639 = minimum_layer([Fla41011,Con56708], Min8639), 
exec_layers([LBat10879,LRes30179,LFla41011,LDot24632,LCon56708,LMin8639],["Bat10879","Res30179","Fla41011","Dot24632","Con56708","Min8639"],Min8639,"Min8639")

Actual (Unparsed): [[0.7910853, 0.4600000, 0.2256000, 0.4570000, 0.7955000, 0.3463000, 0.8444877, 0.1637000, 0.4252000, 0.2549000, 0.0536000, 0.3077000, 0.6630000, 0.9940000, 0.7043000, 0.2383000, 0.2194000, 0.9359000]]

Expected (Unparsed): [[0.7910853186269,0.46,0.2256,0.457,0.7955,0.3463,0.8444877617162395,0.1637,0.4252,0.2549,0.0536,0.3077,0.663,0.994,0.7043,0.2383,0.2194,0.9359]]

Actual:   [[0.7911, 0.46, 0.2256, 0.457, 0.7955, 0.3463, 0.8445, 0.1637, 0.4252, 0.2549, 0.0536, 0.3077, 0.663, 0.994, 0.7043, 0.2383, 0.2194, 0.9359]]

Expected: [[0.7911, 0.46, 0.2256, 0.457, 0.7955, 0.3463, 0.8445, 0.1637, 0.4252, 0.2549, 0.0536, 0.3077, 0.663, 0.994, 0.7043, 0.2383, 0.2194, 0.9359]]