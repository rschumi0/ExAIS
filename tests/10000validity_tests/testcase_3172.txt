import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Dot35285 = tf.keras.layers.Input(shape=([3]))
in1Dot35285 = tf.keras.layers.Input(shape=([3]))
in0Con41225 = tf.keras.layers.Input(shape=([2, 1, 2]))
in0PRe69659 = tf.keras.layers.Input(shape=([2, 1, 1, 1]))
in0Con50627 = tf.keras.layers.Input(shape=([2, 1, 2]))
in0Con59741 = tf.keras.layers.Input(shape=([2, 1, 2]))

Dot35285 = keras.layers.Dot(axes=(1, 1), name = 'Dot35285', )([in0Dot35285,in1Dot35285])
Res64999 = keras.layers.Reshape((1, 1), name = 'Res64999', )(Dot35285)
Res43001 = keras.layers.Reshape((1, 1, 1), name = 'Res43001', )(Res64999)
Zer45611 = keras.layers.ZeroPadding2D(padding=((1, 0), (0, 0)), name = 'Zer45611', )(Res43001)
Con41225 = keras.layers.Concatenate(axis=3, name = 'Con41225', )([Zer45611,in0Con41225])
PRe69659 = keras.layers.PReLU(name = 'PRe69659', input_shape=(2, 1, 1, 1))(in0PRe69659)
Res11698 = keras.layers.Reshape((2, 1, 1), name = 'Res11698', )(PRe69659)
Con50627 = keras.layers.Concatenate(axis=3, name = 'Con50627', )([Res11698,in0Con50627])
Con59741 = keras.layers.Conv2DTranspose(3, (1, 1),strides=(1, 1), padding='valid', name = 'Con59741', )(in0Con59741)
Sub84274 = keras.layers.Subtract(name = 'Sub84274', )([Con50627,Con59741])
Add94578 = keras.layers.Add(name = 'Add94578', )([Con41225,Sub84274])
model = tf.keras.models.Model(inputs=[in0Dot35285,in1Dot35285,in0Con41225,in0PRe69659,in0Con50627,in0Con59741], outputs=Add94578)
w = model.get_layer('PRe69659').get_weights() 
w[0] = np.array([[[[0.1769]]], [[[0.8659]]]])
model.get_layer('PRe69659').set_weights(w) 
w = model.get_layer('Con59741').get_weights() 
w[0] = np.array([[[[0.4271, 0.4513], [0.9997, 0.0532], [0.5429, 0.1181]]]])
w[1] = np.array([0, 0, 0])
model.get_layer('Con59741').set_weights(w) 
in0Dot35285 = tf.constant([[0.3351, 0.2616, 0.6832]])
in1Dot35285 = tf.constant([[0.9452, 0.1951, 0.5207]])
in0Con41225 = tf.constant([[[[0.3332, 0.4782]], [[0.0136, 0.7706]]]])
in0PRe69659 = tf.constant([[[[[0.0598]]], [[[0.9626]]]]])
in0Con50627 = tf.constant([[[[0.2645, 0.4362]], [[0.8218, 0.2316]]]])
in0Con59741 = tf.constant([[[[0.3201, 0.1639]], [[0.3355, 0.5746]]]])
print (np.array2string(model.predict([in0Dot35285,in1Dot35285,in0Con41225,in0PRe69659,in0Con50627,in0Con59741],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Add94578.png')

LDot35285 = dot_layer([[0.3351, 0.2616, 0.6832]], [[0.9452, 0.1951, 0.5207]], 1, 1, Dot35285), 
LRes64999 = reshape_layer(Dot35285, [1, 1], Res64999), 
LRes43001 = reshape_layer(Res64999, [1, 1, 1], Res43001), 
LZer45611 = zero_padding2D_layer(Res43001, 1, 0, 0, 0, Zer45611), 
LCon41225 = concatenate_layer([Zer45611,[[[[0.3332, 0.4782]], [[0.0136, 0.7706]]]]], 3, Con41225), 
LPRe69659 = prelu_layer([[[[[0.0598]]], [[[0.9626]]]]], [[[[0.1769]]], [[[0.8659]]]], PRe69659), 
LRes11698 = reshape_layer(PRe69659, [2, 1, 1], Res11698), 
LCon50627 = concatenate_layer([Res11698,[[[[0.2645, 0.4362]], [[0.8218, 0.2316]]]]], 3, Con50627), 
LCon59741 = conv2D_transpose_layer([[[[0.3201, 0.1639]], [[0.3355, 0.5746]]]], 1, 1,[[[[0.4271, 0.4513], [0.9997, 0.0532], [0.5429, 0.1181]]]],[0, 0, 0], 1, 1, false, Con59741), 
LSub84274 = subtract_layer(Con50627,Con59741, Sub84274), 
LAdd94578 = add_layer([Con41225,Sub84274], Add94578), 
exec_layers([LDot35285,LRes64999,LRes43001,LZer45611,LCon41225,LPRe69659,LRes11698,LCon50627,LCon59741,LSub84274,LAdd94578],["Dot35285","Res64999","Res43001","Zer45611","Con41225","PRe69659","Res11698","Con50627","Con59741","Sub84274","Add94578"],Add94578,"Add94578")

Actual (Unparsed): [[[[-0.1508828, 0.2689765, 0.7212611]], [[1.2835079, 0.4694319, 0.7521968]]]]

Expected (Unparsed): [[[[-0.15088278,0.26897655,0.72126112]],[[1.28350789,0.46943192999999994,0.75219679]]]]

Actual:   [[[[-0.1508, 0.269, 0.7213]], [[1.2836, 0.4695, 0.7522]]]]

Expected: [[[[-0.1508, 0.269, 0.7213]], [[1.2836, 0.4695, 0.7522]]]]