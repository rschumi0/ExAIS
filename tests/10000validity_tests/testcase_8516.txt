import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Mul56725 = tf.keras.layers.Input(shape=([2, 2, 1, 2]))
in1Mul56725 = tf.keras.layers.Input(shape=([2, 2, 1, 2]))
in0Add59349 = tf.keras.layers.Input(shape=([2, 1, 2]))
in1Add59349 = tf.keras.layers.Input(shape=([2, 1, 2]))
in0Con49546 = tf.keras.layers.Input(shape=([4, 2, 3, 1]))
in0ReL29166 = tf.keras.layers.Input(shape=([1, 1, 2, 2]))
in0Glo30697 = tf.keras.layers.Input(shape=([2, 2]))
in0Con19848 = tf.keras.layers.Input(shape=([46]))

Mul56725 = keras.layers.Multiply(name = 'Mul56725', )([in0Mul56725,in1Mul56725])
Res29812 = keras.layers.Reshape((2, 2, 2), name = 'Res29812', )(Mul56725)
Res42248 = keras.layers.Reshape((2, 4), name = 'Res42248', )(Res29812)
Add59349 = keras.layers.Add(name = 'Add59349', )([in0Add59349,in1Add59349])
Res10514 = keras.layers.Reshape((2, 2), name = 'Res10514', )(Add59349)
Dot50093 = keras.layers.Dot(axes=(1, 1), name = 'Dot50093', )([Res42248,Res10514])
Res87830 = keras.layers.Reshape((4, 2, 1), name = 'Res87830', )(Dot50093)
Res36888 = keras.layers.Reshape((4, 2, 1, 1), name = 'Res36888', )(Res87830)
Zer70485 = keras.layers.ZeroPadding3D(padding=((0, 0), (0, 0), (2, 0)), name = 'Zer70485', )(Res36888)
Con49546 = keras.layers.Concatenate(axis=4, name = 'Con49546', )([Zer70485,in0Con49546])
ReL29166 = keras.layers.ReLU(max_value=4.205895191128698, negative_slope=8.029188234870983, threshold=7.870065283200595, name = 'ReL29166', input_shape=(1, 1, 2, 2))(in0ReL29166)
Zer3479 = keras.layers.ZeroPadding3D(padding=((3, 0), (1, 0), (1, 0)), name = 'Zer3479', )(ReL29166)
Sub40927 = keras.layers.Subtract(name = 'Sub40927', )([Con49546,Zer3479])
Res43114 = keras.layers.Reshape((4, 2, 6), name = 'Res43114', )(Sub40927)
Res27761 = keras.layers.Reshape((4, 12), name = 'Res27761', )(Res43114)
Fla27957 = keras.layers.Flatten(name = 'Fla27957', )(Res27761)
Glo30697 = keras.layers.GlobalAveragePooling1D(name = 'Glo30697', )(in0Glo30697)
Con19848 = keras.layers.Concatenate(axis=1, name = 'Con19848', )([Glo30697,in0Con19848])
Add46634 = keras.layers.Add(name = 'Add46634', )([Fla27957,Con19848])
model = tf.keras.models.Model(inputs=[in0Mul56725,in1Mul56725,in0Add59349,in1Add59349,in0Con49546,in0ReL29166,in0Glo30697,in0Con19848], outputs=Add46634)
in0Mul56725 = tf.constant([[[[[0.5827, 0.3171]], [[0.5847, 0.2012]]], [[[0.7518, 0.6588]], [[0.3627, 0.3993]]]]])
in1Mul56725 = tf.constant([[[[[0.2401, 0.2467]], [[0.3417, 0.2966]]], [[[0.7847, 0.4345]], [[0.143, 0.0999]]]]])
in0Add59349 = tf.constant([[[[0.5886, 0.2081]], [[0.447, 0.1344]]]])
in1Add59349 = tf.constant([[[[0.2259, 0.3834]], [[0.4945, 0.5193]]]])
in0Con49546 = tf.constant([[[[[0.8774], [0.4373], [0.988]], [[0.0898], [0.5862], [0.689]]], [[[0.9], [0.6358], [0.0106]], [[0.0997], [0.4844], [0.6497]]], [[[0.7137], [0.9555], [0.4459]], [[0.0801], [0.4631], [0.7634]]], [[[0.8289], [0.2607], [0.5943]], [[0.284], [0.9121], [0.0574]]]]])
in0ReL29166 = tf.constant([[[[[0.8942, 0.6551], [0.4281, 0.0899]]]]])
in0Glo30697 = tf.constant([[[1.2745, 1.8552], [1.1323, 1.0612]]])
in0Con19848 = tf.constant([[0.0443, 0.7069, 0.2514, 0.7635, 0.9724, 0.3004, 0.8356, 0.0141, 0.1831, 0.3297, 0.0921, 0.3419, 0.0869, 0.9633, 0.2109, 0.3137, 0.2888, 0.7178, 0.7645, 0.2478, 0.1897, 0.6793, 0.5797, 0.8463, 0.5907, 0.4636, 0.0322, 0.5131, 0.1577, 0.701, 0.6131, 0.4174, 0.8117, 0.1712, 0.9001, 0.908, 0.7238, 0.7124, 0.261, 0.5851, 0.6159, 0.6798, 0.7143, 0.1888, 0.4933, 0.041]])
print (np.array2string(model.predict([in0Mul56725,in1Mul56725,in0Add59349,in1Add59349,in0Con49546,in0ReL29166,in0Glo30697,in0Con19848],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Add46634.png')

LMul56725 = multiply_layer([[[[[[0.5827, 0.3171]], [[0.5847, 0.2012]]], [[[0.7518, 0.6588]], [[0.3627, 0.3993]]]]], [[[[[0.2401, 0.2467]], [[0.3417, 0.2966]]], [[[0.7847, 0.4345]], [[0.143, 0.0999]]]]]], Mul56725), 
LRes29812 = reshape_layer(Mul56725, [2, 2, 2], Res29812), 
LRes42248 = reshape_layer(Res29812, [2, 4], Res42248), 
LAdd59349 = add_layer([[[[[0.5886, 0.2081]], [[0.447, 0.1344]]]], [[[[0.2259, 0.3834]], [[0.4945, 0.5193]]]]], Add59349), 
LRes10514 = reshape_layer(Add59349, [2, 2], Res10514), 
LDot50093 = dot_layer(Res42248,Res10514, 1, 1, Dot50093), 
LRes87830 = reshape_layer(Dot50093, [4, 2, 1], Res87830), 
LRes36888 = reshape_layer(Res87830, [4, 2, 1, 1], Res36888), 
LZer70485 = zero_padding3D_layer(Res36888, 0, 0, 0, 0, 2, 0, Zer70485), 
LCon49546 = concatenate_layer([Zer70485,[[[[[0.8774], [0.4373], [0.988]], [[0.0898], [0.5862], [0.689]]], [[[0.9], [0.6358], [0.0106]], [[0.0997], [0.4844], [0.6497]]], [[[0.7137], [0.9555], [0.4459]], [[0.0801], [0.4631], [0.7634]]], [[[0.8289], [0.2607], [0.5943]], [[0.284], [0.9121], [0.0574]]]]]], 4, Con49546), 
LReL29166 = relu_layer([[[[[0.8942, 0.6551], [0.4281, 0.0899]]]]], 4.205895191128698, 8.029188234870983, 7.870065283200595, ReL29166), 
LZer3479 = zero_padding3D_layer(ReL29166, 3, 0, 1, 0, 1, 0, Zer3479), 
LSub40927 = subtract_layer(Con49546,Zer3479, Sub40927), 
LRes43114 = reshape_layer(Sub40927, [4, 2, 6], Res43114), 
LRes27761 = reshape_layer(Res43114, [4, 12], Res27761), 
LFla27957 = flatten_layer(Res27761, Fla27957), 
LGlo30697 = global_average_pooling1D_layer([[[1.2745, 1.8552], [1.1323, 1.0612]]], Glo30697), 
LCon19848 = concatenate_layer([Glo30697,[[0.0443, 0.7069, 0.2514, 0.7635, 0.9724, 0.3004, 0.8356, 0.0141, 0.1831, 0.3297, 0.0921, 0.3419, 0.0869, 0.9633, 0.2109, 0.3137, 0.2888, 0.7178, 0.7645, 0.2478, 0.1897, 0.6793, 0.5797, 0.8463, 0.5907, 0.4636, 0.0322, 0.5131, 0.1577, 0.701, 0.6131, 0.4174, 0.8117, 0.1712, 0.9001, 0.908, 0.7238, 0.7124, 0.261, 0.5851, 0.6159, 0.6798, 0.7143, 0.1888, 0.4933, 0.041]]], 1, Con19848), 
LAdd46634 = add_layer([Fla27957,Con19848], Add46634), 
exec_layers([LMul56725,LRes29812,LRes42248,LAdd59349,LRes10514,LDot50093,LRes87830,LRes36888,LZer70485,LCon49546,LReL29166,LZer3479,LSub40927,LRes43114,LRes27761,LFla27957,LGlo30697,LCon19848,LAdd46634],["Mul56725","Res29812","Res42248","Add59349","Res10514","Dot50093","Res87830","Res36888","Zer70485","Con49546","ReL29166","Zer3479","Sub40927","Res43114","Res27761","Fla27957","Glo30697","Con19848","Add46634"],Add46634,"Add46634")

Actual (Unparsed): [[1.2034000, 2.3356000, 0.0443000, 1.1442000, 0.9207798, 1.7515000, 0.9724000, 0.3902000, 0.8356000, 0.6003000, 0.6514967, 1.0187000, 0.0921000, 1.2419000, 0.0869000, 1.5991000, 0.5441202, 0.3243000, 0.2888000, 0.8175000, 0.7645000, 0.7322000, 0.4230929, 1.3290000, 0.5797000, 1.5600000, 0.5907000, 1.4191000, 0.2437625, 0.9590000, 0.1577000, 0.7811000, 0.6131000, 0.8805000, 0.9637818, 0.9346000, 0.9001000, 1.7369000, 0.7238000, 0.9731000, 0.3471625, 1.1794000, 0.6159000, 0.9638000, 56.7248352, 59.0312145, 60.3076146, 62.5668115]]

Expected (Unparsed): [[1.2034,2.3356,0.0443,1.1442,0.920779775505,1.7515,0.9724,0.3902,0.8356,0.6003000000000001,0.651496676307,1.0187,0.0921,1.2419,0.0869,1.5991,0.5441202271650001,0.3243,0.2888,0.8175,0.7645,0.7322,0.423092908975,1.3290000000000002,0.5797,1.56,0.5907,1.4191,0.24376250900500002,0.9590000000000001,0.1577,0.7810999999999999,0.6131,0.8805000000000001,0.963781831655,0.9346,0.9001,1.7368999999999999,0.7238,0.9731000000000001,0.347162537745,1.1794,0.6159,0.9638,56.72483545991915,59.0312143668768,60.307614541631516,62.56681155722588]]

Actual:   [[1.2034, 2.3356, 0.0443, 1.1442, 0.9208, 1.7515, 0.9724, 0.3902, 0.8356, 0.6003, 0.6515, 1.0187, 0.0921, 1.2419, 0.0869, 1.5991, 0.5442, 0.3243, 0.2888, 0.8175, 0.7645, 0.7322, 0.4231, 1.329, 0.5797, 1.56, 0.5907, 1.4191, 0.2438, 0.959, 0.1577, 0.7811, 0.6131, 0.8805, 0.9638, 0.9346, 0.9001, 1.7369, 0.7238, 0.9731, 0.3472, 1.1794, 0.6159, 0.9638, 56.7249, 59.0313, 60.3077, 62.5669]]

Expected: [[1.2034, 2.3356, 0.0443, 1.1442, 0.9208, 1.7515, 0.9724, 0.3902, 0.8356, 0.6004, 0.6515, 1.0187, 0.0921, 1.2419, 0.0869, 1.5991, 0.5442, 0.3243, 0.2888, 0.8175, 0.7645, 0.7322, 0.4231, 1.3291, 0.5797, 1.56, 0.5907, 1.4191, 0.2438, 0.9591, 0.1577, 0.7811, 0.6131, 0.8806, 0.9638, 0.9346, 0.9001, 1.7369, 0.7238, 0.9732, 0.3472, 1.1794, 0.6159, 0.9638, 56.7249, 59.0313, 60.3077, 62.5669]]