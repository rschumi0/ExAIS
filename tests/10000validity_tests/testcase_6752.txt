import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Sub94683 = tf.keras.layers.Input(shape=([2, 2, 2]))
in1Sub94683 = tf.keras.layers.Input(shape=([2, 2, 2]))
in0Mas38682 = tf.keras.layers.Input(shape=([3]))
in0Con47835 = tf.keras.layers.Input(shape=([7]))

Sub94683 = keras.layers.Subtract(name = 'Sub94683', )([in0Sub94683,in1Sub94683])
Res13199 = keras.layers.Reshape((2, 4), name = 'Res13199', )(Sub94683)
Fla11477 = keras.layers.Flatten(name = 'Fla11477', )(Res13199)
Mas38682 = keras.layers.Masking(mask_value=1, name = 'Mas38682', )(in0Mas38682)
Res24204 = keras.layers.Reshape((3, 1), name = 'Res24204', )(Mas38682)
Res24874 = keras.layers.Reshape((3, 1, 1), name = 'Res24874', )(Res24204)
Res89199 = keras.layers.Reshape((3, 1, 1, 1), name = 'Res89199', )(Res24874)
Glo64236 = keras.layers.GlobalMaxPool3D(name = 'Glo64236', )(Res89199)
Con47835 = keras.layers.Concatenate(axis=1, name = 'Con47835', )([Glo64236,in0Con47835])
Min54593 = keras.layers.Minimum(name = 'Min54593', )([Fla11477,Con47835])
Res78384 = keras.layers.Reshape((8, 1), name = 'Res78384', )(Min54593)
Res45603 = keras.layers.Reshape((8, 1, 1), name = 'Res45603', )(Res78384)
Con98280 = keras.layers.Conv2DTranspose(3, (7, 1),strides=(1, 1), padding='valid', name = 'Con98280', )(Res45603)
model = tf.keras.models.Model(inputs=[in0Sub94683,in1Sub94683,in0Mas38682,in0Con47835], outputs=Con98280)
w = model.get_layer('Con98280').get_weights() 
w[0] = np.array([[[[0.1236], [0.0105], [0.418]]], [[[0.8831], [0.3758], [0.4365]]], [[[0.1759], [0.3676], [0.3083]]], [[[0.8692], [0.8822], [0.321]]], [[[0.2385], [0.8158], [0.9785]]], [[[0.6865], [0.441], [0.0772]]], [[[0.3306], [0.9061], [0.3624]]]])
w[1] = np.array([0, 0, 0])
model.get_layer('Con98280').set_weights(w) 
in0Sub94683 = tf.constant([[[[0.1568, 0.3272], [0.2604, 0.7908]], [[0.4826, 0.4333], [0.7947, 0.9936]]]])
in1Sub94683 = tf.constant([[[[0.4376, 0.5933], [0.0994, 0.9375]], [[0.8605, 0.5219], [0.0207, 0.9487]]]])
in0Mas38682 = tf.constant([[1.3439, 1.5621, 1.7016]])
in0Con47835 = tf.constant([[0.6986, 0.0979, 0.1161, 0.7088, 0.6515, 0.7933, 0.1472]])
print (np.array2string(model.predict([in0Sub94683,in1Sub94683,in0Mas38682,in0Con47835],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Con98280.png')

LSub94683 = subtract_layer([[[[0.1568, 0.3272], [0.2604, 0.7908]], [[0.4826, 0.4333], [0.7947, 0.9936]]]], [[[[0.4376, 0.5933], [0.0994, 0.9375]], [[0.8605, 0.5219], [0.0207, 0.9487]]]], Sub94683), 
LRes13199 = reshape_layer(Sub94683, [2, 4], Res13199), 
LFla11477 = flatten_layer(Res13199, Fla11477), 
LMas38682 = masking_layer([[1.3439, 1.5621, 1.7016]], 1, Mas38682), 
LRes24204 = reshape_layer(Mas38682, [3, 1], Res24204), 
LRes24874 = reshape_layer(Res24204, [3, 1, 1], Res24874), 
LRes89199 = reshape_layer(Res24874, [3, 1, 1, 1], Res89199), 
LGlo64236 = global_max_pool3D_layer(Res89199, Glo64236), 
LCon47835 = concatenate_layer([Glo64236,[[0.6986, 0.0979, 0.1161, 0.7088, 0.6515, 0.7933, 0.1472]]], 1, Con47835), 
LMin54593 = minimum_layer([Fla11477,Con47835], Min54593), 
LRes78384 = reshape_layer(Min54593, [8, 1], Res78384), 
LRes45603 = reshape_layer(Res78384, [8, 1, 1], Res45603), 
LCon98280 = conv2D_transpose_layer(Res45603, 7, 1,[[[[0.1236], [0.0105], [0.418]]], [[[0.8831], [0.3758], [0.4365]]], [[[0.1759], [0.3676], [0.3083]]], [[[0.8692], [0.8822], [0.321]]], [[[0.2385], [0.8158], [0.9785]]], [[[0.6865], [0.441], [0.0772]]], [[[0.3306], [0.9061], [0.3624]]]],[0, 0, 0], 1, 1, false, Con98280), 
exec_layers([LSub94683,LRes13199,LFla11477,LMas38682,LRes24204,LRes24874,LRes89199,LGlo64236,LCon47835,LMin54593,LRes78384,LRes45603,LCon98280],["Sub94683","Res13199","Fla11477","Mas38682","Res24204","Res24874","Res89199","Glo64236","Con47835","Min54593","Res78384","Res45603","Con98280"],Con98280,"Con98280")

Actual (Unparsed): [[[[-0.0347069, -0.0029484, -0.1173744]], [[-0.2808644, -0.1083187, -0.2337990]], [[-0.2722852, -0.2021945, -0.1618011]], [[-0.2225550, -0.3102896, -0.1907627]], [[-0.4573035, -0.4869398, -0.5519951]], [[-0.5416183, -0.4514218, -0.4978465]], [[-0.4287215, -0.5854198, 0.0947512]], [[0.2892614, -0.3922292, -0.0244248]], [[-0.0596863, -0.0610454, -0.1158390]], [[0.3516003, 0.3274694, 0.0932636]], [[0.0378684, 0.2895522, 0.6279811]], [[0.5127685, 0.2976830, 0.0715788]], [[0.2867083, 0.7211223, 0.2839639]], [[0.0148439, 0.0406839, 0.0162718]]]]

Expected (Unparsed): [[[[-0.03470688,-0.0029484000000000003,-0.11737439999999999]],[[-0.28086444,-0.10831869000000001,-0.23379900000000003]],[[-0.27228519000000007,-0.20219451000000002,-0.16180109000000004]],[[-0.22255498,-0.31028965000000003,-0.19076268000000002]],[[-0.4573035200000001,-0.4869398300000001,-0.5519950800000001]],[[-0.54161835,-0.4514218400000001,-0.49784647000000015]],[[-0.4287214900000001,-0.58541982,0.09475113999999987]],[[0.2892613599999998,-0.39222926000000014,-0.024424790000000134]],[[-0.05968629000000002,-0.061045430000000074,-0.11583898000000009]],[[0.3516002399999999,0.32746938999999986,0.09326360999999998]],[[0.03786843999999997,0.2895521899999999,0.6279810199999999]],[[0.5127684899999999,0.29768296,0.07157881000000005]],[[0.28670825,0.7211223,0.28396387999999995]],[[0.014843940000000017,0.04068389000000005,0.016271760000000017]]]]

Actual:   [[[[-0.0347, -0.0029, -0.1173]], [[-0.2808, -0.1083, -0.2337]], [[-0.2722, -0.2021, -0.1618]], [[-0.2225, -0.3102, -0.1907]], [[-0.4573, -0.4869, -0.5519]], [[-0.5416, -0.4514, -0.4978]], [[-0.4287, -0.5854, 0.0948]], [[0.2893, -0.3922, -0.0244]], [[-0.0596, -0.061, -0.1158]], [[0.3517, 0.3275, 0.0933]], [[0.0379, 0.2896, 0.628]], [[0.5128, 0.2977, 0.0716]], [[0.2868, 0.7212, 0.284]], [[0.0149, 0.0407, 0.0163]]]]

Expected: [[[[-0.0347, -0.0029, -0.1173]], [[-0.2808, -0.1083, -0.2337]], [[-0.2722, -0.2021, -0.1618]], [[-0.2225, -0.3102, -0.1907]], [[-0.4573, -0.4869, -0.5519]], [[-0.5416, -0.4514, -0.4978]], [[-0.4287, -0.5854, 0.0948]], [[0.2893, -0.3922, -0.0244]], [[-0.0596, -0.061, -0.1158]], [[0.3517, 0.3275, 0.0933]], [[0.0379, 0.2896, 0.628]], [[0.5128, 0.2977, 0.0716]], [[0.2868, 0.7212, 0.284]], [[0.0149, 0.0407, 0.0163]]]]