import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Dot49075 = tf.keras.layers.Input(shape=([3]))
in1Dot49075 = tf.keras.layers.Input(shape=([3]))
in0Ave94822 = tf.keras.layers.Input(shape=([1, 2, 2]))
in0Glo95902 = tf.keras.layers.Input(shape=([1, 2, 2]))

Dot49075 = keras.layers.Dot(axes=(1, 1), name = 'Dot49075', )([in0Dot49075,in1Dot49075])
Ave94822 = keras.layers.AveragePooling2D(pool_size=(1, 1), strides=(6, 12), padding='valid', name = 'Ave94822', )(in0Ave94822)
Res16710 = keras.layers.Reshape((1, 2), name = 'Res16710', )(Ave94822)
Fla46293 = keras.layers.Flatten(name = 'Fla46293', )(Res16710)
Glo95902 = keras.layers.GlobalAveragePooling2D(name = 'Glo95902', )(in0Glo95902)
Dot12250 = keras.layers.Dot(axes=(1, 1), name = 'Dot12250', )([Fla46293,Glo95902])
Bat49726 = keras.layers.BatchNormalization(axis=1, epsilon=0.8635622616057805,  name = 'Bat49726', )(Dot12250)
ELU9705 = keras.layers.ELU(alpha=-4.638080234853681, name = 'ELU9705', )(Bat49726)
Max31088 = keras.layers.Maximum(name = 'Max31088', )([Dot49075,ELU9705])
model = tf.keras.models.Model(inputs=[in0Dot49075,in1Dot49075,in0Ave94822,in0Glo95902], outputs=Max31088)
w = model.get_layer('Bat49726').get_weights() 
w[0] = np.array([0.1956])
w[1] = np.array([0.4012])
w[2] = np.array([0.6006])
w[3] = np.array([0.4956])
model.get_layer('Bat49726').set_weights(w) 
in0Dot49075 = tf.constant([[0.6474, 0.1147, 0.8027]])
in1Dot49075 = tf.constant([[0.8396, 0.8032, 0.2994]])
in0Ave94822 = tf.constant([[[[1.6217, 1.8399], [1.8287, 1.7527]]]])
in0Glo95902 = tf.constant([[[[1.4722, 1.0033], [1.2646, 1.022]]]])
print (np.array2string(model.predict([in0Dot49075,in1Dot49075,in0Ave94822,in0Glo95902],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Max31088.png')

LDot49075 = dot_layer([[0.6474, 0.1147, 0.8027]], [[0.8396, 0.8032, 0.2994]], 1, 1, Dot49075), 
LAve94822 = average_pooling2D_layer([[[[1.6217, 1.8399], [1.8287, 1.7527]]]], 1, 1, 6, 12, false, Ave94822), 
LRes16710 = reshape_layer(Ave94822, [1, 2], Res16710), 
LFla46293 = flatten_layer(Res16710, Fla46293), 
LGlo95902 = global_average_pooling2D_layer([[[[1.4722, 1.0033], [1.2646, 1.022]]]], Glo95902), 
LDot12250 = dot_layer(Fla46293,Glo95902, 1, 1, Dot12250), 
LBat49726 = batch_normalization_layer(Dot12250, 1, 0.8635622616057805, [0.1956], [0.4012], [0.6006], [0.4956], Bat49726), 
LELU9705 = elu_layer(Bat49726, -4.638080234853681, ELU9705), 
LMax31088 = maximum_layer([Dot49075,ELU9705], Max31088), 
exec_layers([LDot49075,LAve94822,LRes16710,LFla46293,LGlo95902,LDot12250,LBat49726,LELU9705,LMax31088],["Dot49075","Ave94822","Res16710","Fla46293","Glo95902","Dot12250","Bat49726","ELU9705","Max31088"],Max31088,"Max31088")

Actual (Unparsed): [[0.9853517]]

Expected (Unparsed): [[0.9853517320194698]]

Actual:   [[0.9854]]

Expected: [[0.9854]]