import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Glo56275 = tf.keras.layers.Input(shape=([2, 1]))
in0Sim2400 = tf.keras.layers.Input(shape=([2, 2]))
in0Max88998 = tf.keras.layers.Input(shape=([1, 1, 1]))
in1Max88998 = tf.keras.layers.Input(shape=([1, 1, 1]))
in0Con90541 = tf.keras.layers.Input(shape=([1, 3]))

Glo56275 = keras.layers.GlobalAveragePooling1D(name = 'Glo56275', )(in0Glo56275)
Lea10041 = keras.layers.LeakyReLU(alpha=5.025235399103506, name = 'Lea10041', )(Glo56275)
Res20171 = keras.layers.Reshape((1, 1), name = 'Res20171', )(Lea10041)
Res51810 = keras.layers.Reshape((1, 1, 1), name = 'Res51810', )(Res20171)
Con8719 = keras.layers.Conv2DTranspose(4, (1, 1),strides=(1, 1), padding='same', name = 'Con8719', )(Res51810)
Bat66775 = keras.layers.BatchNormalization(axis=2, epsilon=0.5466595791845199,  name = 'Bat66775', )(Con8719)
Res9541 = keras.layers.Reshape((1, 4), name = 'Res9541', )(Bat66775)
Sim2400 = keras.layers.SimpleRNN(1,name = 'Sim2400', )(in0Sim2400)
Res49172 = keras.layers.Reshape((1, 1), name = 'Res49172', )(Sim2400)
Max88998 = keras.layers.Maximum(name = 'Max88998', )([in0Max88998,in1Max88998])
Res21945 = keras.layers.Reshape((1, 1), name = 'Res21945', )(Max88998)
Dot23028 = keras.layers.Dot(axes=(2, 2), name = 'Dot23028', )([Res49172,Res21945])
Con90541 = keras.layers.Concatenate(axis=2, name = 'Con90541', )([Dot23028,in0Con90541])
Sub27187 = keras.layers.Subtract(name = 'Sub27187', )([Res9541,Con90541])
Zer76543 = keras.layers.ZeroPadding1D(padding=((1, 1)), name = 'Zer76543', )(Sub27187)
model = tf.keras.models.Model(inputs=[in0Glo56275,in0Sim2400,in0Max88998,in1Max88998,in0Con90541], outputs=Zer76543)
w = model.get_layer('Con8719').get_weights() 
w[0] = np.array([[[[0.4597], [0.6173], [0.7415], [0.4076]]]])
w[1] = np.array([0, 0, 0, 0])
model.get_layer('Con8719').set_weights(w) 
w = model.get_layer('Bat66775').get_weights() 
w[0] = np.array([0.6755])
w[1] = np.array([0.6129])
w[2] = np.array([0.5482])
w[3] = np.array([0.3423])
model.get_layer('Bat66775').set_weights(w) 
w = model.get_layer('Sim2400').get_weights() 
w[0] = np.array([[4], [2]])
w[1] = np.array([[10]])
w[2] = np.array([9])
model.get_layer('Sim2400').set_weights(w) 
in0Glo56275 = tf.constant([[[1.5852], [1.475]]])
in0Sim2400 = tf.constant([[[6, 4], [7, 9]]])
in0Max88998 = tf.constant([[[[0.8922]]]])
in1Max88998 = tf.constant([[[[0.5288]]]])
in0Con90541 = tf.constant([[[0.0641, 0.9685, 0.8973]]])
print (np.array2string(model.predict([in0Glo56275,in0Sim2400,in0Max88998,in1Max88998,in0Con90541],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Zer76543.png')

LGlo56275 = global_average_pooling1D_layer([[[1.5852], [1.475]]], Glo56275), 
LLea10041 = leaky_relu_layer(Glo56275, 5.025235399103506, Lea10041), 
LRes20171 = reshape_layer(Lea10041, [1, 1], Res20171), 
LRes51810 = reshape_layer(Res20171, [1, 1, 1], Res51810), 
LCon8719 = conv2D_transpose_layer(Res51810, 1, 1,[[[[0.4597], [0.6173], [0.7415], [0.4076]]]],[0, 0, 0, 0], 1, 1, true, Con8719), 
LBat66775 = batch_normalization_layer(Con8719, 2, 0.5466595791845199, [0.6755], [0.6129], [0.5482], [0.3423], Bat66775), 
LRes9541 = reshape_layer(Bat66775, [1, 4], Res9541), 
LSim2400 = simple_rnn_layer([[[6, 4], [7, 9]]],[[4], [2]],[[10]],[9], Sim2400), 
LRes49172 = reshape_layer(Sim2400, [1, 1], Res49172), 
LMax88998 = maximum_layer([[[[[0.8922]]]], [[[[0.5288]]]]], Max88998), 
LRes21945 = reshape_layer(Max88998, [1, 1], Res21945), 
LDot23028 = dot_layer(Res49172,Res21945, 2, 2, Dot23028), 
LCon90541 = concatenate_layer([Dot23028,[[[0.0641, 0.9685, 0.8973]]]], 2, Con90541), 
LSub27187 = subtract_layer(Res9541,Con90541, Sub27187), 
LZer76543 = zero_padding1D_layer(Sub27187, 1, 1, Zer76543), 
exec_layers([LGlo56275,LLea10041,LRes20171,LRes51810,LCon8719,LBat66775,LRes9541,LSim2400,LRes49172,LMax88998,LRes21945,LDot23028,LCon90541,LSub27187,LZer76543],["Glo56275","Lea10041","Res20171","Res51810","Con8719","Bat66775","Res9541","Sim2400","Res49172","Max88998","Res21945","Dot23028","Con90541","Sub27187","Zer76543"],Zer76543,"Zer76543")

Actual (Unparsed): [[[0.0000000, 0.0000000, 0.0000000, 0.0000000], [-0.1681167, 0.8327501, 0.0645027, -0.2303306], [0.0000000, 0.0000000, 0.0000000, 0.0000000]]]

Expected (Unparsed): [[[0,0,0,0],[-0.16811668978305006,0.8327501441525675,0.06450268713990115,-0.23033059871735018],[0,0,0,0]]]

Actual:   [[[0, 0, 0, 0], [-0.1681, 0.8328, 0.0646, -0.2303], [0, 0, 0, 0]]]

Expected: [[[0, 0, 0, 0], [-0.1681, 0.8328, 0.0646, -0.2303], [0, 0, 0, 0]]]