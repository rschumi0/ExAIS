import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0ReL12611 = tf.keras.layers.Input(shape=([2, 1]))
in0Glo22659 = tf.keras.layers.Input(shape=([1, 1]))
in0Con52744 = tf.keras.layers.Input(shape=([1]))

ReL12611 = keras.layers.ReLU(max_value=6.666997231935068, negative_slope=3.799273818507677, threshold=6.852271033784046, name = 'ReL12611', input_shape=(2, 1))(in0ReL12611)
Res19907 = keras.layers.Reshape((2, 1, 1), name = 'Res19907', )(ReL12611)
PRe53710 = keras.layers.PReLU(name = 'PRe53710', )(Res19907)
Res98652 = keras.layers.Reshape((2, 1), name = 'Res98652', )(PRe53710)
Fla34364 = keras.layers.Flatten(name = 'Fla34364', )(Res98652)
Glo22659 = keras.layers.GlobalMaxPool1D(name = 'Glo22659', )(in0Glo22659)
Res85216 = keras.layers.Reshape((1, 1), name = 'Res85216', )(Glo22659)
Glo79980 = keras.layers.GlobalMaxPool1D(name = 'Glo79980', )(Res85216)
Con52744 = keras.layers.Concatenate(axis=1, name = 'Con52744', )([Glo79980,in0Con52744])
Sub11697 = keras.layers.Subtract(name = 'Sub11697', )([Fla34364,Con52744])
Bat77639 = keras.layers.BatchNormalization(axis=1, epsilon=0.6402162748090139,  name = 'Bat77639', )(Sub11697)
model = tf.keras.models.Model(inputs=[in0ReL12611,in0Glo22659,in0Con52744], outputs=Bat77639)
w = model.get_layer('PRe53710').get_weights() 
w[0] = np.array([[[0.8064]], [[0.5973]]])
model.get_layer('PRe53710').set_weights(w) 
w = model.get_layer('Bat77639').get_weights() 
w[0] = np.array([0.3395, 0.7379])
w[1] = np.array([0.0757, 0.5587])
w[2] = np.array([0.3783, 0.2798])
w[3] = np.array([0.4587, 0.5624])
model.get_layer('Bat77639').set_weights(w) 
in0ReL12611 = tf.constant([[[0.7146], [0.4082]]])
in0Glo22659 = tf.constant([[[1.8423]]])
in0Con52744 = tf.constant([[0.929]])
print (np.array2string(model.predict([in0ReL12611,in0Glo22659,in0Con52744],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Bat77639.png')

LReL12611 = relu_layer([[[0.7146], [0.4082]]], 6.666997231935068, 3.799273818507677, 6.852271033784046, ReL12611), 
LRes19907 = reshape_layer(ReL12611, [2, 1, 1], Res19907), 
LPRe53710 = prelu_layer(Res19907, [[[0.8064]], [[0.5973]]], PRe53710), 
LRes98652 = reshape_layer(PRe53710, [2, 1], Res98652), 
LFla34364 = flatten_layer(Res98652, Fla34364), 
LGlo22659 = global_max_pool1D_layer([[[1.8423]]], Glo22659), 
LRes85216 = reshape_layer(Glo22659, [1, 1], Res85216), 
LGlo79980 = global_max_pool1D_layer(Res85216, Glo79980), 
LCon52744 = concatenate_layer([Glo79980,[[0.929]]], 1, Con52744), 
LSub11697 = subtract_layer(Fla34364,Con52744, Sub11697), 
LBat77639 = batch_normalization_layer(Sub11697, 1, 0.6402162748090139, [0.3395, 0.7379], [0.0757, 0.5587], [0.3783, 0.2798], [0.4587, 0.5624], Bat77639), 
exec_layers([LReL12611,LRes19907,LPRe53710,LRes98652,LFla34364,LGlo22659,LRes85216,LGlo79980,LCon52744,LSub11697,LBat77639],["ReL12611","Res19907","PRe53710","Res98652","Fla34364","Glo22659","Res85216","Glo79980","Con52744","Sub11697","Bat77639"],Bat77639,"Bat77639")

Actual (Unparsed): [[-6.7333923, -10.0944961]]

Expected (Unparsed): [[-6.733392339161671,-10.094496100043923]]

Actual:   [[-6.7333, -10.0944]]

Expected: [[-6.7333, -10.0944]]