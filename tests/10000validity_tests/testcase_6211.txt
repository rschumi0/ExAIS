import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Min62630 = tf.keras.layers.Input(shape=([2, 1, 2, 2]))
in1Min62630 = tf.keras.layers.Input(shape=([2, 1, 2, 2]))
in0Dot57399 = tf.keras.layers.Input(shape=([2]))
in1Dot57399 = tf.keras.layers.Input(shape=([2]))
in0Con37555 = tf.keras.layers.Input(shape=([94]))

Min62630 = keras.layers.Minimum(name = 'Min62630', )([in0Min62630,in1Min62630])
Zer3319 = keras.layers.ZeroPadding3D(padding=((1, 1), (1, 1), (1, 1)), name = 'Zer3319', )(Min62630)
Res28481 = keras.layers.Reshape((4, 3, 8), name = 'Res28481', )(Zer3319)
Res66095 = keras.layers.Reshape((4, 24), name = 'Res66095', )(Res28481)
Fla88691 = keras.layers.Flatten(name = 'Fla88691', )(Res66095)
Dot57399 = keras.layers.Dot(axes=(1, 1), name = 'Dot57399', )([in0Dot57399,in1Dot57399])
Res58295 = keras.layers.Reshape((1, 1), name = 'Res58295', )(Dot57399)
GRU33337 = keras.layers.GRU(2,reset_after=False, recurrent_activation='sigmoid', name = 'GRU33337', )(Res58295)
Con37555 = keras.layers.Concatenate(axis=1, name = 'Con37555', )([GRU33337,in0Con37555])
Sub76425 = keras.layers.Subtract(name = 'Sub76425', )([Fla88691,Con37555])
model = tf.keras.models.Model(inputs=[in0Min62630,in1Min62630,in0Dot57399,in1Dot57399,in0Con37555], outputs=Sub76425)
w = model.get_layer('GRU33337').get_weights() 
w[0] = np.array([[2, 10, 8, 7, 3, 8]])
w[1] = np.array([[4, 1, 4, 4, 7, 10], [10, 8, 1, 7, 1, 7]])
w[2] = np.array([10, 7, 8, 6, 4, 4])
model.get_layer('GRU33337').set_weights(w) 
in0Min62630 = tf.constant([[[[[0.4519, 0.1686], [0.5955, 0.6091]]], [[[0.1484, 0.969], [0.0384, 0.1211]]]]])
in1Min62630 = tf.constant([[[[[0.4727, 0.6061], [0.9931, 0.7381]]], [[[0.4625, 0.1058], [0.5697, 0.9785]]]]])
in0Dot57399 = tf.constant([[0.4154, 0.3319]])
in1Dot57399 = tf.constant([[0.86, 0.8402]])
in0Con37555 = tf.constant([[0.5583, 0.0522, 0.4083, 0.6951, 0.8201, 0.8962, 0.3863, 0.313, 0.8998, 0.9408, 0.5706, 0.8434, 0.9149, 0.1134, 0.2601, 0.8462, 0.9439, 0.6328, 0.8833, 0.1793, 0.2533, 0.1042, 0.0201, 0.8597, 0.3131, 0.549, 0.353, 0.3179, 0.4465, 0.9983, 0.4602, 0.9443, 0.0367, 0.08, 0.4555, 0.2773, 0.5747, 0.3717, 0.3588, 0.6558, 0.4737, 0.1549, 0.7347, 0.6709, 0.4994, 0.6101, 0.5782, 0.2895, 0.8675, 0.4174, 0.6448, 0.0953, 0.5462, 0.1959, 0.4236, 0.6943, 0.0701, 0.2016, 0.7334, 0.9662, 0.2153, 0.2333, 0.4871, 0.507, 0.2185, 0.7483, 0.5129, 0.3075, 0.8324, 0.7784, 0.4465, 0.4346, 0.3993, 0.3538, 0.6184, 0.2471, 0.2998, 0.7921, 0.5438, 0.4638, 0.5876, 0.3921, 0.0994, 0.2228, 0.3556, 0.7797, 0.7145, 0.1275, 0.3942, 0.4022, 0.943, 0.1527, 0.0049, 0.4358]])
print (np.array2string(model.predict([in0Min62630,in1Min62630,in0Dot57399,in1Dot57399,in0Con37555],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Sub76425.png')

LMin62630 = minimum_layer([[[[[[0.4519, 0.1686], [0.5955, 0.6091]]], [[[0.1484, 0.969], [0.0384, 0.1211]]]]], [[[[[0.4727, 0.6061], [0.9931, 0.7381]]], [[[0.4625, 0.1058], [0.5697, 0.9785]]]]]], Min62630), 
LZer3319 = zero_padding3D_layer(Min62630, 1, 1, 1, 1, 1, 1, Zer3319), 
LRes28481 = reshape_layer(Zer3319, [4, 3, 8], Res28481), 
LRes66095 = reshape_layer(Res28481, [4, 24], Res66095), 
LFla88691 = flatten_layer(Res66095, Fla88691), 
LDot57399 = dot_layer([[0.4154, 0.3319]], [[0.86, 0.8402]], 1, 1, Dot57399), 
LRes58295 = reshape_layer(Dot57399, [1, 1], Res58295), 
LGRU33337 = gru_layer(Res58295,[[2, 10, 8, 7, 3, 8]],[[4, 1, 4, 4, 7, 10], [10, 8, 1, 7, 1, 7]],[10, 7, 8, 6, 4, 4], false, GRU33337), 
LCon37555 = concatenate_layer([GRU33337,[[0.5583, 0.0522, 0.4083, 0.6951, 0.8201, 0.8962, 0.3863, 0.313, 0.8998, 0.9408, 0.5706, 0.8434, 0.9149, 0.1134, 0.2601, 0.8462, 0.9439, 0.6328, 0.8833, 0.1793, 0.2533, 0.1042, 0.0201, 0.8597, 0.3131, 0.549, 0.353, 0.3179, 0.4465, 0.9983, 0.4602, 0.9443, 0.0367, 0.08, 0.4555, 0.2773, 0.5747, 0.3717, 0.3588, 0.6558, 0.4737, 0.1549, 0.7347, 0.6709, 0.4994, 0.6101, 0.5782, 0.2895, 0.8675, 0.4174, 0.6448, 0.0953, 0.5462, 0.1959, 0.4236, 0.6943, 0.0701, 0.2016, 0.7334, 0.9662, 0.2153, 0.2333, 0.4871, 0.507, 0.2185, 0.7483, 0.5129, 0.3075, 0.8324, 0.7784, 0.4465, 0.4346, 0.3993, 0.3538, 0.6184, 0.2471, 0.2998, 0.7921, 0.5438, 0.4638, 0.5876, 0.3921, 0.0994, 0.2228, 0.3556, 0.7797, 0.7145, 0.1275, 0.3942, 0.4022, 0.943, 0.1527, 0.0049, 0.4358]]], 1, Con37555), 
LSub76425 = subtract_layer(Fla88691,Con37555, Sub76425), 
exec_layers([LMin62630,LZer3319,LRes28481,LRes66095,LFla88691,LDot57399,LRes58295,LGRU33337,LCon37555,LSub76425],["Min62630","Zer3319","Res28481","Res66095","Fla88691","Dot57399","Res58295","GRU33337","Con37555","Sub76425"],Sub76425,"Sub76425")

Actual (Unparsed): [[-0.0000127, -0.0000016, -0.5583000, -0.0522000, -0.4083000, -0.6951000, -0.8201000, -0.8962000, -0.3863000, -0.3130000, -0.8998000, -0.9408000, -0.5706000, -0.8434000, -0.9149000, -0.1134000, -0.2601000, -0.8462000, -0.9439000, -0.6328000, -0.8833000, -0.1793000, -0.2533000, -0.1042000, -0.0201000, -0.8597000, -0.3131000, -0.5490000, -0.3530000, -0.3179000, -0.4465000, -0.9983000, -0.4602000, -0.9443000, 0.4152000, 0.0886000, 0.1400000, 0.3318000, -0.5747000, -0.3717000, -0.3588000, -0.6558000, -0.4737000, -0.1549000, -0.7347000, -0.6709000, -0.4994000, -0.6101000, -0.5782000, -0.2895000, -0.8675000, -0.4174000, -0.6448000, -0.0953000, -0.5462000, -0.1959000, -0.4236000, -0.6943000, 0.0783000, -0.0958000, -0.6950000, -0.8451000, -0.2153000, -0.2333000, -0.4871000, -0.5070000, -0.2185000, -0.7483000, -0.5129000, -0.3075000, -0.8324000, -0.7784000, -0.4465000, -0.4346000, -0.3993000, -0.3538000, -0.6184000, -0.2471000, -0.2998000, -0.7921000, -0.5438000, -0.4638000, -0.5876000, -0.3921000, -0.0994000, -0.2228000, -0.3556000, -0.7797000, -0.7145000, -0.1275000, -0.3942000, -0.4022000, -0.9430000, -0.1527000, -0.0049000, -0.4358000]]

Expected (Unparsed): [[-1.2721205367771253e-5,-1.5752990971860006e-6,-0.5583,-0.0522,-0.4083,-0.6951,-0.8201,-0.8962,-0.3863,-0.313,-0.8998,-0.9408,-0.5706,-0.8434,-0.9149,-0.1134,-0.2601,-0.8462,-0.9439,-0.6328,-0.8833,-0.1793,-0.2533,-0.1042,-0.0201,-0.8597,-0.3131,-0.549,-0.353,-0.3179,-0.4465,-0.9983,-0.4602,-0.9443,0.4152,0.0886,0.14,0.3318,-0.5747,-0.3717,-0.3588,-0.6558,-0.4737,-0.1549,-0.7347,-0.6709,-0.4994,-0.6101,-0.5782,-0.2895,-0.8675,-0.4174,-0.6448,-0.0953,-0.5462,-0.1959,-0.4236,-0.6943,0.07830000000000001,-0.0958,-0.6950000000000001,-0.8451,-0.2153,-0.2333,-0.4871,-0.507,-0.2185,-0.7483,-0.5129,-0.3075,-0.8324,-0.7784,-0.4465,-0.4346,-0.3993,-0.3538,-0.6184,-0.2471,-0.2998,-0.7921,-0.5438,-0.4638,-0.5876,-0.3921,-0.0994,-0.2228,-0.3556,-0.7797,-0.7145,-0.1275,-0.3942,-0.4022,-0.943,-0.1527,-0.0049,-0.4358]]

Actual:   [[-0, -0, -0.5583, -0.0522, -0.4083, -0.6951, -0.8201, -0.8962, -0.3863, -0.313, -0.8998, -0.9408, -0.5706, -0.8434, -0.9149, -0.1134, -0.2601, -0.8462, -0.9439, -0.6328, -0.8833, -0.1793, -0.2533, -0.1042, -0.0201, -0.8597, -0.3131, -0.549, -0.353, -0.3179, -0.4465, -0.9983, -0.4602, -0.9443, 0.4152, 0.0886, 0.14, 0.3318, -0.5747, -0.3717, -0.3588, -0.6558, -0.4737, -0.1549, -0.7347, -0.6709, -0.4994, -0.6101, -0.5782, -0.2895, -0.8675, -0.4174, -0.6448, -0.0953, -0.5462, -0.1959, -0.4236, -0.6943, 0.0783, -0.0958, -0.695, -0.8451, -0.2153, -0.2333, -0.4871, -0.507, -0.2185, -0.7483, -0.5129, -0.3075, -0.8324, -0.7784, -0.4465, -0.4346, -0.3993, -0.3538, -0.6184, -0.2471, -0.2998, -0.7921, -0.5438, -0.4638, -0.5876, -0.3921, -0.0994, -0.2228, -0.3556, -0.7797, -0.7145, -0.1275, -0.3942, -0.4022, -0.943, -0.1527, -0.0049, -0.4358]]

Expected: [[-0, -0, -0.5583, -0.0522, -0.4083, -0.6951, -0.8201, -0.8962, -0.3863, -0.313, -0.8998, -0.9408, -0.5706, -0.8434, -0.9149, -0.1134, -0.2601, -0.8462, -0.9439, -0.6328, -0.8833, -0.1793, -0.2533, -0.1042, -0.0201, -0.8597, -0.3131, -0.549, -0.353, -0.3179, -0.4465, -0.9983, -0.4602, -0.9443, 0.4152, 0.0886, 0.14, 0.3318, -0.5747, -0.3717, -0.3588, -0.6558, -0.4737, -0.1549, -0.7347, -0.6709, -0.4994, -0.6101, -0.5782, -0.2895, -0.8675, -0.4174, -0.6448, -0.0953, -0.5462, -0.1959, -0.4236, -0.6943, 0.0784, -0.0958, -0.695, -0.8451, -0.2153, -0.2333, -0.4871, -0.507, -0.2185, -0.7483, -0.5129, -0.3075, -0.8324, -0.7784, -0.4465, -0.4346, -0.3993, -0.3538, -0.6184, -0.2471, -0.2998, -0.7921, -0.5438, -0.4638, -0.5876, -0.3921, -0.0994, -0.2228, -0.3556, -0.7797, -0.7145, -0.1275, -0.3942, -0.4022, -0.943, -0.1527, -0.0049, -0.4358]]