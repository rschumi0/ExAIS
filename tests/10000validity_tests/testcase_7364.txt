import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Min24794 = tf.keras.layers.Input(shape=([2, 1, 2, 2]))
in1Min24794 = tf.keras.layers.Input(shape=([2, 1, 2, 2]))
in0Sub635 = tf.keras.layers.Input(shape=([2, 2, 2]))
in1Sub635 = tf.keras.layers.Input(shape=([2, 2, 2]))
in0Con26566 = tf.keras.layers.Input(shape=([4]))

Min24794 = keras.layers.Minimum(name = 'Min24794', )([in0Min24794,in1Min24794])
Lea57513 = keras.layers.LeakyReLU(alpha=5.480620293649129, name = 'Lea57513', )(Min24794)
Res59042 = keras.layers.Reshape((2, 1, 4), name = 'Res59042', )(Lea57513)
Res43734 = keras.layers.Reshape((2, 4), name = 'Res43734', )(Res59042)
Fla27693 = keras.layers.Flatten(name = 'Fla27693', )(Res43734)
Sub635 = keras.layers.Subtract(name = 'Sub635', )([in0Sub635,in1Sub635])
Res3886 = keras.layers.Reshape((2, 4), name = 'Res3886', )(Sub635)
Glo50322 = keras.layers.GlobalMaxPool1D(name = 'Glo50322', )(Res3886)
Con26566 = keras.layers.Concatenate(axis=1, name = 'Con26566', )([Glo50322,in0Con26566])
Add8583 = keras.layers.Add(name = 'Add8583', )([Fla27693,Con26566])
model = tf.keras.models.Model(inputs=[in0Min24794,in1Min24794,in0Sub635,in1Sub635,in0Con26566], outputs=Add8583)
in0Min24794 = tf.constant([[[[[0.6605, 0.7448], [0.1596, 0.9981]]], [[[0.9575, 0.5871], [0.3297, 0.8786]]]]])
in1Min24794 = tf.constant([[[[[0.4552, 0.3588], [0.3923, 0.388]]], [[[0.6713, 0.3966], [0.7203, 0.5901]]]]])
in0Sub635 = tf.constant([[[[0.7243, 0.6449], [0.7805, 0.7412]], [[0.8983, 0.6174], [0.5713, 0.3017]]]])
in1Sub635 = tf.constant([[[[0.1471, 0.3431], [0.8827, 0.923]], [[0.5063, 0.255], [0.9485, 0.4209]]]])
in0Con26566 = tf.constant([[0.5488, 0.8483, 0.559, 0.588]])
print (np.array2string(model.predict([in0Min24794,in1Min24794,in0Sub635,in1Sub635,in0Con26566],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Add8583.png')

LMin24794 = minimum_layer([[[[[[0.6605, 0.7448], [0.1596, 0.9981]]], [[[0.9575, 0.5871], [0.3297, 0.8786]]]]], [[[[[0.4552, 0.3588], [0.3923, 0.388]]], [[[0.6713, 0.3966], [0.7203, 0.5901]]]]]], Min24794), 
LLea57513 = leaky_relu_layer(Min24794, 5.480620293649129, Lea57513), 
LRes59042 = reshape_layer(Lea57513, [2, 1, 4], Res59042), 
LRes43734 = reshape_layer(Res59042, [2, 4], Res43734), 
LFla27693 = flatten_layer(Res43734, Fla27693), 
LSub635 = subtract_layer([[[[0.7243, 0.6449], [0.7805, 0.7412]], [[0.8983, 0.6174], [0.5713, 0.3017]]]], [[[[0.1471, 0.3431], [0.8827, 0.923]], [[0.5063, 0.255], [0.9485, 0.4209]]]], Sub635), 
LRes3886 = reshape_layer(Sub635, [2, 4], Res3886), 
LGlo50322 = global_max_pool1D_layer(Res3886, Glo50322), 
LCon26566 = concatenate_layer([Glo50322,[[0.5488, 0.8483, 0.559, 0.588]]], 1, Con26566), 
LAdd8583 = add_layer([Fla27693,Con26566], Add8583), 
exec_layers([LMin24794,LLea57513,LRes59042,LRes43734,LFla27693,LSub635,LRes3886,LGlo50322,LCon26566,LAdd8583],["Min24794","Lea57513","Res59042","Res43734","Fla27693","Sub635","Res3886","Glo50322","Con26566","Add8583"],Add8583,"Add8583")

Actual (Unparsed): [[1.0324000, 0.7212000, 0.0574000, 0.2688000, 1.2201000, 1.2449000, 0.8887000, 1.1781000]]

Expected (Unparsed): [[1.0324,0.7212,0.05739999999999992,0.26880000000000004,1.2201,1.2449000000000001,0.8887,1.1781]]

Actual:   [[1.0324, 0.7212, 0.0574, 0.2688, 1.2201, 1.2449, 0.8887, 1.1781]]

Expected: [[1.0324, 0.7212, 0.0574, 0.2689, 1.2201, 1.245, 0.8887, 1.1781]]