import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Min69034 = tf.keras.layers.Input(shape=([1, 2, 1]))
in1Min69034 = tf.keras.layers.Input(shape=([1, 2, 1]))
in0Con53396 = tf.keras.layers.Input(shape=([4, 4, 2, 2]))
in0Up_39365 = tf.keras.layers.Input(shape=([4, 4, 1, 3]))
in0Add25429 = tf.keras.layers.Input(shape=([1, 2]))
in1Add25429 = tf.keras.layers.Input(shape=([1, 2]))
in0Con66140 = tf.keras.layers.Input(shape=([4, 23]))

Min69034 = keras.layers.Minimum(name = 'Min69034', )([in0Min69034,in1Min69034])
Res42421 = keras.layers.Reshape((1, 2, 1, 1), name = 'Res42421', )(Min69034)
Zer9895 = keras.layers.ZeroPadding3D(padding=((3, 0), (2, 0), (1, 0)), name = 'Zer9895', )(Res42421)
Con53396 = keras.layers.Concatenate(axis=4, name = 'Con53396', )([Zer9895,in0Con53396])
Up_39365 = keras.layers.UpSampling3D(size=(1, 1, 2), name = 'Up_39365', )(in0Up_39365)
Bat44105 = keras.layers.BatchNormalization(axis=3, epsilon=0.7790465991150606,  name = 'Bat44105', )(Up_39365)
Max82019 = keras.layers.Maximum(name = 'Max82019', )([Con53396,Bat44105])
Res74158 = keras.layers.Reshape((4, 4, 6), name = 'Res74158', )(Max82019)
Res40450 = keras.layers.Reshape((4, 24), name = 'Res40450', )(Res74158)
Add25429 = keras.layers.Add(name = 'Add25429', )([in0Add25429,in1Add25429])
Sof54695 = keras.layers.Softmax(axis=1, name = 'Sof54695', )(Add25429)
Res39042 = keras.layers.Reshape((1, 2, 1), name = 'Res39042', )(Sof54695)
Res56342 = keras.layers.Reshape((1, 2, 1, 1), name = 'Res56342', )(Res39042)
Glo34625 = keras.layers.GlobalMaxPool3D(name = 'Glo34625', )(Res56342)
Res9097 = keras.layers.Reshape((1, 1), name = 'Res9097', )(Glo34625)
Ave46226 = keras.layers.AveragePooling1D(pool_size=(1), name = 'Ave46226', )(Res9097)
Zer41226 = keras.layers.ZeroPadding1D(padding=((3, 0)), name = 'Zer41226', )(Ave46226)
Con66140 = keras.layers.Concatenate(axis=2, name = 'Con66140', )([Zer41226,in0Con66140])
Add81759 = keras.layers.Add(name = 'Add81759', )([Res40450,Con66140])
model = tf.keras.models.Model(inputs=[in0Min69034,in1Min69034,in0Con53396,in0Up_39365,in0Add25429,in1Add25429,in0Con66140], outputs=Add81759)
w = model.get_layer('Bat44105').get_weights() 
w[0] = np.array([0.5241, 0.2834])
w[1] = np.array([0.8017, 0.2402])
w[2] = np.array([0.7618, 0.4487])
w[3] = np.array([0.0961, 0.8046])
model.get_layer('Bat44105').set_weights(w) 
in0Min69034 = tf.constant([[[[0.0814], [0.6977]]]])
in1Min69034 = tf.constant([[[[0.8786], [0.3874]]]])
in0Con53396 = tf.constant([[[[[0.4863, 0.8987], [0.0135, 0.119]], [[0.7995, 0.0137], [0.6001, 0.6821]], [[0.1763, 0.4193], [0.9633, 0.8085]], [[0.2816, 0.7791], [0.7796, 0.0715]]], [[[0.3899, 0.0093], [0.0987, 0.301]], [[0.1527, 0.0783], [0.7314, 0.3352]], [[0.5171, 0.947], [0.2989, 0.1051]], [[0.6985, 0.1768], [0.1462, 0.0525]]], [[[0.3379, 0.1215], [0.933, 0.2871]], [[0.2308, 0.2956], [0.872, 0.0783]], [[0.3055, 0.4623], [0.2609, 0.2462]], [[0.8527, 0.222], [0.1669, 0.7669]]], [[[0.3549, 0.3972], [0.8679, 0.4409]], [[0.6554, 0.3098], [0.9254, 0.7865]], [[0.6147, 0.1223], [0.5739, 0.4119]], [[0.8529, 0.7787], [0.4442, 0.0013]]]]])
in0Up_39365 = tf.constant([[[[[1.3469, 1.1167, 1.1824]], [[1.4967, 1.9571, 1.0241]], [[1.6243, 1.0574, 1.1493]], [[1.8435, 1.0473, 1.0916]]], [[[1.4615, 1.3664, 1.7186]], [[1.2305, 1.3189, 1.6984]], [[1.2627, 1.3197, 1.913]], [[1.5845, 1.1091, 1.4093]]], [[[1.5587, 1.1015, 1.1725]], [[1.401, 1.6949, 1.2529]], [[1.9191, 1.755, 1.8319]], [[1.9622, 1.0613, 1.6963]]], [[[1.0434, 1.3224, 1.871]], [[1.3007, 1.0495, 1.734]], [[1.5066, 1.6909, 1.546]], [[1.7355, 1.8056, 1.3279]]]]])
in0Add25429 = tf.constant([[[0.1957, 0.9177]]])
in1Add25429 = tf.constant([[[0.7121, 0.0423]]])
in0Con66140 = tf.constant([[[0.8383, 0.9513, 0.2544, 0.453, 0.8261, 0.8547, 0.0645, 0.0761, 0.1682, 0.0847, 0.1547, 0.4683, 0.3286, 0.4427, 0.147, 0.505, 0.6651, 0.3866, 0.9731, 0.6379, 0.5418, 0.7412, 0.8697], [0.8933, 0.7162, 0.6923, 0.9939, 0.7463, 0.8022, 0.3835, 0.373, 0.057, 0.8821, 0.9297, 0.1865, 0.3937, 0.8818, 0.6853, 0.4376, 0.227, 0.1601, 0.1606, 0.9214, 0.6799, 0.8662, 0.0661], [0.0186, 0.743, 0.0168, 0.5855, 0.253, 0.1262, 0.9133, 0.1225, 0.7398, 0.2043, 0.4754, 0.453, 0.6185, 0.8389, 0.303, 0.4214, 0.3207, 0.2092, 0.793, 0.0654, 0.0486, 0.1688, 0.4384], [0.3286, 0.1589, 0.0607, 0.513, 0.3222, 0.2393, 0.3698, 0.7083, 0.4398, 0.5398, 0.0669, 0.9278, 0.4921, 0.0813, 0.0115, 0.3709, 0.4301, 0.1981, 0.3159, 0.2366, 0.1568, 0.4957, 0.1487]]])
print (np.array2string(model.predict([in0Min69034,in1Min69034,in0Con53396,in0Up_39365,in0Add25429,in1Add25429,in0Con66140],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Add81759.png')

LMin69034 = minimum_layer([[[[[0.0814], [0.6977]]]], [[[[0.8786], [0.3874]]]]], Min69034), 
LRes42421 = reshape_layer(Min69034, [1, 2, 1, 1], Res42421), 
LZer9895 = zero_padding3D_layer(Res42421, 3, 0, 2, 0, 1, 0, Zer9895), 
LCon53396 = concatenate_layer([Zer9895,[[[[[0.4863, 0.8987], [0.0135, 0.119]], [[0.7995, 0.0137], [0.6001, 0.6821]], [[0.1763, 0.4193], [0.9633, 0.8085]], [[0.2816, 0.7791], [0.7796, 0.0715]]], [[[0.3899, 0.0093], [0.0987, 0.301]], [[0.1527, 0.0783], [0.7314, 0.3352]], [[0.5171, 0.947], [0.2989, 0.1051]], [[0.6985, 0.1768], [0.1462, 0.0525]]], [[[0.3379, 0.1215], [0.933, 0.2871]], [[0.2308, 0.2956], [0.872, 0.0783]], [[0.3055, 0.4623], [0.2609, 0.2462]], [[0.8527, 0.222], [0.1669, 0.7669]]], [[[0.3549, 0.3972], [0.8679, 0.4409]], [[0.6554, 0.3098], [0.9254, 0.7865]], [[0.6147, 0.1223], [0.5739, 0.4119]], [[0.8529, 0.7787], [0.4442, 0.0013]]]]]], 4, Con53396), 
LUp_39365 = up_sampling3D_layer([[[[[1.3469, 1.1167, 1.1824]], [[1.4967, 1.9571, 1.0241]], [[1.6243, 1.0574, 1.1493]], [[1.8435, 1.0473, 1.0916]]], [[[1.4615, 1.3664, 1.7186]], [[1.2305, 1.3189, 1.6984]], [[1.2627, 1.3197, 1.913]], [[1.5845, 1.1091, 1.4093]]], [[[1.5587, 1.1015, 1.1725]], [[1.401, 1.6949, 1.2529]], [[1.9191, 1.755, 1.8319]], [[1.9622, 1.0613, 1.6963]]], [[[1.0434, 1.3224, 1.871]], [[1.3007, 1.0495, 1.734]], [[1.5066, 1.6909, 1.546]], [[1.7355, 1.8056, 1.3279]]]]], 1, 1, 2, Up_39365), 
LBat44105 = batch_normalization_layer(Up_39365, 3, 0.7790465991150606, [0.5241, 0.2834], [0.8017, 0.2402], [0.7618, 0.4487], [0.0961, 0.8046], Bat44105), 
LMax82019 = maximum_layer([Con53396,Bat44105], Max82019), 
LRes74158 = reshape_layer(Max82019, [4, 4, 6], Res74158), 
LRes40450 = reshape_layer(Res74158, [4, 24], Res40450), 
LAdd25429 = add_layer([[[[0.1957, 0.9177]]], [[[0.7121, 0.0423]]]], Add25429), 
LSof54695 = softmax_layer(Add25429, 1, Sof54695), 
LRes39042 = reshape_layer(Sof54695, [1, 2, 1], Res39042), 
LRes56342 = reshape_layer(Res39042, [1, 2, 1, 1], Res56342), 
LGlo34625 = global_max_pool3D_layer(Res56342, Glo34625), 
LRes9097 = reshape_layer(Glo34625, [1, 1], Res9097), 
LAve46226 = average_pooling1D_layer(Res9097, 1, Ave46226), 
LZer41226 = zero_padding1D_layer(Ave46226, 3, 0, Zer41226), 
LCon66140 = concatenate_layer([Zer41226,[[[0.8383, 0.9513, 0.2544, 0.453, 0.8261, 0.8547, 0.0645, 0.0761, 0.1682, 0.0847, 0.1547, 0.4683, 0.3286, 0.4427, 0.147, 0.505, 0.6651, 0.3866, 0.9731, 0.6379, 0.5418, 0.7412, 0.8697], [0.8933, 0.7162, 0.6923, 0.9939, 0.7463, 0.8022, 0.3835, 0.373, 0.057, 0.8821, 0.9297, 0.1865, 0.3937, 0.8818, 0.6853, 0.4376, 0.227, 0.1601, 0.1606, 0.9214, 0.6799, 0.8662, 0.0661], [0.0186, 0.743, 0.0168, 0.5855, 0.253, 0.1262, 0.9133, 0.1225, 0.7398, 0.2043, 0.4754, 0.453, 0.6185, 0.8389, 0.303, 0.4214, 0.3207, 0.2092, 0.793, 0.0654, 0.0486, 0.1688, 0.4384], [0.3286, 0.1589, 0.0607, 0.513, 0.3222, 0.2393, 0.3698, 0.7083, 0.4398, 0.5398, 0.0669, 0.9278, 0.4921, 0.0813, 0.0115, 0.3709, 0.4301, 0.1981, 0.3159, 0.2366, 0.1568, 0.4957, 0.1487]]]], 2, Con66140), 
LAdd81759 = add_layer([Res40450,Con66140], Add81759), 
exec_layers([LMin69034,LRes42421,LZer9895,LCon53396,LUp_39365,LBat44105,LMax82019,LRes74158,LRes40450,LAdd25429,LSof54695,LRes39042,LRes56342,LGlo34625,LRes9097,LAve46226,LZer41226,LCon66140,LAdd81759],["Min69034","Res42421","Zer9895","Con53396","Up_39365","Bat44105","Max82019","Res74158","Res40450","Add25429","Sof54695","Res39042","Res56342","Glo34625","Res9097","Ave46226","Zer41226","Con66140","Add81759"],Add81759,"Add81759")

Actual (Unparsed): [[[1.1294961, 1.8388290, 1.9886368, 0.6968757, 0.8436344, 1.2315301, 2.0681201, 1.5358543, 1.0247508, 0.6444109, 0.6848000, 0.8368000, 1.7532066, 1.2959068, 1.4614928, 0.6519465, 1.4683000, 1.4736000, 1.7943111, 1.9347484, 1.6243670, 1.0961107, 1.5208000, 1.2546819], [1.1936996, 2.0337208, 2.0539372, 1.1605838, 1.4407671, 1.2724830, 1.8664843, 1.4973095, 1.6994204, 0.4732623, 1.6135000, 1.4513340, 1.2688240, 1.5079576, 2.3284477, 1.1088138, 0.8739502, 0.7969621, 1.4227090, 1.1568712, 2.0858551, 1.1758835, 1.2551228, 0.5226283], [1.2481549, 1.0106134, 1.7747904, 0.5069733, 1.5185000, 0.6562006, 1.2860051, 2.2377595, 1.1993336, 1.1944591, 1.0763000, 0.8967068, 1.9030653, 1.9766299, 2.2401124, 0.8743359, 0.9557803, 0.8723983, 1.6834116, 1.7624917, 1.3906439, 0.6296420, 0.5469583, 1.2053000], [1.9594635, 1.4443703, 1.5820177, 0.4348272, 1.3809000, 0.8827037, 1.3429131, 1.3326809, 2.0546649, 0.8718714, 1.4652000, 0.8534000, 2.1467664, 1.8143185, 1.3223398, 0.4899404, 0.9448000, 0.9174133, 1.5453052, 1.7023781, 1.3554516, 0.6867889, 1.0414755, 0.5868969]]]

Expected (Unparsed): [[[1.1294961535244685,1.8388290119395556,1.9886367495682644,0.6968757221824458,0.8436344048295188,1.2315301239871526,2.068120036276076,1.5358543194459024,1.0247508307459718,0.6444108626666702,0.6848,0.8368,1.7532066012901286,1.2959068073523037,1.461492820869478,0.6519465364035665,1.4683000000000002,1.4736,1.7943111079600373,1.9347483880212542,1.6243669995425907,1.0961106405033125,1.5208,1.2546818545881702],[1.1936996045480615,2.0337208245101586,2.053937189697849,1.160583780256492,1.4407671456767206,1.27248301001947,1.8664842713329661,1.4973094464680934,1.69942035103575,0.47326230193969726,1.6135000000000002,1.4513339456818108,1.2688239844477973,1.5079576380982755,2.3284477558321113,1.1088137807353717,0.8739502494109445,0.796962124239318,1.4227090676885665,1.1568711914528251,2.0858551006786765,1.1758835284511489,1.2551228756727757,0.5226282773641254],[1.2481548876151924,1.0106133709660947,1.7747903781447603,0.506973337366416,1.5185,0.65620063205929,1.2860051125155365,2.237759512653703,1.1993336369780663,1.1944591073639983,1.0763,0.8967068089279926,1.9030652170122504,1.976629908871137,2.240112329322396,0.8743358515888091,0.9557803338754497,0.8723983065272312,1.6834115410883135,1.7624917415494417,1.3906438480065217,0.6296420235171807,0.5469582580816814,1.2053],[1.9594634538241162,1.4443702848501403,1.5820176952475484,0.434827156515142,1.3809,0.8827036736362643,1.3429130868814496,1.3326809150042551,2.0546648785788557,0.8718714265190869,1.4651999999999998,0.8533999999999999,2.14676640769958,1.8143185545027924,1.3223398454860507,0.4899403545945329,0.9448,0.9174132820650165,1.545305237885447,1.702378029480158,1.3554516023076426,0.6867889103811748,1.041475514840081,0.5868968992905883]]]

Actual:   [[[1.1295, 1.8389, 1.9887, 0.6969, 0.8437, 1.2316, 2.0682, 1.5359, 1.0248, 0.6445, 0.6848, 0.8368, 1.7533, 1.296, 1.4615, 0.652, 1.4683, 1.4736, 1.7944, 1.9348, 1.6244, 1.0962, 1.5208, 1.2547], [1.1937, 2.0338, 2.054, 1.1606, 1.4408, 1.2725, 1.8665, 1.4974, 1.6995, 0.4733, 1.6135, 1.4514, 1.2689, 1.508, 2.3285, 1.1089, 0.874, 0.797, 1.4228, 1.1569, 2.0859, 1.1759, 1.2552, 0.5227], [1.2482, 1.0107, 1.7748, 0.507, 1.5185, 0.6563, 1.2861, 2.2378, 1.1994, 1.1945, 1.0763, 0.8968, 1.9031, 1.9767, 2.2402, 0.8744, 0.9558, 0.8724, 1.6835, 1.7625, 1.3907, 0.6297, 0.547, 1.2053], [1.9595, 1.4444, 1.5821, 0.4349, 1.3809, 0.8828, 1.343, 1.3327, 2.0547, 0.8719, 1.4652, 0.8534, 2.1468, 1.8144, 1.3224, 0.49, 0.9448, 0.9175, 1.5454, 1.7024, 1.3555, 0.6868, 1.0415, 0.5869]]]

Expected: [[[1.1295, 1.8389, 1.9887, 0.6969, 0.8437, 1.2316, 2.0682, 1.5359, 1.0248, 0.6445, 0.6848, 0.8368, 1.7533, 1.296, 1.4615, 0.652, 1.4684, 1.4736, 1.7944, 1.9348, 1.6244, 1.0962, 1.5208, 1.2547], [1.1937, 2.0338, 2.054, 1.1606, 1.4408, 1.2725, 1.8665, 1.4974, 1.6995, 0.4733, 1.6136, 1.4514, 1.2689, 1.508, 2.3285, 1.1089, 0.874, 0.797, 1.4228, 1.1569, 2.0859, 1.1759, 1.2552, 0.5227], [1.2482, 1.0107, 1.7748, 0.507, 1.5185, 0.6563, 1.2861, 2.2378, 1.1994, 1.1945, 1.0763, 0.8968, 1.9031, 1.9767, 2.2402, 0.8744, 0.9558, 0.8724, 1.6835, 1.7625, 1.3907, 0.6297, 0.547, 1.2053], [1.9595, 1.4444, 1.5821, 0.4349, 1.3809, 0.8828, 1.343, 1.3327, 2.0547, 0.8719, 1.4652, 0.8534, 2.1468, 1.8144, 1.3224, 0.49, 0.9448, 0.9175, 1.5454, 1.7024, 1.3555, 0.6868, 1.0415, 0.5869]]]