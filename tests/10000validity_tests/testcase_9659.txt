import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Add78412 = tf.keras.layers.Input(shape=([2, 2, 1]))
in1Add78412 = tf.keras.layers.Input(shape=([2, 2, 1]))
in0Con39849 = tf.keras.layers.Input(shape=([3, 1]))
in0Bat82406 = tf.keras.layers.Input(shape=([3, 2]))
in0Con95210 = tf.keras.layers.Input(shape=([3, 10, 2]))
in0Min15985 = tf.keras.layers.Input(shape=([1, 2, 1]))
in1Min15985 = tf.keras.layers.Input(shape=([1, 2, 1]))
in0Con96155 = tf.keras.layers.Input(shape=([2, 6, 2]))
in0Up_11909 = tf.keras.layers.Input(shape=([2, 3, 3]))

Add78412 = keras.layers.Add(name = 'Add78412', )([in0Add78412,in1Add78412])
Glo86233 = keras.layers.GlobalAveragePooling2D(name = 'Glo86233', )(Add78412)
Res31585 = keras.layers.Reshape((1, 1), name = 'Res31585', )(Glo86233)
Zer45715 = keras.layers.ZeroPadding1D(padding=((2, 0)), name = 'Zer45715', )(Res31585)
Con39849 = keras.layers.Concatenate(axis=2, name = 'Con39849', )([Zer45715,in0Con39849])
Bat82406 = keras.layers.BatchNormalization(axis=1, epsilon=0.5863367673600033,  name = 'Bat82406', )(in0Bat82406)
Ave90214 = keras.layers.Average(name = 'Ave90214', )([Con39849,Bat82406])
Res23912 = keras.layers.Reshape((3, 2, 1), name = 'Res23912', )(Ave90214)
Zer65559 = keras.layers.ZeroPadding2D(padding=((0, 0), (8, 0)), name = 'Zer65559', )(Res23912)
Con95210 = keras.layers.Concatenate(axis=3, name = 'Con95210', )([Zer65559,in0Con95210])
Min15985 = keras.layers.Minimum(name = 'Min15985', )([in0Min15985,in1Min15985])
Zer95475 = keras.layers.ZeroPadding2D(padding=((1, 0), (4, 0)), name = 'Zer95475', )(Min15985)
Con96155 = keras.layers.Concatenate(axis=3, name = 'Con96155', )([Zer95475,in0Con96155])
Up_11909 = keras.layers.UpSampling2D(size=(1, 2), name = 'Up_11909', )(in0Up_11909)
Add13360 = keras.layers.Add(name = 'Add13360', )([Con96155,Up_11909])
Zer24138 = keras.layers.ZeroPadding2D(padding=((1, 0), (4, 0)), name = 'Zer24138', )(Add13360)
Add68266 = keras.layers.Add(name = 'Add68266', )([Con95210,Zer24138])
ELU68176 = keras.layers.ELU(alpha=-1.535240583793252, name = 'ELU68176', )(Add68266)
model = tf.keras.models.Model(inputs=[in0Add78412,in1Add78412,in0Con39849,in0Bat82406,in0Con95210,in0Min15985,in1Min15985,in0Con96155,in0Up_11909], outputs=ELU68176)
w = model.get_layer('Bat82406').get_weights() 
w[0] = np.array([0.616, 0.4189, 0.5659])
w[1] = np.array([0.3513, 0.0999, 0.6845])
w[2] = np.array([0.0732, 0.2974, 0.4375])
w[3] = np.array([0.0085, 0.0826, 0.8878])
model.get_layer('Bat82406').set_weights(w) 
in0Add78412 = tf.constant([[[[0.7865], [0.6425]], [[0.3646], [0.5498]]]])
in1Add78412 = tf.constant([[[[0.7021], [0.6963]], [[0.2104], [0.8198]]]])
in0Con39849 = tf.constant([[[0.1719], [0.7229], [0.6419]]])
in0Bat82406 = tf.constant([[[1.3992, 1.7776], [1.5743, 1.2435], [1.458, 1.2586]]])
in0Con95210 = tf.constant([[[[0.8804, 0.9245], [0.8958, 0.0845], [0.22, 0.7934], [0.1798, 0.2307], [0.5662, 0.4462], [0.5436, 0.3845], [0.268, 0.3766], [0.0958, 0.9161], [0.7153, 0.0224], [0.6266, 0.3178]], [[0.515, 0.8922], [0.4793, 0.4259], [0.2156, 0.9575], [0.9092, 0.2155], [0.2126, 0.9457], [0.6124, 0.0312], [0.2016, 0.0611], [0.2262, 0.0167], [0.3002, 0.8341], [0.2529, 0.1987]], [[0.7512, 0.2528], [0.0388, 0.1493], [0.9421, 0.0662], [0.1518, 0.4023], [0.3263, 0.1646], [0.7442, 0.8292], [0.7167, 0.2419], [0.2638, 0.9977], [0.3367, 0.523], [0.1778, 0.1267]]]])
in0Min15985 = tf.constant([[[[0.0629], [0.4001]]]])
in1Min15985 = tf.constant([[[[0.6792], [0.966]]]])
in0Con96155 = tf.constant([[[[0.4218, 0.908], [0.3763, 0.6303], [0.1456, 0.9925], [0.5981, 0.7617], [0.1389, 0.4096], [0.0451, 0.7863]], [[0.7621, 0.1506], [0.8013, 0.8609], [0.0658, 0.6315], [0.3639, 0.3499], [0.133, 0.9796], [0.8622, 0.8703]]]])
in0Up_11909 = tf.constant([[[[1.6034, 1.7897, 1.7681], [1.4733, 1.5399, 1.2461], [1.9318, 1.7211, 1.4247]], [[1.3515, 1.5886, 1.8181], [1.1073, 1.2545, 1.5192], [1.6073, 1.6183, 1.412]]]])
print (np.array2string(model.predict([in0Add78412,in1Add78412,in0Con39849,in0Bat82406,in0Con95210,in0Min15985,in1Min15985,in0Con96155,in0Up_11909],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='ELU68176.png')

LAdd78412 = add_layer([[[[[0.7865], [0.6425]], [[0.3646], [0.5498]]]], [[[[0.7021], [0.6963]], [[0.2104], [0.8198]]]]], Add78412), 
LGlo86233 = global_average_pooling2D_layer(Add78412, Glo86233), 
LRes31585 = reshape_layer(Glo86233, [1, 1], Res31585), 
LZer45715 = zero_padding1D_layer(Res31585, 2, 0, Zer45715), 
LCon39849 = concatenate_layer([Zer45715,[[[0.1719], [0.7229], [0.6419]]]], 2, Con39849), 
LBat82406 = batch_normalization_layer([[[1.3992, 1.7776], [1.5743, 1.2435], [1.458, 1.2586]]], 1, 0.5863367673600033, [0.616, 0.4189, 0.5659], [0.3513, 0.0999, 0.6845], [0.0732, 0.2974, 0.4375], [0.0085, 0.0826, 0.8878], Bat82406), 
LAve90214 = average_layer([Con39849,Bat82406], Ave90214), 
LRes23912 = reshape_layer(Ave90214, [3, 2, 1], Res23912), 
LZer65559 = zero_padding2D_layer(Res23912, 0, 0, 8, 0, Zer65559), 
LCon95210 = concatenate_layer([Zer65559,[[[[0.8804, 0.9245], [0.8958, 0.0845], [0.22, 0.7934], [0.1798, 0.2307], [0.5662, 0.4462], [0.5436, 0.3845], [0.268, 0.3766], [0.0958, 0.9161], [0.7153, 0.0224], [0.6266, 0.3178]], [[0.515, 0.8922], [0.4793, 0.4259], [0.2156, 0.9575], [0.9092, 0.2155], [0.2126, 0.9457], [0.6124, 0.0312], [0.2016, 0.0611], [0.2262, 0.0167], [0.3002, 0.8341], [0.2529, 0.1987]], [[0.7512, 0.2528], [0.0388, 0.1493], [0.9421, 0.0662], [0.1518, 0.4023], [0.3263, 0.1646], [0.7442, 0.8292], [0.7167, 0.2419], [0.2638, 0.9977], [0.3367, 0.523], [0.1778, 0.1267]]]]], 3, Con95210), 
LMin15985 = minimum_layer([[[[[0.0629], [0.4001]]]], [[[[0.6792], [0.966]]]]], Min15985), 
LZer95475 = zero_padding2D_layer(Min15985, 1, 0, 4, 0, Zer95475), 
LCon96155 = concatenate_layer([Zer95475,[[[[0.4218, 0.908], [0.3763, 0.6303], [0.1456, 0.9925], [0.5981, 0.7617], [0.1389, 0.4096], [0.0451, 0.7863]], [[0.7621, 0.1506], [0.8013, 0.8609], [0.0658, 0.6315], [0.3639, 0.3499], [0.133, 0.9796], [0.8622, 0.8703]]]]], 3, Con96155), 
LUp_11909 = up_sampling2D_layer([[[[1.6034, 1.7897, 1.7681], [1.4733, 1.5399, 1.2461], [1.9318, 1.7211, 1.4247]], [[1.3515, 1.5886, 1.8181], [1.1073, 1.2545, 1.5192], [1.6073, 1.6183, 1.412]]]], 1, 2, Up_11909), 
LAdd13360 = add_layer([Con96155,Up_11909], Add13360), 
LZer24138 = zero_padding2D_layer(Add13360, 1, 0, 4, 0, Zer24138), 
LAdd68266 = add_layer([Con95210,Zer24138], Add68266), 
LELU68176 = elu_layer(Add68266, -1.535240583793252, ELU68176), 
exec_layers([LAdd78412,LGlo86233,LRes31585,LZer45715,LCon39849,LBat82406,LAve90214,LRes23912,LZer65559,LCon95210,LMin15985,LZer95475,LCon96155,LUp_11909,LAdd13360,LZer24138,LAdd68266,LELU68176],["Add78412","Glo86233","Res31585","Zer45715","Con39849","Bat82406","Ave90214","Res23912","Zer65559","Con95210","Min15985","Zer95475","Con96155","Up_11909","Add13360","Zer24138","Add68266","ELU68176"],ELU68176,"ELU68176")

Actual (Unparsed): [[[[-0.0000000, 0.8804000, 0.9245000], [-0.0000000, 0.8958000, 0.0845000], [-0.0000000, 0.2200000, 0.7934000], [-0.0000000, 0.1798000, 0.2307000], [-0.0000000, 0.5662000, 0.4462000], [-0.0000000, 0.5436000, 0.3845000], [-0.0000000, 0.2680000, 0.3766000], [-0.0000000, 0.0958000, 0.9161000], [0.7051858, 0.7153000, 0.0224000], [0.9422492, 0.6266000, 0.3178000]], [[-0.0000000, 0.5150000, 0.8922000], [-0.0000000, 0.4793000, 0.4259000], [-0.0000000, 0.2156000, 0.9575000], [-0.0000000, 0.9092000, 0.2155000], [1.6034000, 2.4241000, 3.6218000], [1.6034000, 2.7784000, 2.4296000], [1.4733000, 1.8871000, 2.2997000], [1.4733000, 2.3642000, 2.0244999], [2.3087477, 2.1602000, 2.6684000], [2.5854841, 2.0191000, 2.4097000]], [[-0.0000000, 0.7512000, 0.2528000], [-0.0000000, 0.0388000, 0.1493000], [-0.0000000, 0.9421000, 0.0662000], [-0.0000000, 0.1518000, 0.4023000], [1.3515000, 2.6770000, 2.1333000], [1.3515000, 3.1341000, 3.5082000], [1.1073000, 2.0370001, 2.3926000], [1.1073000, 1.8822000, 2.8668000], [2.8467730, 2.0880000, 2.9146000], [2.8619537, 2.6583000, 2.4089999]]]]

Expected (Unparsed): [[[[0,0.8804,0.9245],[0,0.8958,0.0845],[0,0.22,0.7934],[0,0.1798,0.2307],[0,0.5662,0.4462],[0,0.5436,0.3845],[0,0.268,0.3766],[0,0.0958,0.9161],[0.7051858142183891,0.7153,0.0224],[0.9422492019259596,0.6266,0.3178]],[[0,0.515,0.8922],[0,0.4793,0.4259],[0,0.2156,0.9575],[0,0.9092,0.2155],[1.6034,2.4241,3.6218],[1.6034,2.7784,2.4296],[1.4733,1.8871,2.2997],[1.4733,2.3642,2.0245],[2.3087477151537033,2.1602,2.6684],[2.585484077301996,2.0191,2.4097000000000004]],[[0,0.7512,0.2528],[0,0.0388,0.1493],[0,0.9421,0.0662],[0,0.1518,0.4023],[1.3515,2.6769999999999996,2.1333],[1.3515,3.1341,3.5082000000000004],[1.1073,2.037,2.3926],[1.1073,1.8821999999999999,2.8668],[2.846772980817812,2.088,2.9146],[2.86195369872563,2.6583,2.409]]]]

Actual:   [[[[-0, 0.8804, 0.9245], [-0, 0.8958, 0.0845], [-0, 0.22, 0.7934], [-0, 0.1798, 0.2307], [-0, 0.5662, 0.4462], [-0, 0.5436, 0.3845], [-0, 0.268, 0.3766], [-0, 0.0958, 0.9161], [0.7052, 0.7153, 0.0224], [0.9423, 0.6266, 0.3178]], [[-0, 0.515, 0.8922], [-0, 0.4793, 0.4259], [-0, 0.2156, 0.9575], [-0, 0.9092, 0.2155], [1.6034, 2.4241, 3.6218], [1.6034, 2.7784, 2.4296], [1.4733, 1.8871, 2.2997], [1.4733, 2.3642, 2.0245], [2.3088, 2.1602, 2.6684], [2.5855, 2.0191, 2.4097]], [[-0, 0.7512, 0.2528], [-0, 0.0388, 0.1493], [-0, 0.9421, 0.0662], [-0, 0.1518, 0.4023], [1.3515, 2.677, 2.1333], [1.3515, 3.1341, 3.5082], [1.1073, 2.0371, 2.3926], [1.1073, 1.8822, 2.8668], [2.8468, 2.088, 2.9146], [2.862, 2.6583, 2.409]]]]

Expected: [[[[0, 0.8804, 0.9245], [0, 0.8958, 0.0845], [0, 0.22, 0.7934], [0, 0.1798, 0.2307], [0, 0.5662, 0.4462], [0, 0.5436, 0.3845], [0, 0.268, 0.3766], [0, 0.0958, 0.9161], [0.7052, 0.7153, 0.0224], [0.9423, 0.6266, 0.3178]], [[0, 0.515, 0.8922], [0, 0.4793, 0.4259], [0, 0.2156, 0.9575], [0, 0.9092, 0.2155], [1.6034, 2.4241, 3.6218], [1.6034, 2.7784, 2.4296], [1.4733, 1.8871, 2.2997], [1.4733, 2.3642, 2.0245], [2.3088, 2.1602, 2.6684], [2.5855, 2.0191, 2.4098]], [[0, 0.7512, 0.2528], [0, 0.0388, 0.1493], [0, 0.9421, 0.0662], [0, 0.1518, 0.4023], [1.3515, 2.677, 2.1333], [1.3515, 3.1341, 3.5083], [1.1073, 2.037, 2.3926], [1.1073, 1.8822, 2.8668], [2.8468, 2.088, 2.9146], [2.862, 2.6583, 2.409]]]]