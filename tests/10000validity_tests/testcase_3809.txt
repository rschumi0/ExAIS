import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Zer72104 = tf.keras.layers.Input(shape=([1, 1, 4, 1]))
in0Glo86724 = tf.keras.layers.Input(shape=([2, 1]))
in0Con36394 = tf.keras.layers.Input(shape=([53]))
in0Lay85945 = tf.keras.layers.Input(shape=([2, 1, 2]))
in0Con15471 = tf.keras.layers.Input(shape=([53]))

Zer72104 = keras.layers.ZeroPadding3D(padding=((1, 1), (1, 1), (1, 1)), name = 'Zer72104', )(in0Zer72104)
Res52345 = keras.layers.Reshape((3, 3, 6), name = 'Res52345', )(Zer72104)
Res9345 = keras.layers.Reshape((3, 18), name = 'Res9345', )(Res52345)
Fla13885 = keras.layers.Flatten(name = 'Fla13885', )(Res9345)
Glo86724 = keras.layers.GlobalAveragePooling1D(name = 'Glo86724', )(in0Glo86724)
Con36394 = keras.layers.Concatenate(axis=1, name = 'Con36394', )([Glo86724,in0Con36394])
Mul79199 = keras.layers.Multiply(name = 'Mul79199', )([Fla13885,Con36394])
Sof5600 = keras.layers.Softmax(axis=1, name = 'Sof5600', )(Mul79199)
Lay85945 = keras.layers.LayerNormalization(axis=3, epsilon=1.7557965235243278, name = 'Lay85945', )(in0Lay85945)
Res48240 = keras.layers.Reshape((2, 2), name = 'Res48240', )(Lay85945)
Fla81463 = keras.layers.Flatten(name = 'Fla81463', )(Res48240)
Res53948 = keras.layers.Reshape((4, 1), name = 'Res53948', )(Fla81463)
PRe41278 = keras.layers.PReLU(name = 'PRe41278', )(Res53948)
Res79129 = keras.layers.Reshape((4, 1, 1), name = 'Res79129', )(PRe41278)
Glo18744 = keras.layers.GlobalAveragePooling2D(name = 'Glo18744', )(Res79129)
Con15471 = keras.layers.Concatenate(axis=1, name = 'Con15471', )([Glo18744,in0Con15471])
Add92463 = keras.layers.Add(name = 'Add92463', )([Sof5600,Con15471])
model = tf.keras.models.Model(inputs=[in0Zer72104,in0Glo86724,in0Con36394,in0Lay85945,in0Con15471], outputs=Add92463)
w = model.get_layer('PRe41278').get_weights() 
w[0] = np.array([[0.9049], [0.189], [0.5616], [0.4353]])
model.get_layer('PRe41278').set_weights(w) 
in0Zer72104 = tf.constant([[[[[1.3538], [1.563], [1.2105], [1.489]]]]])
in0Glo86724 = tf.constant([[[1.4752], [1.7522]]])
in0Con36394 = tf.constant([[0.8469, 0.7615, 0.64, 0.2707, 0.1213, 0.9399, 0.7646, 0.7642, 0.368, 0.9452, 0.3784, 0.9818, 0.7049, 0.4446, 0.0758, 0.1792, 0.9779, 0.4958, 0.6855, 0.2047, 0.9772, 0.9814, 0.4367, 0.3309, 0.9095, 0.4828, 0.8486, 0.5088, 0.1377, 0.2098, 0.1691, 0.2724, 0.7096, 0.5182, 0.5868, 0.9577, 0.7369, 0.7583, 0.3718, 0.1228, 0.2566, 0.6786, 0.5623, 0.0532, 0.8092, 0.3755, 0.8082, 0.0222, 0.6032, 0.8318, 0.5051, 0.5372, 0.7998]])
in0Lay85945 = tf.constant([[[[1.3486, 1.7805]], [[1.4704, 1.5218]]]])
in0Con15471 = tf.constant([[0.9357, 0.6937, 0.8994, 0.768, 0.1085, 0.925, 0.4908, 0.7532, 0.8271, 0.6445, 0.8098, 0.2009, 0.4482, 0.2443, 0.0805, 0.6376, 0.9142, 0.3502, 0.7346, 0.6923, 0.1068, 0.4593, 0.7831, 0.2834, 0.8097, 0.8748, 0.0066, 0.989, 0.5644, 0.006, 0.7315, 0.4219, 0.2369, 0.4898, 0.7541, 0.7051, 0.4246, 0.003, 0.0555, 0.9489, 0.9278, 0.081, 0.3091, 0.9692, 0.3887, 0.7372, 0.8272, 0.351, 0.8534, 0.1841, 0.5108, 0.366, 0.4553]])
print (np.array2string(model.predict([in0Zer72104,in0Glo86724,in0Con36394,in0Lay85945,in0Con15471],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Add92463.png')

LZer72104 = zero_padding3D_layer([[[[[1.3538], [1.563], [1.2105], [1.489]]]]], 1, 1, 1, 1, 1, 1, Zer72104), 
LRes52345 = reshape_layer(Zer72104, [3, 3, 6], Res52345), 
LRes9345 = reshape_layer(Res52345, [3, 18], Res9345), 
LFla13885 = flatten_layer(Res9345, Fla13885), 
LGlo86724 = global_average_pooling1D_layer([[[1.4752], [1.7522]]], Glo86724), 
LCon36394 = concatenate_layer([Glo86724,[[0.8469, 0.7615, 0.64, 0.2707, 0.1213, 0.9399, 0.7646, 0.7642, 0.368, 0.9452, 0.3784, 0.9818, 0.7049, 0.4446, 0.0758, 0.1792, 0.9779, 0.4958, 0.6855, 0.2047, 0.9772, 0.9814, 0.4367, 0.3309, 0.9095, 0.4828, 0.8486, 0.5088, 0.1377, 0.2098, 0.1691, 0.2724, 0.7096, 0.5182, 0.5868, 0.9577, 0.7369, 0.7583, 0.3718, 0.1228, 0.2566, 0.6786, 0.5623, 0.0532, 0.8092, 0.3755, 0.8082, 0.0222, 0.6032, 0.8318, 0.5051, 0.5372, 0.7998]]], 1, Con36394), 
LMul79199 = multiply_layer([Fla13885,Con36394], Mul79199), 
LSof5600 = softmax_layer(Mul79199, 1, Sof5600), 
LLay85945 = layer_normalization_layer([[[[1.3486, 1.7805]], [[1.4704, 1.5218]]]], 3, 1.7557965235243278, Lay85945), 
LRes48240 = reshape_layer(Lay85945, [2, 2], Res48240), 
LFla81463 = flatten_layer(Res48240, Fla81463), 
LRes53948 = reshape_layer(Fla81463, [4, 1], Res53948), 
LPRe41278 = prelu_layer(Res53948, [[0.9049], [0.189], [0.5616], [0.4353]], PRe41278), 
LRes79129 = reshape_layer(PRe41278, [4, 1, 1], Res79129), 
LGlo18744 = global_average_pooling2D_layer(Res79129, Glo18744), 
LCon15471 = concatenate_layer([Glo18744,[[0.9357, 0.6937, 0.8994, 0.768, 0.1085, 0.925, 0.4908, 0.7532, 0.8271, 0.6445, 0.8098, 0.2009, 0.4482, 0.2443, 0.0805, 0.6376, 0.9142, 0.3502, 0.7346, 0.6923, 0.1068, 0.4593, 0.7831, 0.2834, 0.8097, 0.8748, 0.0066, 0.989, 0.5644, 0.006, 0.7315, 0.4219, 0.2369, 0.4898, 0.7541, 0.7051, 0.4246, 0.003, 0.0555, 0.9489, 0.9278, 0.081, 0.3091, 0.9692, 0.3887, 0.7372, 0.8272, 0.351, 0.8534, 0.1841, 0.5108, 0.366, 0.4553]]], 1, Con15471), 
LAdd92463 = add_layer([Sof5600,Con15471], Add92463), 
exec_layers([LZer72104,LRes52345,LRes9345,LFla13885,LGlo86724,LCon36394,LMul79199,LSof5600,LLay85945,LRes48240,LFla81463,LRes53948,LPRe41278,LRes79129,LGlo18744,LCon15471,LAdd92463],["Zer72104","Res52345","Res9345","Fla13885","Glo86724","Con36394","Mul79199","Sof5600","Lay85945","Res48240","Fla81463","Res53948","PRe41278","Res79129","Glo18744","Con15471","Add92463"],Add92463,"Add92463")

Actual (Unparsed): [[0.0224843, 0.9522347, 0.7102347, 0.9159347, 0.7845347, 0.1250347, 0.9415347, 0.5073347, 0.7697347, 0.8436347, 0.6610347, 0.8263347, 0.2174347, 0.4647347, 0.2608347, 0.0970347, 0.6541347, 0.9307347, 0.3667347, 0.7511347, 0.7088347, 0.1233347, 0.4758347, 0.7996347, 0.2999347, 0.8663415, 0.9099659, 0.0527867, 1.0242711, 0.5809347, 0.0225347, 0.7480347, 0.4384347, 0.2534347, 0.5063347, 0.7706347, 0.7216347, 0.4411347, 0.0195347, 0.0720347, 0.9654347, 0.9443347, 0.0975347, 0.3256347, 0.9857347, 0.4052347, 0.7537347, 0.8437347, 0.3675347, 0.8699347, 0.2006347, 0.5273347, 0.3825347, 0.4718347]]

Expected (Unparsed): [[0.022484252444536754,0.9522346959270522,0.7102346959270522,0.9159346959270522,0.7845346959270523,0.1250346959270523,0.9415346959270523,0.5073346959270523,0.7697346959270522,0.8436346959270522,0.6610346959270522,0.8263346959270522,0.2174346959270523,0.4647346959270523,0.2608346959270523,0.0970346959270523,0.6541346959270522,0.9307346959270523,0.3667346959270523,0.7511346959270523,0.7088346959270523,0.1233346959270523,0.4758346959270523,0.7996346959270523,0.2999346959270523,0.86634150705322,0.9099659173560297,0.05278667130674161,1.0242711079313938,0.5809346959270523,0.022534695927052303,0.7480346959270523,0.4384346959270523,0.2534346959270523,0.5063346959270523,0.7706346959270522,0.7216346959270522,0.4411346959270523,0.0195346959270523,0.0720346959270523,0.9654346959270522,0.9443346959270522,0.0975346959270523,0.3256346959270523,0.9857346959270522,0.4052346959270523,0.7537346959270522,0.8437346959270523,0.3675346959270523,0.8699346959270523,0.20063469592705233,0.5273346959270523,0.3825346959270523,0.4718346959270523]]

Actual:   [[0.0225, 0.9523, 0.7103, 0.916, 0.7846, 0.1251, 0.9416, 0.5074, 0.7698, 0.8437, 0.6611, 0.8264, 0.2175, 0.4648, 0.2609, 0.0971, 0.6542, 0.9308, 0.3668, 0.7512, 0.7089, 0.1234, 0.4759, 0.7997, 0.3, 0.8664, 0.91, 0.0528, 1.0243, 0.581, 0.0226, 0.7481, 0.4385, 0.2535, 0.5064, 0.7707, 0.7217, 0.4412, 0.0196, 0.0721, 0.9655, 0.9444, 0.0976, 0.3257, 0.9858, 0.4053, 0.7538, 0.8438, 0.3676, 0.87, 0.2007, 0.5274, 0.3826, 0.4719]]

Expected: [[0.0225, 0.9523, 0.7103, 0.916, 0.7846, 0.1251, 0.9416, 0.5074, 0.7698, 0.8437, 0.6611, 0.8264, 0.2175, 0.4648, 0.2609, 0.0971, 0.6542, 0.9308, 0.3668, 0.7512, 0.7089, 0.1234, 0.4759, 0.7997, 0.3, 0.8664, 0.91, 0.0528, 1.0243, 0.581, 0.0226, 0.7481, 0.4385, 0.2535, 0.5064, 0.7707, 0.7217, 0.4412, 0.0196, 0.0721, 0.9655, 0.9444, 0.0976, 0.3257, 0.9858, 0.4053, 0.7538, 0.8438, 0.3676, 0.87, 0.2007, 0.5274, 0.3826, 0.4719]]