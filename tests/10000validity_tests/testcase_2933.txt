import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Bat9000 = tf.keras.layers.Input(shape=([2]))
in0Dot33028 = tf.keras.layers.Input(shape=([3]))
in1Dot33028 = tf.keras.layers.Input(shape=([3]))
in0Lea69059 = tf.keras.layers.Input(shape=([1, 2, 1]))

Bat9000 = keras.layers.BatchNormalization(axis=1, epsilon=0.49380150968633674,  name = 'Bat9000', )(in0Bat9000)
Res41248 = keras.layers.Reshape((2, 1), name = 'Res41248', )(Bat9000)
Dot33028 = keras.layers.Dot(axes=(1, 1), name = 'Dot33028', )([in0Dot33028,in1Dot33028])
Res40352 = keras.layers.Reshape((1, 1), name = 'Res40352', )(Dot33028)
Up_73975 = keras.layers.UpSampling1D(size=(2), name = 'Up_73975', )(Res40352)
Dot76457 = keras.layers.Dot(axes=(1, 1), name = 'Dot76457', )([Res41248,Up_73975])
Res64227 = keras.layers.Reshape((1, 1, 1), name = 'Res64227', )(Dot76457)
Zer72222 = keras.layers.ZeroPadding2D(padding=((0, 0), (1, 0)), name = 'Zer72222', )(Res64227)
Lea69059 = keras.layers.LeakyReLU(alpha=8.538704533193352, name = 'Lea69059', input_shape=(1, 2, 1))(in0Lea69059)
Max62909 = keras.layers.Maximum(name = 'Max62909', )([Zer72222,Lea69059])
model = tf.keras.models.Model(inputs=[in0Bat9000,in0Dot33028,in1Dot33028,in0Lea69059], outputs=Max62909)
w = model.get_layer('Bat9000').get_weights() 
w[0] = np.array([0.1421, 0.6206])
w[1] = np.array([0.4499, 0.4843])
w[2] = np.array([0.2901, 0.1342])
w[3] = np.array([0.9702, 0.3974])
model.get_layer('Bat9000').set_weights(w) 
in0Bat9000 = tf.constant([[1.5327, 1.0798]])
in0Dot33028 = tf.constant([[0.7101, 0.6109, 0.08]])
in1Dot33028 = tf.constant([[0.0787, 0.077, 0.5292]])
in0Lea69059 = tf.constant([[[[0.6245], [0.2937]]]])
print (np.array2string(model.predict([in0Bat9000,in0Dot33028,in1Dot33028,in0Lea69059],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Max62909.png')

LBat9000 = batch_normalization_layer([[1.5327, 1.0798]], 1, 0.49380150968633674, [0.1421, 0.6206], [0.4499, 0.4843], [0.2901, 0.1342], [0.9702, 0.3974], Bat9000), 
LRes41248 = reshape_layer(Bat9000, [2, 1], Res41248), 
LDot33028 = dot_layer([[0.7101, 0.6109, 0.08]], [[0.0787, 0.077, 0.5292]], 1, 1, Dot33028), 
LRes40352 = reshape_layer(Dot33028, [1, 1], Res40352), 
LUp_73975 = up_sampling1D_layer(Res40352, 2, Up_73975), 
LDot76457 = dot_layer(Res41248,Up_73975, 1, 1, Dot76457), 
LRes64227 = reshape_layer(Dot76457, [1, 1, 1], Res64227), 
LZer72222 = zero_padding2D_layer(Res64227, 0, 0, 1, 0, Zer72222), 
LLea69059 = leaky_relu_layer([[[[0.6245], [0.2937]]]], 8.538704533193352, Lea69059), 
LMax62909 = maximum_layer([Zer72222,Lea69059], Max62909), 
exec_layers([LBat9000,LRes41248,LDot33028,LRes40352,LUp_73975,LDot76457,LRes64227,LZer72222,LLea69059,LMax62909],["Bat9000","Res41248","Dot33028","Res40352","Up_73975","Dot76457","Res64227","Zer72222","Lea69059","Max62909"],Max62909,"Max62909")

Actual (Unparsed): [[[[0.6245000], [0.2937000]]]]

Expected (Unparsed): [[[[0.6245],[0.2937]]]]

Actual:   [[[[0.6245], [0.2937]]]]

Expected: [[[[0.6245], [0.2937]]]]