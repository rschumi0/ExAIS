import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Bat29465 = tf.keras.layers.Input(shape=([1, 3, 3]))

Bat29465 = keras.layers.BatchNormalization(axis=1, epsilon=0.40558531887737304,  name = 'Bat29465', )(in0Bat29465)
Fla85783 = keras.layers.Flatten(name = 'Fla85783', )(Bat29465)
Sof17502 = keras.layers.Softmax(axis=1, name = 'Sof17502', )(Fla85783)
model = tf.keras.models.Model(inputs=[in0Bat29465], outputs=Sof17502)
w = model.get_layer('Bat29465').get_weights() 
w[0] = np.array([0.5178])
w[1] = np.array([0.3913])
w[2] = np.array([0.7033])
w[3] = np.array([0.2538])
model.get_layer('Bat29465').set_weights(w) 
in0Bat29465 = tf.constant([[[[1.0166, 1.5291, 1.1301], [1.0511, 1.3113, 1.5772], [1.9517, 1.3014, 1.6555]]]])
print (np.array2string(model.predict([in0Bat29465],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Sof17502.png')

LBat29465 = batch_normalization_layer([[[[1.0166, 1.5291, 1.1301], [1.0511, 1.3113, 1.5772], [1.9517, 1.3014, 1.6555]]]], 1, 0.40558531887737304, [0.5178], [0.3913], [0.7033], [0.2538], Bat29465), 
LFla85783 = flatten_layer(Bat29465, Fla85783), 
LSof17502 = softmax_layer(Fla85783, 1, Sof17502), 
exec_layers([LBat29465,LFla85783,LSof17502],["Bat29465","Fla85783","Sof17502"],Sof17502,"Sof17502")

Actual (Unparsed): [[0.0859277, 0.1191412, 0.0923773, 0.0878390, 0.1036921, 0.1228521, 0.1559892, 0.1030396, 0.1291417]]

Expected (Unparsed): [[0.08592771251265459,0.11914122743287693,0.09237730574399525,0.08783902029958471,0.10369212708097743,0.12285210690285189,0.1559891880709034,0.10303959287278344,0.12914171908337257]]

Actual:   [[0.086, 0.1192, 0.0924, 0.0879, 0.1037, 0.1229, 0.156, 0.1031, 0.1292]]

Expected: [[0.086, 0.1192, 0.0924, 0.0879, 0.1037, 0.1229, 0.156, 0.1031, 0.1292]]