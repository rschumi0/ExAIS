import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Sub41526 = tf.keras.layers.Input(shape=([2, 3, 3, 2]))
in1Sub41526 = tf.keras.layers.Input(shape=([2, 3, 3, 2]))
in0Bat99291 = tf.keras.layers.Input(shape=([1, 2]))
in0Con45087 = tf.keras.layers.Input(shape=([2, 16]))

Sub41526 = keras.layers.Subtract(name = 'Sub41526', )([in0Sub41526,in1Sub41526])
Lay44394 = keras.layers.LayerNormalization(axis=1, epsilon=2.3644659497449796, name = 'Lay44394', )(Sub41526)
Res43625 = keras.layers.Reshape((2, 3, 6), name = 'Res43625', )(Lay44394)
Res18300 = keras.layers.Reshape((2, 18), name = 'Res18300', )(Res43625)
Bat99291 = keras.layers.BatchNormalization(axis=2, epsilon=0.5282730099102109,  name = 'Bat99291', )(in0Bat99291)
Lea94428 = keras.layers.LeakyReLU(alpha=9.012696358187975, name = 'Lea94428', )(Bat99291)
Zer56130 = keras.layers.ZeroPadding1D(padding=((1, 0)), name = 'Zer56130', )(Lea94428)
Con45087 = keras.layers.Concatenate(axis=2, name = 'Con45087', )([Zer56130,in0Con45087])
Min19831 = keras.layers.Minimum(name = 'Min19831', )([Res18300,Con45087])
model = tf.keras.models.Model(inputs=[in0Sub41526,in1Sub41526,in0Bat99291,in0Con45087], outputs=Min19831)
w = model.get_layer('Bat99291').get_weights() 
w[0] = np.array([0.9321, 0.4437])
w[1] = np.array([0.4795, 0.5622])
w[2] = np.array([0.8014, 0.507])
w[3] = np.array([0.0237, 0.9515])
model.get_layer('Bat99291').set_weights(w) 
in0Sub41526 = tf.constant([[[[[0.688, 0.1387], [0.4188, 0.5481], [0.2277, 0.8174]], [[0.1143, 0.927], [0.5425, 0.7939], [0.7874, 0.9975]], [[0.0774, 0.3735], [0.4552, 0.2693], [0.8232, 0.0074]]], [[[0.4173, 0.2612], [0.6397, 0.1115], [0.6909, 0.9246]], [[0.2125, 0.4157], [0.3967, 0.2582], [0.9447, 0.9533]], [[0.3593, 0.8333], [0.3322, 0.6513], [0.203, 0.1919]]]]])
in1Sub41526 = tf.constant([[[[[0.4049, 0.3678], [0.5224, 0.8532], [0.63, 0.1359]], [[0.2853, 0.9711], [0.2681, 0.3212], [0.4864, 0.5214]], [[0.5483, 0.3765], [0.2125, 0.2659], [0.1851, 0.8872]]], [[[0.7001, 0.64], [0.7015, 0.6404], [0.6743, 0.1405]], [[0.738, 0.5207], [0.2406, 0.7537], [0.0317, 0.1411]], [[0.8674, 0.4545], [0.03, 0.2489], [0.548, 0.8349]]]]])
in0Bat99291 = tf.constant([[[1.1676, 1.1112]]])
in0Con45087 = tf.constant([[[0.2954, 0.8558, 0.4412, 0.7535, 0.2815, 0.3216, 0.8221, 0.1917, 0.3716, 0.2558, 0.3799, 0.4489, 0.5518, 0.1039, 0.3149, 0.6564], [0.6017, 0.7681, 0.8133, 0.0524, 0.1566, 0.1543, 0.6946, 0.206, 0.0947, 0.8057, 0.3919, 0.9367, 0.0534, 0.3439, 0.2123, 0.1242]]])
print (np.array2string(model.predict([in0Sub41526,in1Sub41526,in0Bat99291,in0Con45087],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Min19831.png')

LSub41526 = subtract_layer([[[[[0.688, 0.1387], [0.4188, 0.5481], [0.2277, 0.8174]], [[0.1143, 0.927], [0.5425, 0.7939], [0.7874, 0.9975]], [[0.0774, 0.3735], [0.4552, 0.2693], [0.8232, 0.0074]]], [[[0.4173, 0.2612], [0.6397, 0.1115], [0.6909, 0.9246]], [[0.2125, 0.4157], [0.3967, 0.2582], [0.9447, 0.9533]], [[0.3593, 0.8333], [0.3322, 0.6513], [0.203, 0.1919]]]]], [[[[[0.4049, 0.3678], [0.5224, 0.8532], [0.63, 0.1359]], [[0.2853, 0.9711], [0.2681, 0.3212], [0.4864, 0.5214]], [[0.5483, 0.3765], [0.2125, 0.2659], [0.1851, 0.8872]]], [[[0.7001, 0.64], [0.7015, 0.6404], [0.6743, 0.1405]], [[0.738, 0.5207], [0.2406, 0.7537], [0.0317, 0.1411]], [[0.8674, 0.4545], [0.03, 0.2489], [0.548, 0.8349]]]]], Sub41526), 
LLay44394 = layer_normalization_layer(Sub41526, 1, 2.3644659497449796, Lay44394), 
LRes43625 = reshape_layer(Lay44394, [2, 3, 6], Res43625), 
LRes18300 = reshape_layer(Res43625, [2, 18], Res18300), 
LBat99291 = batch_normalization_layer([[[1.1676, 1.1112]]], 2, 0.5282730099102109, [0.9321, 0.4437], [0.4795, 0.5622], [0.8014, 0.507], [0.0237, 0.9515], Bat99291), 
LLea94428 = leaky_relu_layer(Bat99291, 9.012696358187975, Lea94428), 
LZer56130 = zero_padding1D_layer(Lea94428, 1, 0, Zer56130), 
LCon45087 = concatenate_layer([Zer56130,[[[0.2954, 0.8558, 0.4412, 0.7535, 0.2815, 0.3216, 0.8221, 0.1917, 0.3716, 0.2558, 0.3799, 0.4489, 0.5518, 0.1039, 0.3149, 0.6564], [0.6017, 0.7681, 0.8133, 0.0524, 0.1566, 0.1543, 0.6946, 0.206, 0.0947, 0.8057, 0.3919, 0.9367, 0.0534, 0.3439, 0.2123, 0.1242]]]], 2, Con45087), 
LMin19831 = minimum_layer([Res18300,Con45087], Min19831), 
exec_layers([LSub41526,LLay44394,LRes43625,LRes18300,LBat99291,LLea94428,LZer56130,LCon45087,LMin19831],["Sub41526","Lay44394","Res43625","Res18300","Bat99291","Lea94428","Zer56130","Con45087","Min19831"],Min19831,"Min19831")

Actual (Unparsed): [[[0.0000000, 0.0000000, -0.0135906, 0.0725799, -0.1349652, -0.0333434, 0.1145126, 0.0197987, 0.0384386, 0.1917000, -0.1951738, -0.1086410, 0.0120952, -0.1232021, -0.0193437, -0.1286624, 0.3044901, -0.0767718], [-0.1809724, -0.0486196, 0.0135906, -0.0725799, 0.1349652, 0.0333434, -0.1145126, -0.0197987, -0.0384386, -0.3002943, 0.0947000, 0.1086410, -0.0120952, 0.1232021, 0.0193437, 0.1286624, -0.3044901, 0.0767718]]]

Expected (Unparsed): [[[0,0,-0.013590631656839965,0.07257994455965844,-0.13496522819528578,-0.03334335366600521,0.1145126313839542,0.0197986523296332,0.03843856265051668,0.1917,-0.19517378337083469,-0.10864100618971222,0.012095244232545793,-0.12320209710894922,-0.019343683203523893,-0.12866239297888535,0.3044901275880936,-0.07677176737960542],[-0.18097238501141813,-0.04861959737903418,0.013590631656839965,-0.07257994455965844,0.13496522819528578,0.03334335366600521,-0.11451263138395425,-0.0197986523296332,-0.0384385626505167,-0.3002942981523776,0.0947,0.1086410061897123,-0.012095244232545793,0.12320209710894922,0.019343683203523872,0.12866239297888535,-0.3044901275880936,0.07677176737960549]]]

Actual:   [[[0, 0, -0.0135, 0.0726, -0.1349, -0.0333, 0.1146, 0.0198, 0.0385, 0.1917, -0.1951, -0.1086, 0.0121, -0.1232, -0.0193, -0.1286, 0.3045, -0.0767], [-0.1809, -0.0486, 0.0136, -0.0725, 0.135, 0.0334, -0.1145, -0.0197, -0.0384, -0.3002, 0.0947, 0.1087, -0.012, 0.1233, 0.0194, 0.1287, -0.3044, 0.0768]]]

Expected: [[[0, 0, -0.0135, 0.0726, -0.1349, -0.0333, 0.1146, 0.0198, 0.0385, 0.1917, -0.1951, -0.1086, 0.0121, -0.1232, -0.0193, -0.1286, 0.3045, -0.0767], [-0.1809, -0.0486, 0.0136, -0.0725, 0.135, 0.0334, -0.1145, -0.0197, -0.0384, -0.3002, 0.0947, 0.1087, -0.012, 0.1233, 0.0194, 0.1287, -0.3044, 0.0768]]]