import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Sub32063 = tf.keras.layers.Input(shape=([2, 3, 3, 2]))
in1Sub32063 = tf.keras.layers.Input(shape=([2, 3, 3, 2]))
in0Bat49673 = tf.keras.layers.Input(shape=([2]))
in0Con31807 = tf.keras.layers.Input(shape=([2, 3, 3, 1]))

Sub32063 = keras.layers.Subtract(name = 'Sub32063', )([in0Sub32063,in1Sub32063])
Bat49673 = keras.layers.BatchNormalization(axis=1, epsilon=0.20878229538794643,  name = 'Bat49673', )(in0Bat49673)
Res83591 = keras.layers.Reshape((2, 1), name = 'Res83591', )(Bat49673)
Res17574 = keras.layers.Reshape((2, 1, 1), name = 'Res17574', )(Res83591)
Cro15187 = keras.layers.Cropping2D(cropping=((0, 0), (0, 0)), name = 'Cro15187', )(Res17574)
Res19096 = keras.layers.Reshape((2, 1, 1, 1), name = 'Res19096', )(Cro15187)
Up_58838 = keras.layers.UpSampling3D(size=(1, 2, 1), name = 'Up_58838', )(Res19096)
Zer99472 = keras.layers.ZeroPadding3D(padding=((0, 0), (1, 0), (2, 0)), name = 'Zer99472', )(Up_58838)
Con31807 = keras.layers.Concatenate(axis=4, name = 'Con31807', )([Zer99472,in0Con31807])
Min27881 = keras.layers.Minimum(name = 'Min27881', )([Sub32063,Con31807])
model = tf.keras.models.Model(inputs=[in0Sub32063,in1Sub32063,in0Bat49673,in0Con31807], outputs=Min27881)
w = model.get_layer('Bat49673').get_weights() 
w[0] = np.array([0.3648, 0.0096])
w[1] = np.array([0.3072, 0.8186])
w[2] = np.array([0.9618, 0.8737])
w[3] = np.array([0.4199, 0.3554])
model.get_layer('Bat49673').set_weights(w) 
in0Sub32063 = tf.constant([[[[[0.8946, 0.4782], [0.2872, 0.2497], [0.9212, 0.8139]], [[0.9623, 0.9741], [0.975, 0.6379], [0.4305, 0.5393]], [[0.132, 0.3829], [0.4722, 0.1199], [0.3109, 0.6969]]], [[[0.1657, 0.0008], [0.0494, 0.4802], [0.2459, 0.1727]], [[0.3265, 0.5006], [0.876, 0.1254], [0.5002, 0.5467]], [[0.3545, 0.6764], [0.467, 0.7778], [0.7106, 0.3918]]]]])
in1Sub32063 = tf.constant([[[[[0.2238, 0.9212], [0.0989, 0.6576], [0.1134, 0.2275]], [[0.4045, 0.7411], [0.1688, 0.6486], [0.4878, 0.2461]], [[0.1373, 0.5266], [0.0565, 0.2103], [0.9615, 0.1968]]], [[[0.2401, 0.7194], [0.7211, 0.2957], [0.5585, 0.3192]], [[0.299, 0.2417], [0.1266, 0.6955], [0.3253, 0.3061]], [[0.6176, 0.0618], [0.2726, 0.931], [0.7195, 0.4455]]]]])
in0Bat49673 = tf.constant([[1.931, 1.4651]])
in0Con31807 = tf.constant([[[[[0.4582], [0.1207], [0.7866]], [[0.5873], [0.9021], [0.2247]], [[0.5253], [0.8103], [0.8841]]], [[[0.5083], [0.8017], [0.9454]], [[0.6594], [0.0819], [0.1001]], [[0.9129], [0.9457], [0.3756]]]]])
print (np.array2string(model.predict([in0Sub32063,in1Sub32063,in0Bat49673,in0Con31807],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Min27881.png')

LSub32063 = subtract_layer([[[[[0.8946, 0.4782], [0.2872, 0.2497], [0.9212, 0.8139]], [[0.9623, 0.9741], [0.975, 0.6379], [0.4305, 0.5393]], [[0.132, 0.3829], [0.4722, 0.1199], [0.3109, 0.6969]]], [[[0.1657, 0.0008], [0.0494, 0.4802], [0.2459, 0.1727]], [[0.3265, 0.5006], [0.876, 0.1254], [0.5002, 0.5467]], [[0.3545, 0.6764], [0.467, 0.7778], [0.7106, 0.3918]]]]], [[[[[0.2238, 0.9212], [0.0989, 0.6576], [0.1134, 0.2275]], [[0.4045, 0.7411], [0.1688, 0.6486], [0.4878, 0.2461]], [[0.1373, 0.5266], [0.0565, 0.2103], [0.9615, 0.1968]]], [[[0.2401, 0.7194], [0.7211, 0.2957], [0.5585, 0.3192]], [[0.299, 0.2417], [0.1266, 0.6955], [0.3253, 0.3061]], [[0.6176, 0.0618], [0.2726, 0.931], [0.7195, 0.4455]]]]], Sub32063), 
LBat49673 = batch_normalization_layer([[1.931, 1.4651]], 1, 0.20878229538794643, [0.3648, 0.0096], [0.3072, 0.8186], [0.9618, 0.8737], [0.4199, 0.3554], Bat49673), 
LRes83591 = reshape_layer(Bat49673, [2, 1], Res83591), 
LRes17574 = reshape_layer(Res83591, [2, 1, 1], Res17574), 
LCro15187 = cropping2D_layer(Res17574, 0, 0, 0, 0, Cro15187), 
LRes19096 = reshape_layer(Cro15187, [2, 1, 1, 1], Res19096), 
LUp_58838 = up_sampling3D_layer(Res19096, 1, 2, 1, Up_58838), 
LZer99472 = zero_padding3D_layer(Up_58838, 0, 0, 1, 0, 2, 0, Zer99472), 
LCon31807 = concatenate_layer([Zer99472,[[[[[0.4582], [0.1207], [0.7866]], [[0.5873], [0.9021], [0.2247]], [[0.5253], [0.8103], [0.8841]]], [[[0.5083], [0.8017], [0.9454]], [[0.6594], [0.0819], [0.1001]], [[0.9129], [0.9457], [0.3756]]]]]], 4, Con31807), 
LMin27881 = minimum_layer([Sub32063,Con31807], Min27881), 
exec_layers([LSub32063,LBat49673,LRes83591,LRes17574,LCro15187,LRes19096,LUp_58838,LZer99472,LCon31807,LMin27881],["Sub32063","Bat49673","Res83591","Res17574","Cro15187","Res19096","Up_58838","Zer99472","Con31807","Min27881"],Min27881,"Min27881")

Actual (Unparsed): [[[[[0.0000000, -0.4430000], [0.0000000, -0.4079000], [0.0000000, 0.5864000]], [[0.0000000, 0.2330000], [0.0000000, -0.0107000], [-0.0573000, 0.2247000]], [[-0.0053000, -0.1437000], [0.0000000, -0.0904000], [-0.6506000, 0.5001000]]], [[[-0.0744000, -0.7186000], [-0.6717000, 0.1845000], [-0.3126000, -0.1465000]], [[0.0000000, 0.2589000], [0.0000000, -0.5701000], [0.1749000, 0.1001000]], [[-0.2631000, 0.6146000], [0.0000000, -0.1532000], [-0.0089000, -0.0537000]]]]]

Expected (Unparsed): [[[[[0,-0.443],[0,-0.40789999999999993],[0,0.5863999999999999]],[[0,0.23299999999999998],[0,-0.010699999999999932],[-0.05730000000000002,0.2247]],[[-0.005299999999999999,-0.14369999999999994],[0,-0.09039999999999998],[-0.6506000000000001,0.5001]]],[[[-0.07440000000000002,-0.7186],[-0.6717,0.1845],[-0.3126,-0.1465]],[[0,0.2589],[0,-0.5701],[0.1749,0.1001]],[[-0.26310000000000006,0.6146],[0,-0.1532],[-0.008900000000000019,-0.053700000000000025]]]]]

Actual:   [[[[[0, -0.443], [0, -0.4079], [0, 0.5864]], [[0, 0.233], [0, -0.0107], [-0.0573, 0.2247]], [[-0.0053, -0.1437], [0, -0.0904], [-0.6506, 0.5001]]], [[[-0.0744, -0.7186], [-0.6717, 0.1845], [-0.3126, -0.1465]], [[0, 0.2589], [0, -0.5701], [0.1749, 0.1001]], [[-0.2631, 0.6146], [0, -0.1532], [-0.0089, -0.0537]]]]]

Expected: [[[[[0, -0.443], [0, -0.4078], [0, 0.5864]], [[0, 0.233], [0, -0.0106], [-0.0573, 0.2247]], [[-0.0052, -0.1436], [0, -0.0903], [-0.6506, 0.5001]]], [[[-0.0744, -0.7186], [-0.6717, 0.1845], [-0.3126, -0.1465]], [[0, 0.2589], [0, -0.5701], [0.1749, 0.1001]], [[-0.2631, 0.6146], [0, -0.1532], [-0.0089, -0.0537]]]]]