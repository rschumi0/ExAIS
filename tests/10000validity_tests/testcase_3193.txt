import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Bat32533 = tf.keras.layers.Input(shape=([2, 4, 2]))
in0Glo77058 = tf.keras.layers.Input(shape=([2, 2, 1]))
in0Con90722 = tf.keras.layers.Input(shape=([15]))

Bat32533 = keras.layers.BatchNormalization(axis=3, epsilon=0.8910226730903602,  name = 'Bat32533', )(in0Bat32533)
Lea65645 = keras.layers.LeakyReLU(alpha=2.0177936306134017, name = 'Lea65645', )(Bat32533)
Res78963 = keras.layers.Reshape((2, 8), name = 'Res78963', )(Lea65645)
Fla39977 = keras.layers.Flatten(name = 'Fla39977', )(Res78963)
Glo77058 = keras.layers.GlobalMaxPool2D(name = 'Glo77058', )(in0Glo77058)
Con90722 = keras.layers.Concatenate(axis=1, name = 'Con90722', )([Glo77058,in0Con90722])
Sub64784 = keras.layers.Subtract(name = 'Sub64784', )([Fla39977,Con90722])
Res16783 = keras.layers.Reshape((16, 1), name = 'Res16783', )(Sub64784)
Res60003 = keras.layers.Reshape((16, 1, 1), name = 'Res60003', )(Res16783)
Max11327 = keras.layers.MaxPool2D(pool_size=(8, 1), name = 'Max11327', )(Res60003)
model = tf.keras.models.Model(inputs=[in0Bat32533,in0Glo77058,in0Con90722], outputs=Max11327)
w = model.get_layer('Bat32533').get_weights() 
w[0] = np.array([0.5907, 0.741])
w[1] = np.array([0.63, 0.7791])
w[2] = np.array([0.3636, 0.366])
w[3] = np.array([0.8897, 0.1864])
model.get_layer('Bat32533').set_weights(w) 
in0Bat32533 = tf.constant([[[[1.2367, 1.8533], [1.3071, 1.7921], [1.9915, 1.4495], [1.0322, 1.3272]], [[1.8197, 1.73], [1.0786, 1.5738], [1.2201, 1.8021], [1.2747, 1.8774]]]])
in0Glo77058 = tf.constant([[[[1.6405], [1.5863]], [[1.8876], [1.9426]]]])
in0Con90722 = tf.constant([[0.9296, 0.5563, 0.6337, 0.8645, 0.6636, 0.7452, 0.9302, 0.9394, 0.395, 0.3817, 0.4381, 0.0368, 0.8595, 0.1491, 0.2934]])
print (np.array2string(model.predict([in0Bat32533,in0Glo77058,in0Con90722],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Max11327.png')

LBat32533 = batch_normalization_layer([[[[1.2367, 1.8533], [1.3071, 1.7921], [1.9915, 1.4495], [1.0322, 1.3272]], [[1.8197, 1.73], [1.0786, 1.5738], [1.2201, 1.8021], [1.2747, 1.8774]]]], 3, 0.8910226730903602, [0.5907, 0.741], [0.63, 0.7791], [0.3636, 0.366], [0.8897, 0.1864], Bat32533), 
LLea65645 = leaky_relu_layer(Bat32533, 2.0177936306134017, Lea65645), 
LRes78963 = reshape_layer(Lea65645, [2, 8], Res78963), 
LFla39977 = flatten_layer(Res78963, Fla39977), 
LGlo77058 = global_max_pool2D_layer([[[[1.6405], [1.5863]], [[1.8876], [1.9426]]]], Glo77058), 
LCon90722 = concatenate_layer([Glo77058,[[0.9296, 0.5563, 0.6337, 0.8645, 0.6636, 0.7452, 0.9302, 0.9394, 0.395, 0.3817, 0.4381, 0.0368, 0.8595, 0.1491, 0.2934]]], 1, Con90722), 
LSub64784 = subtract_layer(Fla39977,Con90722, Sub64784), 
LRes16783 = reshape_layer(Sub64784, [16, 1], Res16783), 
LRes60003 = reshape_layer(Res16783, [16, 1, 1], Res60003), 
LMax11327 = max_pool2D_layer(Res60003, 8, 1, Max11327), 
exec_layers([LBat32533,LLea65645,LRes78963,LFla39977,LGlo77058,LCon90722,LSub64784,LRes16783,LRes60003,LMax11327],["Bat32533","Lea65645","Res78963","Fla39977","Glo77058","Con90722","Sub64784","Res16783","Res60003","Max11327"],Max11327,"Max11327")

Actual (Unparsed): [[[[1.1634641]], [[1.5646581]]]]

Expected (Unparsed): [[[[1.1634641220652684]],[[1.5646580773364045]]]]

Actual:   [[[[1.1635]], [[1.5647]]]]

Expected: [[[[1.1635]], [[1.5647]]]]