import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Glo91795 = tf.keras.layers.Input(shape=([1, 2, 1, 1]))
in0Con13199 = tf.keras.layers.Input(shape=([44]))
in0LST59267 = tf.keras.layers.Input(shape=([1, 2]))
in0LST77673 = tf.keras.layers.Input(shape=([2, 2]))
in0Con20614 = tf.keras.layers.Input(shape=([44]))

Glo91795 = keras.layers.GlobalMaxPool3D(name = 'Glo91795', )(in0Glo91795)
Res60386 = keras.layers.Reshape((1, 1), name = 'Res60386', )(Glo91795)
Glo96324 = keras.layers.GlobalAveragePooling1D(name = 'Glo96324', )(Res60386)
Res9947 = keras.layers.Reshape((1, 1), name = 'Res9947', )(Glo96324)
Res6322 = keras.layers.Reshape((1, 1, 1), name = 'Res6322', )(Res9947)
Res90982 = keras.layers.Reshape((1, 1, 1, 1), name = 'Res90982', )(Res6322)
Glo99966 = keras.layers.GlobalAveragePooling3D(name = 'Glo99966', )(Res90982)
Con13199 = keras.layers.Concatenate(axis=1, name = 'Con13199', )([Glo99966,in0Con13199])
LST59267 = keras.layers.LSTM(3,recurrent_activation='sigmoid', name = 'LST59267', )(in0LST59267)
Res10347 = keras.layers.Reshape((3, 1), name = 'Res10347', )(LST59267)
Res99054 = keras.layers.Reshape((3, 1, 1), name = 'Res99054', )(Res10347)
Res54100 = keras.layers.Reshape((3, 1, 1, 1), name = 'Res54100', )(Res99054)
Zer95429 = keras.layers.ZeroPadding3D(padding=((1, 1), (1, 1), (1, 1)), name = 'Zer95429', )(Res54100)
Res80771 = keras.layers.Reshape((5, 3, 3), name = 'Res80771', )(Zer95429)
Res76406 = keras.layers.Reshape((5, 9), name = 'Res76406', )(Res80771)
Fla43355 = keras.layers.Flatten(name = 'Fla43355', )(Res76406)
LST77673 = keras.layers.LSTM(1,recurrent_activation='sigmoid', name = 'LST77673', )(in0LST77673)
Con20614 = keras.layers.Concatenate(axis=1, name = 'Con20614', )([LST77673,in0Con20614])
Max14911 = keras.layers.Maximum(name = 'Max14911', )([Fla43355,Con20614])
Ave60295 = keras.layers.Average(name = 'Ave60295', )([Con13199,Max14911])
model = tf.keras.models.Model(inputs=[in0Glo91795,in0Con13199,in0LST59267,in0LST77673,in0Con20614], outputs=Ave60295)
w = model.get_layer('LST59267').get_weights() 
w[0] = np.array([[9, 10, 6, 10, 2, 1, 10, 6, 5, 1, 10, 9], [3, 10, 4, 4, 8, 2, 5, 4, 6, 10, 5, 7]])
w[1] = np.array([[8, 10, 4, 6, 1, 5, 2, 8, 7, 9, 5, 6], [9, 10, 6, 1, 1, 8, 8, 4, 8, 10, 7, 6], [6, 2, 10, 1, 3, 2, 7, 8, 5, 1, 9, 1]])
w[2] = np.array([3, 1, 2, 2, 7, 3, 7, 3, 7, 2, 5, 5])
model.get_layer('LST59267').set_weights(w) 
w = model.get_layer('LST77673').get_weights() 
w[0] = np.array([[8, 5, 7, 3], [3, 1, 6, 8]])
w[1] = np.array([[8, 1, 10, 9]])
w[2] = np.array([3, 10, 3, 9])
model.get_layer('LST77673').set_weights(w) 
in0Glo91795 = tf.constant([[[[[1.0732]], [[1.3707]]]]])
in0Con13199 = tf.constant([[0.4443, 0.1313, 0.4282, 0.8083, 0.6217, 0.1913, 0.2153, 0.1413, 0.5993, 0.5583, 0.1816, 0.3847, 0.5617, 0.9865, 0.8135, 0.9955, 0.0969, 0.8846, 0.9539, 0.4643, 0.5334, 0.2907, 0.4326, 0.2182, 0.7126, 0.2157, 0.0332, 0.1735, 0.3914, 0.9933, 0.0319, 0.3875, 0.1262, 0.782, 0.4721, 0.6601, 0.9299, 0.9056, 0.5372, 0.8644, 0.2106, 0.2893, 0.8734, 0.2494]])
in0LST59267 = tf.constant([[[7, 7]]])
in0LST77673 = tf.constant([[[4, 4], [6, 9]]])
in0Con20614 = tf.constant([[0.2494, 0.3444, 0.1689, 0.4201, 0.5502, 0.2516, 0.601, 0.181, 0.7984, 0.9337, 0.6934, 0.4566, 0.8494, 0.0803, 0.8427, 0.2895, 0.7909, 0.7306, 0.8159, 0.5352, 0.2331, 0.3035, 0.5124, 0.8824, 0.2569, 0.3867, 0.5523, 0.2384, 0.1513, 0.6126, 0.5744, 0.8574, 0.4594, 0.7475, 0.4017, 0.9051, 0.2628, 0.197, 0.0401, 0.2469, 0.0848, 0.4962, 0.8084, 0.1226]])
print (np.array2string(model.predict([in0Glo91795,in0Con13199,in0LST59267,in0LST77673,in0Con20614],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Ave60295.png')

LGlo91795 = global_max_pool3D_layer([[[[[1.0732]], [[1.3707]]]]], Glo91795), 
LRes60386 = reshape_layer(Glo91795, [1, 1], Res60386), 
LGlo96324 = global_average_pooling1D_layer(Res60386, Glo96324), 
LRes9947 = reshape_layer(Glo96324, [1, 1], Res9947), 
LRes6322 = reshape_layer(Res9947, [1, 1, 1], Res6322), 
LRes90982 = reshape_layer(Res6322, [1, 1, 1, 1], Res90982), 
LGlo99966 = global_average_pooling3D_layer(Res90982, Glo99966), 
LCon13199 = concatenate_layer([Glo99966,[[0.4443, 0.1313, 0.4282, 0.8083, 0.6217, 0.1913, 0.2153, 0.1413, 0.5993, 0.5583, 0.1816, 0.3847, 0.5617, 0.9865, 0.8135, 0.9955, 0.0969, 0.8846, 0.9539, 0.4643, 0.5334, 0.2907, 0.4326, 0.2182, 0.7126, 0.2157, 0.0332, 0.1735, 0.3914, 0.9933, 0.0319, 0.3875, 0.1262, 0.782, 0.4721, 0.6601, 0.9299, 0.9056, 0.5372, 0.8644, 0.2106, 0.2893, 0.8734, 0.2494]]], 1, Con13199), 
LLST59267 = lstm_layer([[[7, 7]]],[[9, 10, 6, 10, 2, 1, 10, 6, 5, 1, 10, 9], [3, 10, 4, 4, 8, 2, 5, 4, 6, 10, 5, 7]],[[8, 10, 4, 6, 1, 5, 2, 8, 7, 9, 5, 6], [9, 10, 6, 1, 1, 8, 8, 4, 8, 10, 7, 6], [6, 2, 10, 1, 3, 2, 7, 8, 5, 1, 9, 1]],[3, 1, 2, 2, 7, 3, 7, 3, 7, 2, 5, 5], LST59267), 
LRes10347 = reshape_layer(LST59267, [3, 1], Res10347), 
LRes99054 = reshape_layer(Res10347, [3, 1, 1], Res99054), 
LRes54100 = reshape_layer(Res99054, [3, 1, 1, 1], Res54100), 
LZer95429 = zero_padding3D_layer(Res54100, 1, 1, 1, 1, 1, 1, Zer95429), 
LRes80771 = reshape_layer(Zer95429, [5, 3, 3], Res80771), 
LRes76406 = reshape_layer(Res80771, [5, 9], Res76406), 
LFla43355 = flatten_layer(Res76406, Fla43355), 
LLST77673 = lstm_layer([[[4, 4], [6, 9]]],[[8, 5, 7, 3], [3, 1, 6, 8]],[[8, 1, 10, 9]],[3, 10, 3, 9], LST77673), 
LCon20614 = concatenate_layer([LST77673,[[0.2494, 0.3444, 0.1689, 0.4201, 0.5502, 0.2516, 0.601, 0.181, 0.7984, 0.9337, 0.6934, 0.4566, 0.8494, 0.0803, 0.8427, 0.2895, 0.7909, 0.7306, 0.8159, 0.5352, 0.2331, 0.3035, 0.5124, 0.8824, 0.2569, 0.3867, 0.5523, 0.2384, 0.1513, 0.6126, 0.5744, 0.8574, 0.4594, 0.7475, 0.4017, 0.9051, 0.2628, 0.197, 0.0401, 0.2469, 0.0848, 0.4962, 0.8084, 0.1226]]], 1, Con20614), 
LMax14911 = maximum_layer([Fla43355,Con20614], Max14911), 
LAve60295 = average_layer([Con13199,Max14911], Ave60295), 
exec_layers([LGlo91795,LRes60386,LGlo96324,LRes9947,LRes6322,LRes90982,LGlo99966,LCon13199,LLST59267,LRes10347,LRes99054,LRes54100,LZer95429,LRes80771,LRes76406,LFla43355,LLST77673,LCon20614,LMax14911,LAve60295],["Glo91795","Res60386","Glo96324","Res9947","Res6322","Res90982","Glo99966","Con13199","LST59267","Res10347","Res99054","Res54100","Zer95429","Res80771","Res76406","Fla43355","LST77673","Con20614","Max14911","Ave60295"],Ave60295,"Ave60295")

Actual (Unparsed): [[1.1673638, 0.3468500, 0.2378500, 0.2985500, 0.6142000, 0.5859500, 0.2214500, 0.4081500, 0.1611500, 0.6988500, 0.7460000, 0.4375000, 0.4206500, 0.7055500, 0.5334000, 0.8281000, 0.6425000, 0.4439000, 0.8076000, 0.8849000, 0.4997500, 0.3832500, 0.5261471, 0.4725000, 0.5503000, 0.4847500, 0.3012000, 0.2927500, 0.2059500, 0.2713500, 0.8029500, 0.3967471, 0.6224500, 0.2928000, 0.7647500, 0.4369000, 0.7826000, 0.5963500, 0.5513000, 0.2886500, 0.5556500, 0.1477000, 0.3927500, 0.8409000, 0.1860000]]

Expected (Unparsed): [[1.1673637900379084,0.34685,0.23785,0.29855,0.6142,0.58595,0.22144999999999998,0.40815,0.16115000000000002,0.69885,0.746,0.4375,0.42064999999999997,0.70555,0.5334,0.8281000000000001,0.6425000000000001,0.4439,0.8076000000000001,0.8849,0.49975,0.38325,0.5261470779778824,0.4725,0.5503,0.48475,0.3012,0.29275,0.20595,0.27135,0.80295,0.39674707797788245,0.6224500000000001,0.2928,0.76475,0.4369,0.7826,0.5963499999999999,0.5513,0.28865,0.55565,0.1477,0.39275,0.8409,0.186]]

Actual:   [[1.1674, 0.3469, 0.2379, 0.2986, 0.6142, 0.586, 0.2215, 0.4082, 0.1612, 0.6989, 0.746, 0.4375, 0.4207, 0.7056, 0.5334, 0.8281, 0.6425, 0.4439, 0.8076, 0.8849, 0.4998, 0.3833, 0.5262, 0.4725, 0.5503, 0.4848, 0.3012, 0.2928, 0.206, 0.2714, 0.803, 0.3968, 0.6225, 0.2928, 0.7648, 0.4369, 0.7826, 0.5964, 0.5513, 0.2887, 0.5557, 0.1477, 0.3928, 0.8409, 0.186]]

Expected: [[1.1674, 0.3469, 0.2379, 0.2986, 0.6142, 0.586, 0.2215, 0.4082, 0.1612, 0.6989, 0.746, 0.4375, 0.4207, 0.7056, 0.5334, 0.8282, 0.6426, 0.4439, 0.8077, 0.8849, 0.4998, 0.3833, 0.5262, 0.4725, 0.5503, 0.4848, 0.3012, 0.2928, 0.206, 0.2714, 0.803, 0.3968, 0.6225, 0.2928, 0.7648, 0.4369, 0.7826, 0.5964, 0.5513, 0.2887, 0.5557, 0.1477, 0.3928, 0.8409, 0.186]]