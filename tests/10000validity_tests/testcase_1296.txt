import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Ave35822 = tf.keras.layers.Input(shape=([1, 1, 2]))
in1Ave35822 = tf.keras.layers.Input(shape=([1, 1, 2]))
in0Con86815 = tf.keras.layers.Input(shape=([4, 10]))
in0Sof81254 = tf.keras.layers.Input(shape=([1, 1, 2]))
in0Con6688 = tf.keras.layers.Input(shape=([1, 2, 2, 2]))
in0Con69787 = tf.keras.layers.Input(shape=([1, 2, 2, 2]))
in0Mul83693 = tf.keras.layers.Input(shape=([2, 2, 2, 2]))
in1Mul83693 = tf.keras.layers.Input(shape=([2, 2, 2, 2]))
in0Con80594 = tf.keras.layers.Input(shape=([4, 4]))

Ave35822 = keras.layers.Average(name = 'Ave35822', )([in0Ave35822,in1Ave35822])
Thr82604 = keras.layers.ThresholdedReLU(theta=1.4381604616993184, name = 'Thr82604', )(Ave35822)
Res87561 = keras.layers.Reshape((1, 2), name = 'Res87561', )(Thr82604)
Zer94759 = keras.layers.ZeroPadding1D(padding=((1, 1)), name = 'Zer94759', )(Res87561)
Zer87620 = keras.layers.ZeroPadding1D(padding=((1, 0)), name = 'Zer87620', )(Zer94759)
Con86815 = keras.layers.Concatenate(axis=2, name = 'Con86815', )([Zer87620,in0Con86815])
Sof81254 = keras.layers.Softmax(axis=1, name = 'Sof81254', input_shape=(1, 1, 2))(in0Sof81254)
Res36034 = keras.layers.Reshape((1, 1, 2, 1), name = 'Res36034', )(Sof81254)
Zer25351 = keras.layers.ZeroPadding3D(padding=((0, 0), (1, 0), (0, 0)), name = 'Zer25351', )(Res36034)
Con6688 = keras.layers.Concatenate(axis=4, name = 'Con6688', )([Zer25351,in0Con6688])
Con69787 = keras.layers.Conv3D(3, (1, 2, 1),strides=(1, 1, 1), padding='same', dilation_rate=(1, 1, 1), name = 'Con69787', )(in0Con69787)
Sub38661 = keras.layers.Subtract(name = 'Sub38661', )([Con6688,Con69787])
Res23814 = keras.layers.Reshape((1, 2, 6), name = 'Res23814', )(Sub38661)
Res1606 = keras.layers.Reshape((1, 12), name = 'Res1606', )(Res23814)
Zer12605 = keras.layers.ZeroPadding1D(padding=((3, 0)), name = 'Zer12605', )(Res1606)
Mul83693 = keras.layers.Multiply(name = 'Mul83693', )([in0Mul83693,in1Mul83693])
Res96277 = keras.layers.Reshape((2, 2, 4), name = 'Res96277', )(Mul83693)
Res68808 = keras.layers.Reshape((2, 8), name = 'Res68808', )(Res96277)
Zer11436 = keras.layers.ZeroPadding1D(padding=((1, 1)), name = 'Zer11436', )(Res68808)
Con80594 = keras.layers.Concatenate(axis=2, name = 'Con80594', )([Zer11436,in0Con80594])
Mul76738 = keras.layers.Multiply(name = 'Mul76738', )([Zer12605,Con80594])
Bat42425 = keras.layers.BatchNormalization(axis=1, epsilon=0.5561515460855693,  name = 'Bat42425', )(Mul76738)
Min87107 = keras.layers.Minimum(name = 'Min87107', )([Con86815,Bat42425])
model = tf.keras.models.Model(inputs=[in0Ave35822,in1Ave35822,in0Con86815,in0Sof81254,in0Con6688,in0Con69787,in0Mul83693,in1Mul83693,in0Con80594], outputs=Min87107)
w = model.get_layer('Con69787').get_weights() 
w[0] = np.array([[[[[0.3586, 0.3643, 0.8306], [0.7466, 0.4002, 0.5236]]], [[[0.7816, 0.1187, 0.1368], [0.8207, 0.0029, 0.7819]]]]])
w[1] = np.array([0, 0, 0])
model.get_layer('Con69787').set_weights(w) 
w = model.get_layer('Bat42425').get_weights() 
w[0] = np.array([0.7299, 0.7397, 0.7302, 0.299])
w[1] = np.array([0.8577, 0.4828, 0.2594, 0.8842])
w[2] = np.array([0.8698, 0.9822, 0.8371, 0.0976])
w[3] = np.array([0.9815, 0.4575, 0.5397, 0.7483])
model.get_layer('Bat42425').set_weights(w) 
in0Ave35822 = tf.constant([[[[0.5352, 0.1043]]]])
in1Ave35822 = tf.constant([[[[0.9158, 0.7685]]]])
in0Con86815 = tf.constant([[[0.4471, 0.5115, 0.0793, 0.6658, 0.7377, 0.2476, 0.7983, 0.1183, 0.5149, 0.2062], [0.3646, 0.6089, 0.4078, 0.7511, 0.3863, 0.389, 0.6353, 0.5762, 0.2211, 0.7866], [0.3066, 0.7561, 0.2216, 0.2706, 0.2493, 0.7759, 0.7729, 0.9857, 0.2382, 0.4591], [0.8226, 0.7265, 0.7483, 0.977, 0.6806, 0.5355, 0.5197, 0.5395, 0.3606, 0.9815]]])
in0Sof81254 = tf.constant([[[[0.5758, 0.9857]]]])
in0Con6688 = tf.constant([[[[[0.0503, 0.7715], [0.0971, 0.3347]], [[0.7778, 0.6637], [0.6963, 0.4902]]]]])
in0Con69787 = tf.constant([[[[[0.1818, 0.7266], [0.2181, 0.588]], [[0.5057, 0.1763], [0.3821, 0.6541]]]]])
in0Mul83693 = tf.constant([[[[[0.642, 0.8551], [0.7263, 0.6025]], [[0.8854, 0.4094], [0.5309, 0.4686]]], [[[0.3446, 0.9009], [0.9809, 0.4134]], [[0.1572, 0.3732], [0.272, 0.5159]]]]])
in1Mul83693 = tf.constant([[[[[0.3655, 0.052], [0.5919, 0.3837]], [[0.4958, 0.4], [0.302, 0.8373]]], [[[0.3664, 0.2959], [0.5826, 0.5577]], [[0.2883, 0.0686], [0.3929, 0.1179]]]]])
in0Con80594 = tf.constant([[[0.2093, 0.217, 0.0379, 0.7031], [0.224, 0.7192, 0.7121, 0.8348], [0.7603, 0.6785, 0.2701, 0.2602], [0.4489, 0.1267, 0.4023, 0.6411]]])
print (np.array2string(model.predict([in0Ave35822,in1Ave35822,in0Con86815,in0Sof81254,in0Con6688,in0Con69787,in0Mul83693,in1Mul83693,in0Con80594],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Min87107.png')

LAve35822 = average_layer([[[[[0.5352, 0.1043]]]], [[[[0.9158, 0.7685]]]]], Ave35822), 
LThr82604 = thresholded_relu_layer(Ave35822, 1.4381604616993184, Thr82604), 
LRes87561 = reshape_layer(Thr82604, [1, 2], Res87561), 
LZer94759 = zero_padding1D_layer(Res87561, 1, 1, Zer94759), 
LZer87620 = zero_padding1D_layer(Zer94759, 1, 0, Zer87620), 
LCon86815 = concatenate_layer([Zer87620,[[[0.4471, 0.5115, 0.0793, 0.6658, 0.7377, 0.2476, 0.7983, 0.1183, 0.5149, 0.2062], [0.3646, 0.6089, 0.4078, 0.7511, 0.3863, 0.389, 0.6353, 0.5762, 0.2211, 0.7866], [0.3066, 0.7561, 0.2216, 0.2706, 0.2493, 0.7759, 0.7729, 0.9857, 0.2382, 0.4591], [0.8226, 0.7265, 0.7483, 0.977, 0.6806, 0.5355, 0.5197, 0.5395, 0.3606, 0.9815]]]], 2, Con86815), 
LSof81254 = softmax_layer([[[[0.5758, 0.9857]]]], 1, Sof81254), 
LRes36034 = reshape_layer(Sof81254, [1, 1, 2, 1], Res36034), 
LZer25351 = zero_padding3D_layer(Res36034, 0, 0, 1, 0, 0, 0, Zer25351), 
LCon6688 = concatenate_layer([Zer25351,[[[[[0.0503, 0.7715], [0.0971, 0.3347]], [[0.7778, 0.6637], [0.6963, 0.4902]]]]]], 4, Con6688), 
LCon69787 = conv3D_layer([[[[[0.1818, 0.7266], [0.2181, 0.588]], [[0.5057, 0.1763], [0.3821, 0.6541]]]]], 1, 2, 1,[[[[[0.3586, 0.3643, 0.8306], [0.7466, 0.4002, 0.5236]]], [[[0.7816, 0.1187, 0.1368], [0.8207, 0.0029, 0.7819]]]]],[0, 0, 0], 1, 1, 1, true, 1, 1, 1, Con69787), 
LSub38661 = subtract_layer(Con6688,Con69787, Sub38661), 
LRes23814 = reshape_layer(Sub38661, [1, 2, 6], Res23814), 
LRes1606 = reshape_layer(Res23814, [1, 12], Res1606), 
LZer12605 = zero_padding1D_layer(Res1606, 3, 0, Zer12605), 
LMul83693 = multiply_layer([[[[[[0.642, 0.8551], [0.7263, 0.6025]], [[0.8854, 0.4094], [0.5309, 0.4686]]], [[[0.3446, 0.9009], [0.9809, 0.4134]], [[0.1572, 0.3732], [0.272, 0.5159]]]]], [[[[[0.3655, 0.052], [0.5919, 0.3837]], [[0.4958, 0.4], [0.302, 0.8373]]], [[[0.3664, 0.2959], [0.5826, 0.5577]], [[0.2883, 0.0686], [0.3929, 0.1179]]]]]], Mul83693), 
LRes96277 = reshape_layer(Mul83693, [2, 2, 4], Res96277), 
LRes68808 = reshape_layer(Res96277, [2, 8], Res68808), 
LZer11436 = zero_padding1D_layer(Res68808, 1, 1, Zer11436), 
LCon80594 = concatenate_layer([Zer11436,[[[0.2093, 0.217, 0.0379, 0.7031], [0.224, 0.7192, 0.7121, 0.8348], [0.7603, 0.6785, 0.2701, 0.2602], [0.4489, 0.1267, 0.4023, 0.6411]]]], 2, Con80594), 
LMul76738 = multiply_layer([Zer12605,Con80594], Mul76738), 
LBat42425 = batch_normalization_layer(Mul76738, 1, 0.5561515460855693, [0.7299, 0.7397, 0.7302, 0.299], [0.8577, 0.4828, 0.2594, 0.8842], [0.8698, 0.9822, 0.8371, 0.0976], [0.9815, 0.4575, 0.5397, 0.7483], Bat42425), 
LMin87107 = minimum_layer([Con86815,Bat42425], Min87107), 
exec_layers([LAve35822,LThr82604,LRes87561,LZer94759,LZer87620,LCon86815,LSof81254,LRes36034,LZer25351,LCon6688,LCon69787,LSub38661,LRes23814,LRes1606,LZer12605,LMul83693,LRes96277,LRes68808,LZer11436,LCon80594,LMul76738,LBat42425,LMin87107],["Ave35822","Thr82604","Res87561","Zer94759","Zer87620","Con86815","Sof81254","Res36034","Zer25351","Con6688","Con69787","Sub38661","Res23814","Res1606","Zer12605","Mul83693","Res96277","Res68808","Zer11436","Con80594","Mul76738","Bat42425","Min87107"],Min87107,"Min87107")

Actual (Unparsed): [[[0.0000000, 0.0000000, 0.3457190, 0.3457190, 0.0793000, 0.3457190, 0.3457190, 0.2476000, 0.3457190, 0.1183000, 0.3457190, 0.2062000], [-0.2388244, -0.2388244, -0.2388244, -0.2388244, -0.2388244, -0.2388244, -0.2388244, -0.2388244, -0.2388244, -0.2388244, -0.2388244, -0.2388244], [-0.3245065, -0.3245065, -0.3245065, -0.3245065, -0.3245065, -0.3245065, -0.3245065, -0.3245065, -0.3245065, -0.3245065, -0.3245065, -0.3245065], [0.0000000, 0.0000000, 0.8226000, 0.7265000, 0.7483000, 0.8586491, 0.6806000, 0.5355000, 0.5197000, 0.5395000, 0.3606000, 0.8301743]]]

Expected (Unparsed): [[[0,0,0.3457190494253102,0.3457190494253102,0.0793,0.3457190494253102,0.3457190494253102,0.2476,0.3457190494253102,0.1183,0.3457190494253102,0.2062],[-0.23882439256334398,-0.23882439256334398,-0.23882439256334398,-0.23882439256334398,-0.23882439256334398,-0.23882439256334398,-0.23882439256334398,-0.23882439256334398,-0.23882439256334398,-0.23882439256334398,-0.23882439256334398,-0.23882439256334398],[-0.324506497830711,-0.324506497830711,-0.324506497830711,-0.324506497830711,-0.324506497830711,-0.324506497830711,-0.324506497830711,-0.324506497830711,-0.324506497830711,-0.324506497830711,-0.324506497830711,-0.324506497830711],[0,0,0.8226,0.7265,0.7483,0.8586490512965971,0.6806,0.5355,0.5197,0.5395,0.3606,0.8301742997520668]]]

Actual:   [[[0, 0, 0.3458, 0.3458, 0.0793, 0.3458, 0.3458, 0.2476, 0.3458, 0.1183, 0.3458, 0.2062], [-0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388], [-0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245], [0, 0, 0.8226, 0.7265, 0.7483, 0.8587, 0.6806, 0.5355, 0.5197, 0.5395, 0.3606, 0.8302]]]

Expected: [[[0, 0, 0.3458, 0.3458, 0.0793, 0.3458, 0.3458, 0.2476, 0.3458, 0.1183, 0.3458, 0.2062], [-0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388, -0.2388], [-0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245, -0.3245], [0, 0, 0.8226, 0.7265, 0.7483, 0.8587, 0.6806, 0.5355, 0.5197, 0.5395, 0.3606, 0.8302]]]