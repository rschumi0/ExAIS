import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Add41775 = tf.keras.layers.Input(shape=([2, 1]))
in1Add41775 = tf.keras.layers.Input(shape=([2, 1]))
in0Con82181 = tf.keras.layers.Input(shape=([3, 2]))
in0Dot22138 = tf.keras.layers.Input(shape=([3, 2]))
in1Dot22138 = tf.keras.layers.Input(shape=([3, 2]))

Add41775 = keras.layers.Add(name = 'Add41775', )([in0Add41775,in1Add41775])
Zer16810 = keras.layers.ZeroPadding1D(padding=((1, 0)), name = 'Zer16810', )(Add41775)
Con82181 = keras.layers.Concatenate(axis=2, name = 'Con82181', )([Zer16810,in0Con82181])
Dot22138 = keras.layers.Dot(axes=(2, 2), name = 'Dot22138', )([in0Dot22138,in1Dot22138])
ELU29559 = keras.layers.ELU(alpha=9.26065637231152, name = 'ELU29559', )(Dot22138)
Min92872 = keras.layers.Minimum(name = 'Min92872', )([Con82181,ELU29559])
Bat29184 = keras.layers.BatchNormalization(axis=1, epsilon=0.2259080704974704,  name = 'Bat29184', )(Min92872)
Res53790 = keras.layers.Reshape((3, 3, 1), name = 'Res53790', )(Bat29184)
Glo62352 = keras.layers.GlobalAveragePooling2D(name = 'Glo62352', )(Res53790)
model = tf.keras.models.Model(inputs=[in0Add41775,in1Add41775,in0Con82181,in0Dot22138,in1Dot22138], outputs=Glo62352)
w = model.get_layer('Bat29184').get_weights() 
w[0] = np.array([0.9441, 0.8094, 0.4582])
w[1] = np.array([0.3791, 0.2025, 0.3637])
w[2] = np.array([0.014, 0.9659, 0.0445])
w[3] = np.array([0.585, 0.4061, 0.4011])
model.get_layer('Bat29184').set_weights(w) 
in0Add41775 = tf.constant([[[0.4884], [0.3361]]])
in1Add41775 = tf.constant([[[0.2038], [0.5192]]])
in0Con82181 = tf.constant([[[0.307, 0.2193], [0.0947, 0.1268], [0.6761, 0.133]]])
in0Dot22138 = tf.constant([[[0.5167, 0.5075], [0.1759, 0.4424], [0.6304, 0.0078]]])
in1Dot22138 = tf.constant([[[0.7882, 0.2927], [0.9531, 0.1988], [0.766, 0.1153]]])
print (np.array2string(model.predict([in0Add41775,in1Add41775,in0Con82181,in0Dot22138,in1Dot22138],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Glo62352.png')

LAdd41775 = add_layer([[[[0.4884], [0.3361]]], [[[0.2038], [0.5192]]]], Add41775), 
LZer16810 = zero_padding1D_layer(Add41775, 1, 0, Zer16810), 
LCon82181 = concatenate_layer([Zer16810,[[[0.307, 0.2193], [0.0947, 0.1268], [0.6761, 0.133]]]], 2, Con82181), 
LDot22138 = dot_layer([[[0.5167, 0.5075], [0.1759, 0.4424], [0.6304, 0.0078]]], [[[0.7882, 0.2927], [0.9531, 0.1988], [0.766, 0.1153]]], 2, 2, Dot22138), 
LELU29559 = elu_layer(Dot22138, 9.26065637231152, ELU29559), 
LMin92872 = minimum_layer([Con82181,ELU29559], Min92872), 
LBat29184 = batch_normalization_layer(Min92872, 1, 0.2259080704974704, [0.9441, 0.8094, 0.4582], [0.3791, 0.2025, 0.3637], [0.014, 0.9659, 0.0445], [0.585, 0.4061, 0.4011], Bat29184), 
LRes53790 = reshape_layer(Bat29184, [3, 3, 1], Res53790), 
LGlo62352 = global_average_pooling2D_layer(Res53790, Glo62352), 
exec_layers([LAdd41775,LZer16810,LCon82181,LDot22138,LELU29559,LMin92872,LBat29184,LRes53790,LGlo62352],["Add41775","Zer16810","Con82181","Dot22138","ELU29559","Min92872","Bat29184","Res53790","Glo62352"],Glo62352,"Glo62352")

Actual (Unparsed): [[0.1698951]]

Expected (Unparsed): [[0.16989514431058444]]

Actual:   [[0.1699]]

Expected: [[0.1699]]