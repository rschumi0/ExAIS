import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Lay49413 = tf.keras.layers.Input(shape=([2, 1]))
in0Max50341 = tf.keras.layers.Input(shape=([1, 2]))
in1Max50341 = tf.keras.layers.Input(shape=([1, 2]))
in0Add96382 = tf.keras.layers.Input(shape=([2, 1, 2]))
in1Add96382 = tf.keras.layers.Input(shape=([2, 1, 2]))

Lay49413 = keras.layers.LayerNormalization(axis=1, epsilon=2.860990319573481, name = 'Lay49413', )(in0Lay49413)
Fla38717 = keras.layers.Flatten(name = 'Fla38717', )(Lay49413)
Max50341 = keras.layers.Maximum(name = 'Max50341', )([in0Max50341,in1Max50341])
Glo7390 = keras.layers.GlobalAveragePooling1D(name = 'Glo7390', )(Max50341)
Add88402 = keras.layers.Add(name = 'Add88402', )([Fla38717,Glo7390])
Res47296 = keras.layers.Reshape((2, 1), name = 'Res47296', )(Add88402)
Add96382 = keras.layers.Add(name = 'Add96382', )([in0Add96382,in1Add96382])
ReL63065 = keras.layers.ReLU(max_value=0.92181830128587, negative_slope=0.23195373805931713, threshold=2.946612416775073, name = 'ReL63065', )(Add96382)
Res94333 = keras.layers.Reshape((2, 2), name = 'Res94333', )(ReL63065)
Loc7780 = keras.layers.LocallyConnected1D(4, (1),strides=(1), name = 'Loc7780', )(Res94333)
Con53203 = keras.layers.Conv1D(4, (1),strides=(1), padding='valid', dilation_rate=(1), name = 'Con53203', )(Loc7780)
Dot13390 = keras.layers.Dot(axes=(1, 1), name = 'Dot13390', )([Res47296,Con53203])
model = tf.keras.models.Model(inputs=[in0Lay49413,in0Max50341,in1Max50341,in0Add96382,in1Add96382], outputs=Dot13390)
w = model.get_layer('Loc7780').get_weights() 
w[0] = np.array([[[0.1865, 0.4356, 0.1191, 0.9775], [0.2206, 0.544, 0.4315, 0.4975]], [[0.9132, 0.7184, 0.6768, 0.9973], [0.5746, 0.5595, 0.0524, 0.1044]]])
w[1] = np.array([[0, 0, 0, 0], [0, 0, 0, 0]])
model.get_layer('Loc7780').set_weights(w) 
w = model.get_layer('Con53203').get_weights() 
w[0] = np.array([[[0.2192, 0.7243, 0.4746, 0.9459], [0.6814, 0.0784, 0.503, 0.9304], [0.3689, 0.0934, 0.9307, 0.1136], [0.8774, 0.5928, 0.9526, 0.9965]]])
w[1] = np.array([0, 0, 0, 0])
model.get_layer('Con53203').set_weights(w) 
in0Lay49413 = tf.constant([[[1.0103], [1.1831]]])
in0Max50341 = tf.constant([[[0.6477, 0.9747]]])
in1Max50341 = tf.constant([[[0.9602, 0.8432]]])
in0Add96382 = tf.constant([[[[0.8498, 0.9524]], [[0.9015, 0.8419]]]])
in1Add96382 = tf.constant([[[[0.4345, 0.3112]], [[0.2812, 0.4997]]]])
print (np.array2string(model.predict([in0Lay49413,in0Max50341,in1Max50341,in0Add96382,in1Add96382],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Dot13390.png')

LLay49413 = layer_normalization_layer([[[1.0103], [1.1831]]], 1, 2.860990319573481, Lay49413), 
LFla38717 = flatten_layer(Lay49413, Fla38717), 
LMax50341 = maximum_layer([[[[0.6477, 0.9747]]], [[[0.9602, 0.8432]]]], Max50341), 
LGlo7390 = global_average_pooling1D_layer(Max50341, Glo7390), 
LAdd88402 = add_layer([Fla38717,Glo7390], Add88402), 
LRes47296 = reshape_layer(Add88402, [2, 1], Res47296), 
LAdd96382 = add_layer([[[[[0.8498, 0.9524]], [[0.9015, 0.8419]]]], [[[[0.4345, 0.3112]], [[0.2812, 0.4997]]]]], Add96382), 
LReL63065 = relu_layer(Add96382, 0.92181830128587, 0.23195373805931713, 2.946612416775073, ReL63065), 
LRes94333 = reshape_layer(ReL63065, [2, 2], Res94333), 
LLoc7780 = locally_connected1D_layer(Res94333, 1,[[[0.1865, 0.4356, 0.1191, 0.9775], [0.2206, 0.544, 0.4315, 0.4975]], [[0.9132, 0.7184, 0.6768, 0.9973], [0.5746, 0.5595, 0.0524, 0.1044]]],[[0, 0, 0, 0], [0, 0, 0, 0]], 1, Loc7780), 
LCon53203 = conv1D_layer(Loc7780, 1,[[[0.2192, 0.7243, 0.4746, 0.9459], [0.6814, 0.0784, 0.503, 0.9304], [0.3689, 0.0934, 0.9307, 0.1136], [0.8774, 0.5928, 0.9526, 0.9965]]],[0, 0, 0, 0], 1, false, 1, Con53203), 
LDot13390 = dot_layer(Res47296,Con53203, 1, 1, Dot13390), 
exec_layers([LLay49413,LFla38717,LMax50341,LGlo7390,LAdd88402,LRes47296,LAdd96382,LReL63065,LRes94333,LLoc7780,LCon53203,LDot13390],["Lay49413","Fla38717","Max50341","Glo7390","Add88402","Res47296","Add96382","ReL63065","Res94333","Loc7780","Con53203","Dot13390"],Dot13390,"Dot13390")

Actual (Unparsed): [[[-1.7920857, -1.2342212, -2.1829365, -2.5378762]]]

Expected (Unparsed): [[[-1.7920856833990628,-1.2342212377860085,-2.1829365127997358,-2.5378762053306048]]]

Actual:   [[[-1.792, -1.2342, -2.1829, -2.5378]]]

Expected: [[[-1.792, -1.2342, -2.1829, -2.5378]]]