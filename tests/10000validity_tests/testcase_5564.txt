import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0ELU93390 = tf.keras.layers.Input(shape=([1, 2]))
in0Per1875 = tf.keras.layers.Input(shape=([4, 1]))
in0Con74343 = tf.keras.layers.Input(shape=([4, 1]))
in0Con22904 = tf.keras.layers.Input(shape=([4, 1]))
in0Sub65939 = tf.keras.layers.Input(shape=([2, 2]))
in1Sub65939 = tf.keras.layers.Input(shape=([2, 2]))
in0Con33243 = tf.keras.layers.Input(shape=([4, 1]))
in0Bat22040 = tf.keras.layers.Input(shape=([2, 3]))
in0Per89650 = tf.keras.layers.Input(shape=([3, 4]))

ELU93390 = keras.layers.ELU(alpha=-4.594266177782105, name = 'ELU93390', input_shape=(1, 2))(in0ELU93390)
Zer56980 = keras.layers.ZeroPadding1D(padding=((3, 0)), name = 'Zer56980', )(ELU93390)
Per1875 = keras.layers.Permute((1,2), name = 'Per1875',)(in0Per1875)
Mas92106 = keras.layers.Masking(mask_value=1, name = 'Mas92106', )(Per1875)
Con74343 = keras.layers.Concatenate(axis=2, name = 'Con74343', )([Mas92106,in0Con74343])
Min46142 = keras.layers.Minimum(name = 'Min46142', )([Zer56980,Con74343])
PRe94255 = keras.layers.PReLU(name = 'PRe94255', )(Min46142)
Con22904 = keras.layers.Concatenate(axis=2, name = 'Con22904', )([PRe94255,in0Con22904])
Sub65939 = keras.layers.Subtract(name = 'Sub65939', )([in0Sub65939,in1Sub65939])
Zer4288 = keras.layers.ZeroPadding1D(padding=((2, 0)), name = 'Zer4288', )(Sub65939)
Con33243 = keras.layers.Concatenate(axis=2, name = 'Con33243', )([Zer4288,in0Con33243])
Bat22040 = keras.layers.BatchNormalization(axis=1, epsilon=0.7141171715797808,  name = 'Bat22040', )(in0Bat22040)
Zer49378 = keras.layers.ZeroPadding1D(padding=((2, 0)), name = 'Zer49378', )(Bat22040)
Per89650 = keras.layers.Permute((2,1), name = 'Per89650',)(in0Per89650)
Ave5099 = keras.layers.Average(name = 'Ave5099', )([Zer49378,Per89650])
Sub54068 = keras.layers.Subtract(name = 'Sub54068', )([Con33243,Ave5099])
Ave61735 = keras.layers.Average(name = 'Ave61735', )([Con22904,Sub54068])
model = tf.keras.models.Model(inputs=[in0ELU93390,in0Per1875,in0Con74343,in0Con22904,in0Sub65939,in1Sub65939,in0Con33243,in0Bat22040,in0Per89650], outputs=Ave61735)
w = model.get_layer('PRe94255').get_weights() 
w[0] = np.array([[0.8256, 0.1974], [0.5681, 0.9559], [0.6231, 0.876], [0.3793, 0.7204]])
model.get_layer('PRe94255').set_weights(w) 
w = model.get_layer('Bat22040').get_weights() 
w[0] = np.array([0.7202, 0.0074])
w[1] = np.array([0.4193, 0.5056])
w[2] = np.array([0.7541, 0.4065])
w[3] = np.array([0.4388, 0.8384])
model.get_layer('Bat22040').set_weights(w) 
in0ELU93390 = tf.constant([[[0.453, 0.6799]]])
in0Per1875 = tf.constant([[[1.2818], [1.8588], [1.7399], [1.7476]]])
in0Con74343 = tf.constant([[[0.6037], [0.5492], [0.2072], [0.692]]])
in0Con22904 = tf.constant([[[0.0031], [0.5675], [0.6398], [0.7424]]])
in0Sub65939 = tf.constant([[[0.2679, 0.1269], [0.3103, 0.7044]]])
in1Sub65939 = tf.constant([[[0.939, 0.2047], [0.6204, 0.7992]]])
in0Con33243 = tf.constant([[[0.9734], [0.9382], [0.8583], [0.8422]]])
in0Bat22040 = tf.constant([[[1.0006, 1.4124, 1.2146], [1.6707, 1.1307, 1.7118]]])
in0Per89650 = tf.constant([[[1.0422, 1.7327, 1.9794, 1.6017], [1.1172, 1.474, 1.7266, 1.6183], [1.9822, 1.3817, 1.3976, 1.763]]])
print (np.array2string(model.predict([in0ELU93390,in0Per1875,in0Con74343,in0Con22904,in0Sub65939,in1Sub65939,in0Con33243,in0Bat22040,in0Per89650],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Ave61735.png')

LELU93390 = elu_layer([[[0.453, 0.6799]]], -4.594266177782105, ELU93390), 
LZer56980 = zero_padding1D_layer(ELU93390, 3, 0, Zer56980), 
LPer1875 = permute_layer([[[1.2818], [1.8588], [1.7399], [1.7476]]], 1,2, Per1875), 
LMas92106 = masking_layer(Per1875, 1, Mas92106), 
LCon74343 = concatenate_layer([Mas92106,[[[0.6037], [0.5492], [0.2072], [0.692]]]], 2, Con74343), 
LMin46142 = minimum_layer([Zer56980,Con74343], Min46142), 
LPRe94255 = prelu_layer(Min46142, [[0.8256, 0.1974], [0.5681, 0.9559], [0.6231, 0.876], [0.3793, 0.7204]], PRe94255), 
LCon22904 = concatenate_layer([PRe94255,[[[0.0031], [0.5675], [0.6398], [0.7424]]]], 2, Con22904), 
LSub65939 = subtract_layer([[[0.2679, 0.1269], [0.3103, 0.7044]]], [[[0.939, 0.2047], [0.6204, 0.7992]]], Sub65939), 
LZer4288 = zero_padding1D_layer(Sub65939, 2, 0, Zer4288), 
LCon33243 = concatenate_layer([Zer4288,[[[0.9734], [0.9382], [0.8583], [0.8422]]]], 2, Con33243), 
LBat22040 = batch_normalization_layer([[[1.0006, 1.4124, 1.2146], [1.6707, 1.1307, 1.7118]]], 1, 0.7141171715797808, [0.7202, 0.0074], [0.4193, 0.5056], [0.7541, 0.4065], [0.4388, 0.8384], Bat22040), 
LZer49378 = zero_padding1D_layer(Bat22040, 2, 0, Zer49378), 
LPer89650 = permute_layer([[[1.0422, 1.7327, 1.9794, 1.6017], [1.1172, 1.474, 1.7266, 1.6183], [1.9822, 1.3817, 1.3976, 1.763]]], 2,1, Per89650), 
LAve5099 = average_layer([Zer49378,Per89650], Ave5099), 
LSub54068 = subtract_layer(Con33243,Ave5099, Sub54068), 
LAve61735 = average_layer([Con22904,Sub54068], Ave61735), 
exec_layers([LELU93390,LZer56980,LPer1875,LMas92106,LCon74343,LMin46142,LPRe94255,LCon22904,LSub65939,LZer4288,LCon33243,LBat22040,LZer49378,LPer89650,LAve5099,LSub54068,LAve61735],["ELU93390","Zer56980","Per1875","Mas92106","Con74343","Min46142","PRe94255","Con22904","Sub65939","Zer4288","Con33243","Bat22040","Zer49378","Per89650","Ave5099","Sub54068","Ave61735"],Ave61735,"Ave61735")

Actual (Unparsed): [[[-0.2605500, -0.2793000, -0.0073000], [-0.4331750, -0.3685000, 0.4074250], [-0.9765594, -0.6857620, 0.2176061], [-0.4572520, -0.2395002, 0.2232119]]]

Expected (Unparsed): [[[-0.26055,-0.2793,-0.007299999999999969],[-0.433175,-0.3685,0.40742500000000004],[-0.9765593388599558,-0.6857619990730583,0.21760608298170536],[-0.45725202176367896,-0.23950025641611805,0.22321195498486773]]]

Actual:   [[[-0.2605, -0.2793, -0.0073], [-0.4331, -0.3685, 0.4075], [-0.9765, -0.6857, 0.2177], [-0.4572, -0.2395, 0.2233]]]

Expected: [[[-0.2605, -0.2793, -0.0072], [-0.4331, -0.3685, 0.4075], [-0.9765, -0.6857, 0.2177], [-0.4572, -0.2395, 0.2233]]]