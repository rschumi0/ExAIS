import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Add64100 = tf.keras.layers.Input(shape=([1, 1, 1, 2]))
in1Add64100 = tf.keras.layers.Input(shape=([1, 1, 1, 2]))

Add64100 = keras.layers.Add(name = 'Add64100', )([in0Add64100,in1Add64100])
Fla52466 = keras.layers.Flatten(name = 'Fla52466', )(Add64100)
Bat23887 = keras.layers.BatchNormalization(axis=1, epsilon=0.7544585773057696,  name = 'Bat23887', )(Fla52466)
ELU5473 = keras.layers.ELU(alpha=-9.029373587507735, name = 'ELU5473', )(Bat23887)
Res83449 = keras.layers.Reshape((2, 1), name = 'Res83449', )(ELU5473)
Sim80061 = keras.layers.SimpleRNN(2,name = 'Sim80061', )(Res83449)
model = tf.keras.models.Model(inputs=[in0Add64100,in1Add64100], outputs=Sim80061)
w = model.get_layer('Bat23887').get_weights() 
w[0] = np.array([0.1189, 0.3712])
w[1] = np.array([0.1244, 0.112])
w[2] = np.array([0.3663, 0.2863])
w[3] = np.array([0.3783, 0.5308])
model.get_layer('Bat23887').set_weights(w) 
w = model.get_layer('Sim80061').get_weights() 
w[0] = np.array([[3, 4]])
w[1] = np.array([[1, 6], [9, 2]])
w[2] = np.array([1, 5])
model.get_layer('Sim80061').set_weights(w) 
in0Add64100 = tf.constant([[[[[0.1347, 0.6722]]]]])
in1Add64100 = tf.constant([[[[[0.6794, 0.6994]]]]])
print (np.array2string(model.predict([in0Add64100,in1Add64100],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Sim80061.png')

LAdd64100 = add_layer([[[[[[0.1347, 0.6722]]]]], [[[[[0.6794, 0.6994]]]]]], Add64100), 
LFla52466 = flatten_layer(Add64100, Fla52466), 
LBat23887 = batch_normalization_layer(Fla52466, 1, 0.7544585773057696, [0.1189, 0.3712], [0.1244, 0.112], [0.3663, 0.2863], [0.3783, 0.5308], Bat23887), 
LELU5473 = elu_layer(Bat23887, -9.029373587507735, ELU5473), 
LRes83449 = reshape_layer(ELU5473, [2, 1], Res83449), 
LSim80061 = simple_rnn_layer(Res83449,[[3, 4]],[[1, 6], [9, 2]],[1, 5], Sim80061), 
exec_layers([LAdd64100,LFla52466,LBat23887,LELU5473,LRes83449,LSim80061],["Add64100","Fla52466","Bat23887","ELU5473","Res83449","Sim80061"],Sim80061,"Sim80061")

Actual (Unparsed): [[1.0000000, 1.0000000]]

Expected (Unparsed): [[0.9999999999594754,0.999999999999278]]

Actual:   [[1, 1]]

Expected: [[1, 1]]