import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Ave44715 = tf.keras.layers.Input(shape=([2, 2, 2]))
in1Ave44715 = tf.keras.layers.Input(shape=([2, 2, 2]))
in0Con98691 = tf.keras.layers.Input(shape=([3, 6, 1]))
in0Bat25601 = tf.keras.layers.Input(shape=([1]))
in0Con32844 = tf.keras.layers.Input(shape=([3, 6, 2]))
in0Zer75096 = tf.keras.layers.Input(shape=([1, 4, 3]))
in0Min49365 = tf.keras.layers.Input(shape=([1, 2, 2]))
in1Min49365 = tf.keras.layers.Input(shape=([1, 2, 2]))
in0Con41084 = tf.keras.layers.Input(shape=([3, 6, 1]))

Ave44715 = keras.layers.Average(name = 'Ave44715', )([in0Ave44715,in1Ave44715])
Zer37138 = keras.layers.ZeroPadding2D(padding=((1, 0), (4, 0)), name = 'Zer37138', )(Ave44715)
Con98691 = keras.layers.Concatenate(axis=3, name = 'Con98691', )([Zer37138,in0Con98691])
Bat25601 = keras.layers.BatchNormalization(axis=1, epsilon=0.5667467580665254,  name = 'Bat25601', )(in0Bat25601)
Res74785 = keras.layers.Reshape((1, 1), name = 'Res74785', )(Bat25601)
Res93289 = keras.layers.Reshape((1, 1, 1), name = 'Res93289', )(Res74785)
Zer28779 = keras.layers.ZeroPadding2D(padding=((2, 0), (5, 0)), name = 'Zer28779', )(Res93289)
Con32844 = keras.layers.Concatenate(axis=3, name = 'Con32844', )([Zer28779,in0Con32844])
Zer75096 = keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name = 'Zer75096', )(in0Zer75096)
Ave91234 = keras.layers.Average(name = 'Ave91234', )([Con32844,Zer75096])
Min49365 = keras.layers.Minimum(name = 'Min49365', )([in0Min49365,in1Min49365])
Zer51004 = keras.layers.ZeroPadding2D(padding=((2, 0), (4, 0)), name = 'Zer51004', )(Min49365)
Con41084 = keras.layers.Concatenate(axis=3, name = 'Con41084', )([Zer51004,in0Con41084])
Add97756 = keras.layers.Add(name = 'Add97756', )([Ave91234,Con41084])
Max23454 = keras.layers.Maximum(name = 'Max23454', )([Con98691,Add97756])
model = tf.keras.models.Model(inputs=[in0Ave44715,in1Ave44715,in0Con98691,in0Bat25601,in0Con32844,in0Zer75096,in0Min49365,in1Min49365,in0Con41084], outputs=Max23454)
w = model.get_layer('Bat25601').get_weights() 
w[0] = np.array([0.9044])
w[1] = np.array([0.9337])
w[2] = np.array([0.3575])
w[3] = np.array([0.3175])
model.get_layer('Bat25601').set_weights(w) 
in0Ave44715 = tf.constant([[[[0.2442, 0.4275], [0.6763, 0.5429]], [[0.1706, 0.5411], [0.5029, 0.1706]]]])
in1Ave44715 = tf.constant([[[[0.3611, 0.016], [0.3284, 0.4835]], [[0.2776, 0.4855], [0.1926, 0.5389]]]])
in0Con98691 = tf.constant([[[[0.4607], [0.633], [0.3412], [0.2994], [0.5917], [0.9021]], [[0.2694], [0.4934], [0.6701], [0.462], [0.9311], [0.0354]], [[0.4784], [0.8572], [0.8927], [0.224], [0.7616], [0.5867]]]])
in0Bat25601 = tf.constant([[1.5433]])
in0Con32844 = tf.constant([[[[0.6005, 0.2581], [0.0837, 0.4502], [0.9569, 0.8733], [0.9173, 0.6772], [0.2769, 0.1845], [0.1127, 0.8717]], [[0.3548, 0.1409], [0.9529, 0.3952], [0.4953, 0.6921], [0.4544, 0.6849], [0.3192, 0.1689], [0.3051, 0.2542]], [[0.7313, 0.6242], [0.9859, 0.1134], [0.792, 0.7244], [0.1132, 0.4668], [0.9308, 0.9795], [0.564, 0.6978]]]])
in0Zer75096 = tf.constant([[[[1.7757, 1.3366, 1.0436], [1.9676, 1.2937, 1.3365], [1.1575, 1.2763, 1.4683], [1.7813, 1.3677, 1.5001]]]])
in0Min49365 = tf.constant([[[[0.0535, 0.3804], [0.5733, 0.6649]]]])
in1Min49365 = tf.constant([[[[0.894, 0.8449], [0.6993, 0.8933]]]])
in0Con41084 = tf.constant([[[[0.2602], [0.8771], [0.2436], [0.6766], [0.6309], [0.2444]], [[0.0831], [0.2924], [0.7693], [0.4745], [0.602], [0.6328]], [[0.0875], [0.6581], [0.9556], [0.6062], [0.8906], [0.3741]]]])
print (np.array2string(model.predict([in0Ave44715,in1Ave44715,in0Con98691,in0Bat25601,in0Con32844,in0Zer75096,in0Min49365,in1Min49365,in0Con41084],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Max23454.png')

LAve44715 = average_layer([[[[[0.2442, 0.4275], [0.6763, 0.5429]], [[0.1706, 0.5411], [0.5029, 0.1706]]]], [[[[0.3611, 0.016], [0.3284, 0.4835]], [[0.2776, 0.4855], [0.1926, 0.5389]]]]], Ave44715), 
LZer37138 = zero_padding2D_layer(Ave44715, 1, 0, 4, 0, Zer37138), 
LCon98691 = concatenate_layer([Zer37138,[[[[0.4607], [0.633], [0.3412], [0.2994], [0.5917], [0.9021]], [[0.2694], [0.4934], [0.6701], [0.462], [0.9311], [0.0354]], [[0.4784], [0.8572], [0.8927], [0.224], [0.7616], [0.5867]]]]], 3, Con98691), 
LBat25601 = batch_normalization_layer([[1.5433]], 1, 0.5667467580665254, [0.9044], [0.9337], [0.3575], [0.3175], Bat25601), 
LRes74785 = reshape_layer(Bat25601, [1, 1], Res74785), 
LRes93289 = reshape_layer(Res74785, [1, 1, 1], Res93289), 
LZer28779 = zero_padding2D_layer(Res93289, 2, 0, 5, 0, Zer28779), 
LCon32844 = concatenate_layer([Zer28779,[[[[0.6005, 0.2581], [0.0837, 0.4502], [0.9569, 0.8733], [0.9173, 0.6772], [0.2769, 0.1845], [0.1127, 0.8717]], [[0.3548, 0.1409], [0.9529, 0.3952], [0.4953, 0.6921], [0.4544, 0.6849], [0.3192, 0.1689], [0.3051, 0.2542]], [[0.7313, 0.6242], [0.9859, 0.1134], [0.792, 0.7244], [0.1132, 0.4668], [0.9308, 0.9795], [0.564, 0.6978]]]]], 3, Con32844), 
LZer75096 = zero_padding2D_layer([[[[1.7757, 1.3366, 1.0436], [1.9676, 1.2937, 1.3365], [1.1575, 1.2763, 1.4683], [1.7813, 1.3677, 1.5001]]]], 1, 1, 1, 1, Zer75096), 
LAve91234 = average_layer([Con32844,Zer75096], Ave91234), 
LMin49365 = minimum_layer([[[[[0.0535, 0.3804], [0.5733, 0.6649]]]], [[[[0.894, 0.8449], [0.6993, 0.8933]]]]], Min49365), 
LZer51004 = zero_padding2D_layer(Min49365, 2, 0, 4, 0, Zer51004), 
LCon41084 = concatenate_layer([Zer51004,[[[[0.2602], [0.8771], [0.2436], [0.6766], [0.6309], [0.2444]], [[0.0831], [0.2924], [0.7693], [0.4745], [0.602], [0.6328]], [[0.0875], [0.6581], [0.9556], [0.6062], [0.8906], [0.3741]]]]], 3, Con41084), 
LAdd97756 = add_layer([Ave91234,Con41084], Add97756), 
LMax23454 = maximum_layer([Con98691,Add97756], Max23454), 
exec_layers([LAve44715,LZer37138,LCon98691,LBat25601,LRes74785,LRes93289,LZer28779,LCon32844,LZer75096,LAve91234,LMin49365,LZer51004,LCon41084,LAdd97756,LMax23454],["Ave44715","Zer37138","Con98691","Bat25601","Res74785","Res93289","Zer28779","Con32844","Zer75096","Ave91234","Min49365","Zer51004","Con41084","Add97756","Max23454"],Max23454,"Max23454")

Actual (Unparsed): [[[[0.0000000, 0.3002500, 0.4607000], [0.0000000, 0.0418500, 1.1022000], [0.0000000, 0.4784500, 0.6802500], [0.0000000, 0.4586500, 1.0152000], [0.0000000, 0.1384500, 0.7231500], [0.0000000, 0.0563500, 0.9021000]], [[0.0000000, 0.1774000, 0.2694000], [0.8878500, 1.1447500, 1.0118000], [0.9838000, 0.8945000, 1.7836000], [0.5787500, 0.8653500, 1.5511000], [0.8906500, 0.8434500, 1.4365000], [0.5023500, 0.5132000, 0.7599000]], [[0.0000000, 0.3656500, 0.4784000], [0.0000000, 0.4929500, 0.8572000], [0.0000000, 0.3960000, 1.3178000], [0.0000000, 0.0566000, 0.8396000], [0.2241000, 0.8458000, 1.3803500], [1.6103869, 0.9469000, 0.7230000]]]]

Expected (Unparsed): [[[[0,0.30025,0.4607],[0,0.04185,1.1022],[0,0.47845,0.68025],[0,0.45865,1.0152],[0,0.13845,0.72315],[0,0.05635,0.9021]],[[0,0.1774,0.2694],[0.88785,1.14475,1.0118],[0.9838,0.8945000000000001,1.7835999999999999],[0.57875,0.8653500000000001,1.5511],[0.89065,0.8434499999999999,1.4365],[0.5023500000000001,0.5132,0.7599]],[[0,0.36565,0.4784],[0,0.49295,0.8572],[0,0.396,1.3178],[0,0.0566,0.8395999999999999],[0.22410000000000002,0.8458,1.38035],[1.610386832838012,0.9469000000000001,0.723]]]]

Actual:   [[[[0, 0.3003, 0.4607], [0, 0.0419, 1.1022], [0, 0.4785, 0.6803], [0, 0.4587, 1.0152], [0, 0.1385, 0.7232], [0, 0.0564, 0.9021]], [[0, 0.1774, 0.2694], [0.8879, 1.1448, 1.0118], [0.9838, 0.8945, 1.7836], [0.5788, 0.8654, 1.5511], [0.8907, 0.8435, 1.4365], [0.5024, 0.5132, 0.7599]], [[0, 0.3657, 0.4784], [0, 0.493, 0.8572], [0, 0.396, 1.3178], [0, 0.0566, 0.8396], [0.2241, 0.8458, 1.3804], [1.6104, 0.9469, 0.723]]]]

Expected: [[[[0, 0.3003, 0.4607], [0, 0.0419, 1.1022], [0, 0.4785, 0.6803], [0, 0.4587, 1.0152], [0, 0.1385, 0.7232], [0, 0.0564, 0.9021]], [[0, 0.1774, 0.2694], [0.8879, 1.1448, 1.0118], [0.9838, 0.8946, 1.7836], [0.5788, 0.8654, 1.5511], [0.8907, 0.8435, 1.4365], [0.5024, 0.5132, 0.7599]], [[0, 0.3657, 0.4784], [0, 0.493, 0.8572], [0, 0.396, 1.3178], [0, 0.0566, 0.8396], [0.2242, 0.8458, 1.3804], [1.6104, 0.947, 0.723]]]]