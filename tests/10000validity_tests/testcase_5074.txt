import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Add3517 = tf.keras.layers.Input(shape=([2, 1, 1, 2]))
in1Add3517 = tf.keras.layers.Input(shape=([2, 1, 1, 2]))
in0Sub36598 = tf.keras.layers.Input(shape=([3]))
in1Sub36598 = tf.keras.layers.Input(shape=([3]))
in0Con19150 = tf.keras.layers.Input(shape=([70]))

Add3517 = keras.layers.Add(name = 'Add3517', )([in0Add3517,in1Add3517])
Zer18544 = keras.layers.ZeroPadding3D(padding=((1, 1), (1, 1), (1, 1)), name = 'Zer18544', )(Add3517)
Res25721 = keras.layers.Reshape((4, 3, 6), name = 'Res25721', )(Zer18544)
Up_21521 = keras.layers.UpSampling2D(size=(1, 1), name = 'Up_21521', )(Res25721)
Fla82025 = keras.layers.Flatten(name = 'Fla82025', )(Up_21521)
Sub36598 = keras.layers.Subtract(name = 'Sub36598', )([in0Sub36598,in1Sub36598])
Den41988 = keras.layers.Dense(2,name = 'Den41988', )(Sub36598)
Con19150 = keras.layers.Concatenate(axis=1, name = 'Con19150', )([Den41988,in0Con19150])
Add45915 = keras.layers.Add(name = 'Add45915', )([Fla82025,Con19150])
model = tf.keras.models.Model(inputs=[in0Add3517,in1Add3517,in0Sub36598,in1Sub36598,in0Con19150], outputs=Add45915)
w = model.get_layer('Den41988').get_weights() 
w[0] = np.array([[0.4302, 0.3412], [0.7712, 0.5689], [0.4575, 0.0621]])
w[1] = np.array([0.0706, 0.4065])
model.get_layer('Den41988').set_weights(w) 
in0Add3517 = tf.constant([[[[[0.7171, 0.6856]]], [[[0.9445, 0.5961]]]]])
in1Add3517 = tf.constant([[[[[0.3813, 0.0093]]], [[[0.3856, 0.224]]]]])
in0Sub36598 = tf.constant([[0.738, 0.7056, 0.4608]])
in1Sub36598 = tf.constant([[0.172, 0.7637, 0.4368]])
in0Con19150 = tf.constant([[0.0951, 0.9776, 0.5636, 0.2775, 0.319, 0.3864, 0.2824, 0.5869, 0.0858, 0.0853, 0.6309, 0.3734, 0.43, 0.2952, 0.9489, 0.9106, 0.448, 0.0917, 0.4723, 0.113, 0.9827, 0.7262, 0.3058, 0.8474, 0.457, 0.7344, 0.6478, 0.6649, 0.7939, 0.6543, 0.83, 0.7004, 0.4955, 0.7191, 0.7599, 0.5482, 0.1176, 0.7234, 0.9103, 0.7704, 0.5729, 0.371, 0.0852, 0.3153, 0.9237, 0.8013, 0.8125, 0.5379, 0.5833, 0.6674, 0.3593, 0.6945, 0.1359, 0.0916, 0.0504, 0.7602, 0.3053, 0.0601, 0.7804, 0.1653, 0.9565, 0.3337, 0.3518, 0.5801, 0.3967, 0.7508, 0.5493, 0.1894, 0.6579, 0.8788]])
print (np.array2string(model.predict([in0Add3517,in1Add3517,in0Sub36598,in1Sub36598,in0Con19150],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Add45915.png')

LAdd3517 = add_layer([[[[[[0.7171, 0.6856]]], [[[0.9445, 0.5961]]]]], [[[[[0.3813, 0.0093]]], [[[0.3856, 0.224]]]]]], Add3517), 
LZer18544 = zero_padding3D_layer(Add3517, 1, 1, 1, 1, 1, 1, Zer18544), 
LRes25721 = reshape_layer(Zer18544, [4, 3, 6], Res25721), 
LUp_21521 = up_sampling2D_layer(Res25721, 1, 1, Up_21521), 
LFla82025 = flatten_layer(Up_21521, Fla82025), 
LSub36598 = subtract_layer([[0.738, 0.7056, 0.4608]], [[0.172, 0.7637, 0.4368]], Sub36598), 
LDen41988 = dense_layer(Sub36598, [[0.4302, 0.3412], [0.7712, 0.5689], [0.4575, 0.0621]],[0.0706, 0.4065], Den41988), 
LCon19150 = concatenate_layer([Den41988,[[0.0951, 0.9776, 0.5636, 0.2775, 0.319, 0.3864, 0.2824, 0.5869, 0.0858, 0.0853, 0.6309, 0.3734, 0.43, 0.2952, 0.9489, 0.9106, 0.448, 0.0917, 0.4723, 0.113, 0.9827, 0.7262, 0.3058, 0.8474, 0.457, 0.7344, 0.6478, 0.6649, 0.7939, 0.6543, 0.83, 0.7004, 0.4955, 0.7191, 0.7599, 0.5482, 0.1176, 0.7234, 0.9103, 0.7704, 0.5729, 0.371, 0.0852, 0.3153, 0.9237, 0.8013, 0.8125, 0.5379, 0.5833, 0.6674, 0.3593, 0.6945, 0.1359, 0.0916, 0.0504, 0.7602, 0.3053, 0.0601, 0.7804, 0.1653, 0.9565, 0.3337, 0.3518, 0.5801, 0.3967, 0.7508, 0.5493, 0.1894, 0.6579, 0.8788]]], 1, Con19150), 
LAdd45915 = add_layer([Fla82025,Con19150], Add45915), 
exec_layers([LAdd3517,LZer18544,LRes25721,LUp_21521,LFla82025,LSub36598,LDen41988,LCon19150,LAdd45915],["Add3517","Zer18544","Res25721","Up_21521","Fla82025","Sub36598","Den41988","Con19150","Add45915"],Add45915,"Add45915")

Actual (Unparsed): [[0.2802665, 0.5680565, 0.0951000, 0.9776000, 0.5636000, 0.2775000, 0.3190000, 0.3864000, 0.2824000, 0.5869000, 0.0858000, 0.0853000, 0.6309000, 0.3734000, 0.4300000, 0.2952000, 0.9489000, 0.9106000, 0.4480000, 0.0917000, 0.4723000, 0.1130000, 0.9827000, 0.7262000, 0.3058000, 0.8474000, 1.5554000, 1.4293000, 0.6478000, 0.6649000, 0.7939000, 0.6543000, 0.8300000, 0.7004000, 0.4955000, 0.7191000, 0.7599000, 0.5482000, 0.1176000, 0.7234000, 0.9103000, 0.7704000, 0.5729000, 0.3710000, 1.4153000, 1.1354000, 0.9237000, 0.8013000, 0.8125000, 0.5379000, 0.5833000, 0.6674000, 0.3593000, 0.6945000, 0.1359000, 0.0916000, 0.0504000, 0.7602000, 0.3053000, 0.0601000, 0.7804000, 0.1653000, 0.9565000, 0.3337000, 0.3518000, 0.5801000, 0.3967000, 0.7508000, 0.5493000, 0.1894000, 0.6579000, 0.8788000]]

Expected (Unparsed): [[0.28026648000000004,0.56805651,0.0951,0.9776,0.5636,0.2775,0.319,0.3864,0.2824,0.5869,0.0858,0.0853,0.6309,0.3734,0.43,0.2952,0.9489,0.9106,0.448,0.0917,0.4723,0.113,0.9827,0.7262,0.3058,0.8474,1.5554,1.4293,0.6478,0.6649,0.7939,0.6543,0.83,0.7004,0.4955,0.7191,0.7599,0.5482,0.1176,0.7234,0.9103,0.7704,0.5729,0.371,1.4153,1.1354,0.9237,0.8013,0.8125,0.5379,0.5833,0.6674,0.3593,0.6945,0.1359,0.0916,0.0504,0.7602,0.3053,0.0601,0.7804,0.1653,0.9565,0.3337,0.3518,0.5801,0.3967,0.7508,0.5493,0.1894,0.6579,0.8788]]

Actual:   [[0.2803, 0.5681, 0.0951, 0.9776, 0.5636, 0.2775, 0.319, 0.3864, 0.2824, 0.5869, 0.0858, 0.0853, 0.6309, 0.3734, 0.43, 0.2952, 0.9489, 0.9106, 0.448, 0.0917, 0.4723, 0.113, 0.9827, 0.7262, 0.3058, 0.8474, 1.5554, 1.4293, 0.6478, 0.6649, 0.7939, 0.6543, 0.83, 0.7004, 0.4955, 0.7191, 0.7599, 0.5482, 0.1176, 0.7234, 0.9103, 0.7704, 0.5729, 0.371, 1.4153, 1.1354, 0.9237, 0.8013, 0.8125, 0.5379, 0.5833, 0.6674, 0.3593, 0.6945, 0.1359, 0.0916, 0.0504, 0.7602, 0.3053, 0.0601, 0.7804, 0.1653, 0.9565, 0.3337, 0.3518, 0.5801, 0.3967, 0.7508, 0.5493, 0.1894, 0.6579, 0.8788]]

Expected: [[0.2803, 0.5681, 0.0951, 0.9776, 0.5636, 0.2775, 0.319, 0.3864, 0.2824, 0.5869, 0.0858, 0.0853, 0.6309, 0.3734, 0.43, 0.2952, 0.9489, 0.9106, 0.448, 0.0917, 0.4723, 0.113, 0.9827, 0.7262, 0.3058, 0.8474, 1.5554, 1.4293, 0.6478, 0.6649, 0.7939, 0.6543, 0.83, 0.7004, 0.4955, 0.7191, 0.7599, 0.5482, 0.1176, 0.7234, 0.9103, 0.7704, 0.5729, 0.371, 1.4153, 1.1354, 0.9237, 0.8013, 0.8125, 0.5379, 0.5833, 0.6674, 0.3593, 0.6945, 0.1359, 0.0916, 0.0504, 0.7602, 0.3053, 0.0601, 0.7804, 0.1653, 0.9565, 0.3337, 0.3518, 0.5801, 0.3967, 0.7508, 0.5493, 0.1894, 0.6579, 0.8788]]