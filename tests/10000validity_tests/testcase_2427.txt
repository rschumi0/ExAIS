import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Mul85583 = tf.keras.layers.Input(shape=([2, 2, 2, 2]))
in1Mul85583 = tf.keras.layers.Input(shape=([2, 2, 2, 2]))
in0GRU68752 = tf.keras.layers.Input(shape=([1, 1]))
in0Add79892 = tf.keras.layers.Input(shape=([2, 1, 1, 1]))
in1Add79892 = tf.keras.layers.Input(shape=([2, 1, 1, 1]))
in0Con96351 = tf.keras.layers.Input(shape=([4, 7]))

Mul85583 = keras.layers.Multiply(name = 'Mul85583', )([in0Mul85583,in1Mul85583])
Res66991 = keras.layers.Reshape((2, 2, 4), name = 'Res66991', )(Mul85583)
Res62852 = keras.layers.Reshape((2, 8), name = 'Res62852', )(Res66991)
Zer97101 = keras.layers.ZeroPadding1D(padding=((2, 0)), name = 'Zer97101', )(Res62852)
GRU68752 = keras.layers.GRU(2,reset_after=False, recurrent_activation='sigmoid', name = 'GRU68752', )(in0GRU68752)
Res49287 = keras.layers.Reshape((2, 1), name = 'Res49287', )(GRU68752)
Zer76776 = keras.layers.ZeroPadding1D(padding=((2, 0)), name = 'Zer76776', )(Res49287)
Add79892 = keras.layers.Add(name = 'Add79892', )([in0Add79892,in1Add79892])
Res47492 = keras.layers.Reshape((2, 1, 1), name = 'Res47492', )(Add79892)
Res87829 = keras.layers.Reshape((2, 1), name = 'Res87829', )(Res47492)
Zer65077 = keras.layers.ZeroPadding1D(padding=((1, 1)), name = 'Zer65077', )(Res87829)
Add21915 = keras.layers.Add(name = 'Add21915', )([Zer76776,Zer65077])
Con96351 = keras.layers.Concatenate(axis=2, name = 'Con96351', )([Add21915,in0Con96351])
Sub11831 = keras.layers.Subtract(name = 'Sub11831', )([Zer97101,Con96351])
model = tf.keras.models.Model(inputs=[in0Mul85583,in1Mul85583,in0GRU68752,in0Add79892,in1Add79892,in0Con96351], outputs=Sub11831)
w = model.get_layer('GRU68752').get_weights() 
w[0] = np.array([[2, 7, 4, 10, 1, 6]])
w[1] = np.array([[4, 3, 3, 6, 2, 7], [1, 1, 5, 6, 8, 5]])
w[2] = np.array([5, 9, 5, 3, 10, 1])
model.get_layer('GRU68752').set_weights(w) 
in0Mul85583 = tf.constant([[[[[0.359, 0.9949], [0.0833, 0.9778]], [[0.4721, 0.235], [0.7712, 0.532]]], [[[0.5923, 0.1873], [0.3111, 0.9984]], [[0.8005, 0.709], [0.886, 0.3787]]]]])
in1Mul85583 = tf.constant([[[[[0.9846, 0.6288], [0.4903, 0.0487]], [[0.3029, 0.2575], [0.9699, 0.3769]]], [[[0.8318, 0.3333], [0.2516, 0.2957]], [[0.9752, 0.1648], [0.2439, 0.8385]]]]])
in0GRU68752 = tf.constant([[[6]]])
in0Add79892 = tf.constant([[[[[0.6205]]], [[[0.1085]]]]])
in1Add79892 = tf.constant([[[[[0.3311]]], [[[0.9296]]]]])
in0Con96351 = tf.constant([[[0.0323, 0.6421, 0.858, 0.3153, 0.1005, 0.0333, 0.1404], [0.0094, 0.3876, 0.2652, 0.2746, 0.3509, 0.0932, 0.6223], [0.5189, 0.6111, 0.0434, 0.5295, 0.3133, 0.8512, 0.4765], [0.6355, 0.5058, 0.3193, 0.3027, 0.0498, 0.4098, 0.8235]]])
print (np.array2string(model.predict([in0Mul85583,in1Mul85583,in0GRU68752,in0Add79892,in1Add79892,in0Con96351],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Sub11831.png')

LMul85583 = multiply_layer([[[[[[0.359, 0.9949], [0.0833, 0.9778]], [[0.4721, 0.235], [0.7712, 0.532]]], [[[0.5923, 0.1873], [0.3111, 0.9984]], [[0.8005, 0.709], [0.886, 0.3787]]]]], [[[[[0.9846, 0.6288], [0.4903, 0.0487]], [[0.3029, 0.2575], [0.9699, 0.3769]]], [[[0.8318, 0.3333], [0.2516, 0.2957]], [[0.9752, 0.1648], [0.2439, 0.8385]]]]]], Mul85583), 
LRes66991 = reshape_layer(Mul85583, [2, 2, 4], Res66991), 
LRes62852 = reshape_layer(Res66991, [2, 8], Res62852), 
LZer97101 = zero_padding1D_layer(Res62852, 2, 0, Zer97101), 
LGRU68752 = gru_layer([[[6]]],[[2, 7, 4, 10, 1, 6]],[[4, 3, 3, 6, 2, 7], [1, 1, 5, 6, 8, 5]],[5, 9, 5, 3, 10, 1], false, GRU68752), 
LRes49287 = reshape_layer(GRU68752, [2, 1], Res49287), 
LZer76776 = zero_padding1D_layer(Res49287, 2, 0, Zer76776), 
LAdd79892 = add_layer([[[[[[0.6205]]], [[[0.1085]]]]], [[[[[0.3311]]], [[[0.9296]]]]]], Add79892), 
LRes47492 = reshape_layer(Add79892, [2, 1, 1], Res47492), 
LRes87829 = reshape_layer(Res47492, [2, 1], Res87829), 
LZer65077 = zero_padding1D_layer(Res87829, 1, 1, Zer65077), 
LAdd21915 = add_layer([Zer76776,Zer65077], Add21915), 
LCon96351 = concatenate_layer([Add21915,[[[0.0323, 0.6421, 0.858, 0.3153, 0.1005, 0.0333, 0.1404], [0.0094, 0.3876, 0.2652, 0.2746, 0.3509, 0.0932, 0.6223], [0.5189, 0.6111, 0.0434, 0.5295, 0.3133, 0.8512, 0.4765], [0.6355, 0.5058, 0.3193, 0.3027, 0.0498, 0.4098, 0.8235]]]], 2, Con96351), 
LSub11831 = subtract_layer(Zer97101,Con96351, Sub11831), 
exec_layers([LMul85583,LRes66991,LRes62852,LZer97101,LGRU68752,LRes49287,LZer76776,LAdd79892,LRes47492,LRes87829,LZer65077,LAdd21915,LCon96351,LSub11831],["Mul85583","Res66991","Res62852","Zer97101","GRU68752","Res49287","Zer76776","Add79892","Res47492","Res87829","Zer65077","Add21915","Con96351","Sub11831"],Sub11831,"Sub11831")

Actual (Unparsed): [[[0.0000000, -0.0323000, -0.6421000, -0.8580000, -0.3153000, -0.1005000, -0.0333000, -0.1404000], [-0.9516000, -0.0094000, -0.3876000, -0.2652000, -0.2746000, -0.3509000, -0.0932000, -0.6223000], [-0.6846286, 0.1066931, -0.5702580, 0.0042189, -0.3865009, -0.2527875, -0.1032131, -0.2759892], [0.4926751, -0.5730729, -0.4275272, -0.0240731, 0.4779476, 0.0670432, -0.1937046, -0.5059600]]]

Expected (Unparsed): [[[0,-0.0323,-0.6421,-0.858,-0.3153,-0.1005,-0.0333,-0.1404],[-0.9516,-0.0094,-0.3876,-0.2652,-0.2746,-0.3509,-0.0932,-0.6223],[-0.6846286413993755,0.10669311999999997,-0.57025801,0.004218859999999998,-0.38650091,-0.25278750000000005,-0.10321311999999994,-0.27598919999999993],[0.49267514,-0.5730729099999999,-0.42752724000000003,-0.024073119999999948,0.4779476,0.0670432,-0.19370459999999998,-0.50596005]]]

Actual:   [[[0, -0.0323, -0.6421, -0.858, -0.3153, -0.1005, -0.0333, -0.1404], [-0.9516, -0.0094, -0.3876, -0.2652, -0.2746, -0.3509, -0.0932, -0.6223], [-0.6846, 0.1067, -0.5702, 0.0043, -0.3865, -0.2527, -0.1032, -0.2759], [0.4927, -0.573, -0.4275, -0.024, 0.478, 0.0671, -0.1937, -0.5059]]]

Expected: [[[0, -0.0323, -0.6421, -0.858, -0.3153, -0.1005, -0.0333, -0.1404], [-0.9516, -0.0094, -0.3876, -0.2652, -0.2746, -0.3509, -0.0932, -0.6223], [-0.6846, 0.1067, -0.5702, 0.0043, -0.3865, -0.2527, -0.1032, -0.2759], [0.4927, -0.573, -0.4275, -0.024, 0.478, 0.0671, -0.1937, -0.5059]]]