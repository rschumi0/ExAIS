import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Up_70723 = tf.keras.layers.Input(shape=([3, 1, 2]))
in0Add98429 = tf.keras.layers.Input(shape=([1, 2, 1, 1]))
in1Add98429 = tf.keras.layers.Input(shape=([1, 2, 1, 1]))
in0Dot57589 = tf.keras.layers.Input(shape=([2]))
in1Dot57589 = tf.keras.layers.Input(shape=([2]))
in0Con68997 = tf.keras.layers.Input(shape=([5, 14]))

Up_70723 = keras.layers.UpSampling2D(size=(1, 2), name = 'Up_70723', )(in0Up_70723)
Res99956 = keras.layers.Reshape((3, 2, 2, 1), name = 'Res99956', )(Up_70723)
Add98429 = keras.layers.Add(name = 'Add98429', )([in0Add98429,in1Add98429])
Bat55156 = keras.layers.BatchNormalization(axis=2, epsilon=0.9396737486014017,  name = 'Bat55156', )(Add98429)
Zer33495 = keras.layers.ZeroPadding3D(padding=((2, 0), (0, 0), (1, 0)), name = 'Zer33495', )(Bat55156)
Mul34304 = keras.layers.Multiply(name = 'Mul34304', )([Res99956,Zer33495])
Zer14009 = keras.layers.ZeroPadding3D(padding=((1, 1), (1, 1), (1, 1)), name = 'Zer14009', )(Mul34304)
Res42586 = keras.layers.Reshape((5, 4, 4), name = 'Res42586', )(Zer14009)
Res35035 = keras.layers.Reshape((5, 16), name = 'Res35035', )(Res42586)
Dot57589 = keras.layers.Dot(axes=(1, 1), name = 'Dot57589', )([in0Dot57589,in1Dot57589])
Res32674 = keras.layers.Reshape((1, 1), name = 'Res32674', )(Dot57589)
Max53021 = keras.layers.MaxPool1D(pool_size=(1), strides=(1), padding='same', name = 'Max53021', )(Res32674)
Loc93485 = keras.layers.LocallyConnected1D(2, (1),strides=(1), name = 'Loc93485', )(Max53021)
Zer89956 = keras.layers.ZeroPadding1D(padding=((4, 0)), name = 'Zer89956', )(Loc93485)
Con68997 = keras.layers.Concatenate(axis=2, name = 'Con68997', )([Zer89956,in0Con68997])
Ave85499 = keras.layers.Average(name = 'Ave85499', )([Res35035,Con68997])
model = tf.keras.models.Model(inputs=[in0Up_70723,in0Add98429,in1Add98429,in0Dot57589,in1Dot57589,in0Con68997], outputs=Ave85499)
w = model.get_layer('Bat55156').get_weights() 
w[0] = np.array([0.1263, 0.6686])
w[1] = np.array([0.9568, 0.5868])
w[2] = np.array([0.3691, 0.5151])
w[3] = np.array([0.0481, 0.3817])
model.get_layer('Bat55156').set_weights(w) 
w = model.get_layer('Loc93485').get_weights() 
w[0] = np.array([[[0.3904, 0.2146]]])
w[1] = np.array([[0, 0]])
model.get_layer('Loc93485').set_weights(w) 
in0Up_70723 = tf.constant([[[[1.7666, 1.5327]], [[1.4495, 1.5938]], [[1.9632, 1.7512]]]])
in0Add98429 = tf.constant([[[[[0.5803]], [[0.8412]]]]])
in1Add98429 = tf.constant([[[[[0.9809]], [[0.2527]]]]])
in0Dot57589 = tf.constant([[0.4019, 0.6699]])
in1Dot57589 = tf.constant([[0.9262, 0.8281]])
in0Con68997 = tf.constant([[[0.8291, 0.6254, 0.7509, 0.415, 0.159, 0.7726, 0.1699, 0.2324, 0.0527, 0.1588, 0.3036, 0.4631, 0.133, 0.6749], [0.555, 0.6174, 0.1613, 0.7914, 0.2177, 0.4145, 0.4074, 0.8857, 0.9481, 0.0278, 0.9441, 0.5709, 0.3154, 0.0435], [0.0077, 0.0761, 0.093, 0.4453, 0.8743, 0.3278, 0.9075, 0.8, 0.5368, 0.7526, 0.8319, 0.9604, 0.834, 0.9822], [0.526, 0.0247, 0.5875, 0.5798, 0.5597, 0.9953, 0.7282, 0.6244, 0.6323, 0.5652, 0.3569, 0.494, 0.4996, 0.7279], [0.1803, 0.4678, 0.5553, 0.4529, 0.7818, 0.8325, 0.18, 0.4991, 0.7488, 0.0225, 0.164, 0.5497, 0.76, 0.9497]]])
print (np.array2string(model.predict([in0Up_70723,in0Add98429,in1Add98429,in0Dot57589,in1Dot57589,in0Con68997],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Ave85499.png')

LUp_70723 = up_sampling2D_layer([[[[1.7666, 1.5327]], [[1.4495, 1.5938]], [[1.9632, 1.7512]]]], 1, 2, Up_70723), 
LRes99956 = reshape_layer(Up_70723, [3, 2, 2, 1], Res99956), 
LAdd98429 = add_layer([[[[[[0.5803]], [[0.8412]]]]], [[[[[0.9809]], [[0.2527]]]]]], Add98429), 
LBat55156 = batch_normalization_layer(Add98429, 2, 0.9396737486014017, [0.1263, 0.6686], [0.9568, 0.5868], [0.3691, 0.5151], [0.0481, 0.3817], Bat55156), 
LZer33495 = zero_padding3D_layer(Bat55156, 2, 0, 0, 0, 1, 0, Zer33495), 
LMul34304 = multiply_layer([Res99956,Zer33495], Mul34304), 
LZer14009 = zero_padding3D_layer(Mul34304, 1, 1, 1, 1, 1, 1, Zer14009), 
LRes42586 = reshape_layer(Zer14009, [5, 4, 4], Res42586), 
LRes35035 = reshape_layer(Res42586, [5, 16], Res35035), 
LDot57589 = dot_layer([[0.4019, 0.6699]], [[0.9262, 0.8281]], 1, 1, Dot57589), 
LRes32674 = reshape_layer(Dot57589, [1, 1], Res32674), 
LMax53021 = max_pool1D_layer(Res32674, 1, 1, true, Max53021), 
LLoc93485 = locally_connected1D_layer(Max53021, 1,[[[0.3904, 0.2146]]],[[0, 0]], 1, Loc93485), 
LZer89956 = zero_padding1D_layer(Loc93485, 4, 0, Zer89956), 
LCon68997 = concatenate_layer([Zer89956,[[[0.8291, 0.6254, 0.7509, 0.415, 0.159, 0.7726, 0.1699, 0.2324, 0.0527, 0.1588, 0.3036, 0.4631, 0.133, 0.6749], [0.555, 0.6174, 0.1613, 0.7914, 0.2177, 0.4145, 0.4074, 0.8857, 0.9481, 0.0278, 0.9441, 0.5709, 0.3154, 0.0435], [0.0077, 0.0761, 0.093, 0.4453, 0.8743, 0.3278, 0.9075, 0.8, 0.5368, 0.7526, 0.8319, 0.9604, 0.834, 0.9822], [0.526, 0.0247, 0.5875, 0.5798, 0.5597, 0.9953, 0.7282, 0.6244, 0.6323, 0.5652, 0.3569, 0.494, 0.4996, 0.7279], [0.1803, 0.4678, 0.5553, 0.4529, 0.7818, 0.8325, 0.18, 0.4991, 0.7488, 0.0225, 0.164, 0.5497, 0.76, 0.9497]]]], 2, Con68997), 
LAve85499 = average_layer([Res35035,Con68997], Ave85499), 
exec_layers([LUp_70723,LRes99956,LAdd98429,LBat55156,LZer33495,LMul34304,LZer14009,LRes42586,LRes35035,LDot57589,LRes32674,LMax53021,LLoc93485,LZer89956,LCon68997,LAve85499],["Up_70723","Res99956","Add98429","Bat55156","Zer33495","Mul34304","Zer14009","Res42586","Res35035","Dot57589","Res32674","Max53021","Loc93485","Zer89956","Con68997","Ave85499"],Ave85499,"Ave85499")

Actual (Unparsed): [[[0.0000000, 0.0000000, 0.4145500, 0.3127000, 0.3754500, 0.2075000, 0.0795000, 0.3863000, 0.0849500, 0.1162000, 0.0263500, 0.0794000, 0.1518000, 0.2315500, 0.0665000, 0.3374500], [0.0000000, 0.0000000, 0.2775000, 0.3087000, 0.0806500, 0.3957000, 0.1088500, 0.2072500, 0.2037000, 0.4428500, 0.4740500, 0.0139000, 0.4720500, 0.2854500, 0.1577000, 0.0217500], [0.0000000, 0.0000000, 0.0038500, 0.0380500, 0.0465000, 0.2226500, 0.4371500, 0.1639000, 0.4537500, 0.4000000, 0.2684000, 0.3763000, 0.4159500, 0.4802000, 0.4170000, 0.4911000], [0.0000000, 0.0000000, 0.2630000, 0.0123500, 0.2937500, 0.2899000, 1.2502697, 0.4976500, 0.3641000, 0.3122000, 1.1247251, 0.2826000, 0.1784500, 0.2470000, 0.2498000, 0.3639500], [0.1809473, 0.0994654, 0.0901500, 0.2339000, 0.2776500, 0.2264500, 0.3909000, 0.4162500, 0.0900000, 0.2495500, 0.3744000, 0.0112500, 0.0820000, 0.2748500, 0.3800000, 0.4748500]]]

Expected (Unparsed): [[[0,0,0.41455,0.3127,0.37545,0.2075,0.0795,0.3863,0.08495,0.1162,0.02635,0.0794,0.1518,0.23155,0.0665,0.33745],[0,0,0.2775,0.3087,0.08065,0.3957,0.10885,0.20725,0.2037,0.44285,0.47405,0.0139,0.47205,0.28545,0.1577,0.02175],[0,0,0.00385,0.03805,0.0465,0.22265,0.43715,0.1639,0.45375,0.4,0.2684,0.3763,0.41595,0.4802,0.417,0.4911],[0,0,0.263,0.01235,0.29375,0.2899,1.2502697419657272,0.49765,0.3641,0.3122,1.1247251539267729,0.2826,0.17845,0.247,0.2498,0.36395],[0.180947270944,0.09946537998100001,0.09015,0.2339,0.27765,0.22645,0.3909,0.41625,0.09,0.24955,0.3744,0.01125,0.082,0.27485,0.38,0.47485]]]

Actual:   [[[0, 0, 0.4146, 0.3127, 0.3755, 0.2075, 0.0795, 0.3863, 0.085, 0.1162, 0.0264, 0.0794, 0.1518, 0.2316, 0.0665, 0.3375], [0, 0, 0.2775, 0.3087, 0.0807, 0.3957, 0.1089, 0.2073, 0.2037, 0.4429, 0.4741, 0.0139, 0.4721, 0.2855, 0.1577, 0.0218], [0, 0, 0.0039, 0.0381, 0.0465, 0.2227, 0.4372, 0.1639, 0.4538, 0.4, 0.2684, 0.3763, 0.416, 0.4802, 0.417, 0.4911], [0, 0, 0.263, 0.0124, 0.2938, 0.2899, 1.2503, 0.4977, 0.3641, 0.3122, 1.1248, 0.2826, 0.1785, 0.247, 0.2498, 0.364], [0.181, 0.0995, 0.0902, 0.2339, 0.2777, 0.2265, 0.3909, 0.4163, 0.09, 0.2496, 0.3744, 0.0113, 0.082, 0.2749, 0.38, 0.4749]]]

Expected: [[[0, 0, 0.4146, 0.3127, 0.3755, 0.2075, 0.0795, 0.3863, 0.085, 0.1162, 0.0264, 0.0794, 0.1518, 0.2316, 0.0665, 0.3375], [0, 0, 0.2775, 0.3087, 0.0807, 0.3957, 0.1089, 0.2073, 0.2037, 0.4429, 0.4741, 0.0139, 0.4721, 0.2855, 0.1577, 0.0218], [0, 0, 0.0039, 0.0381, 0.0465, 0.2227, 0.4372, 0.1639, 0.4538, 0.4, 0.2684, 0.3763, 0.416, 0.4802, 0.417, 0.4911], [0, 0, 0.263, 0.0124, 0.2938, 0.2899, 1.2503, 0.4977, 0.3641, 0.3122, 1.1248, 0.2826, 0.1785, 0.247, 0.2498, 0.364], [0.181, 0.0995, 0.0902, 0.2339, 0.2777, 0.2265, 0.3909, 0.4163, 0.09, 0.2496, 0.3744, 0.0113, 0.082, 0.2749, 0.38, 0.4749]]]