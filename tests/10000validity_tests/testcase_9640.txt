import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Sub50472 = tf.keras.layers.Input(shape=([2, 3, 3, 3]))
in1Sub50472 = tf.keras.layers.Input(shape=([2, 3, 3, 3]))

Sub50472 = keras.layers.Subtract(name = 'Sub50472', )([in0Sub50472,in1Sub50472])
Bat88291 = keras.layers.BatchNormalization(axis=2, epsilon=0.8950657875410943,  name = 'Bat88291', )(Sub50472)
model = tf.keras.models.Model(inputs=[in0Sub50472,in1Sub50472], outputs=Bat88291)
w = model.get_layer('Bat88291').get_weights() 
w[0] = np.array([0.6564, 0.5727, 0.1456])
w[1] = np.array([0.5417, 0.0406, 0.9595])
w[2] = np.array([0.4444, 0.393, 0.2451])
w[3] = np.array([0.397, 0.3733, 0.4232])
model.get_layer('Bat88291').set_weights(w) 
in0Sub50472 = tf.constant([[[[[0.3699, 0.1343, 0.2951], [0.2864, 0.668, 0.6104], [0.5801, 0.4767, 0.9812]], [[0.6542, 0.9411, 0.1634], [0.9738, 0.7219, 0.2546], [0.195, 0.6781, 0.6344]], [[0.5879, 0.86, 0.3362], [0.6763, 0.8538, 0.9777], [0.2163, 0.3999, 0.938]]], [[[0.242, 0.901, 0.9806], [0.505, 0.4057, 0.6883], [0.7475, 0.9116, 0.8805]], [[0.2991, 0.948, 0.7868], [0.4594, 0.0424, 0.8957], [0.8729, 0.1667, 0.9529]], [[0.3994, 0.6663, 0.6887], [0.1889, 0.7711, 0.2432], [0.4465, 0.1508, 0.7747]]]]])
in1Sub50472 = tf.constant([[[[[0.4117, 0.1315, 0.3683], [0.589, 0.992, 0.4479], [0.1901, 0.2338, 0.5949]], [[0.8775, 0.7066, 0.9339], [0.7412, 0.5166, 0.1392], [0.5817, 0.5454, 0.4201]], [[0.1348, 0.7992, 0.4729], [0.4334, 0.021, 0.0984], [0.8714, 0.1927, 0.0978]]], [[[0.2903, 0.0783, 0.8298], [0.4658, 0.6056, 0.908], [0.9319, 0.728, 0.9508]], [[0.2356, 0.4732, 0.1372], [0.353, 0.21, 0.7639], [0.4102, 0.9164, 0.3559]], [[0.94, 0.4799, 0.5556], [0.4482, 0.1575, 0.9132], [0.3563, 0.5853, 0.4475]]]]])
print (np.array2string(model.predict([in0Sub50472,in1Sub50472],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Bat88291.png')

LSub50472 = subtract_layer([[[[[0.3699, 0.1343, 0.2951], [0.2864, 0.668, 0.6104], [0.5801, 0.4767, 0.9812]], [[0.6542, 0.9411, 0.1634], [0.9738, 0.7219, 0.2546], [0.195, 0.6781, 0.6344]], [[0.5879, 0.86, 0.3362], [0.6763, 0.8538, 0.9777], [0.2163, 0.3999, 0.938]]], [[[0.242, 0.901, 0.9806], [0.505, 0.4057, 0.6883], [0.7475, 0.9116, 0.8805]], [[0.2991, 0.948, 0.7868], [0.4594, 0.0424, 0.8957], [0.8729, 0.1667, 0.9529]], [[0.3994, 0.6663, 0.6887], [0.1889, 0.7711, 0.2432], [0.4465, 0.1508, 0.7747]]]]], [[[[[0.4117, 0.1315, 0.3683], [0.589, 0.992, 0.4479], [0.1901, 0.2338, 0.5949]], [[0.8775, 0.7066, 0.9339], [0.7412, 0.5166, 0.1392], [0.5817, 0.5454, 0.4201]], [[0.1348, 0.7992, 0.4729], [0.4334, 0.021, 0.0984], [0.8714, 0.1927, 0.0978]]], [[[0.2903, 0.0783, 0.8298], [0.4658, 0.6056, 0.908], [0.9319, 0.728, 0.9508]], [[0.2356, 0.4732, 0.1372], [0.353, 0.21, 0.7639], [0.4102, 0.9164, 0.3559]], [[0.94, 0.4799, 0.5556], [0.4482, 0.1575, 0.9132], [0.3563, 0.5853, 0.4475]]]]], Sub50472), 
LBat88291 = batch_normalization_layer(Sub50472, 2, 0.8950657875410943, [0.6564, 0.5727, 0.1456], [0.5417, 0.0406, 0.9595], [0.4444, 0.393, 0.2451], [0.397, 0.3733, 0.4232], Bat88291), 
exec_layers([LSub50472,LBat88291],["Sub50472","Bat88291"],Bat88291,"Bat88291")

Actual (Unparsed): [[[[[0.2609361, 0.2866911, 0.2428037], [0.1103331, 0.0979753, 0.3789124], [0.5102859, 0.4253407, 0.5081492]], [[-0.2727986, -0.0399998, -0.5510588], [-0.0409660, -0.0548485, -0.1005641], [-0.3558902, -0.0917668, -0.0502719]], [[0.9858769, 0.9361286, 0.9110832], [0.9592210, 1.0340273, 1.0399241], [0.8453440, 0.9546938, 1.0349657]]], [[[0.2571826, 0.7601553, 0.3721560], [0.3077109, 0.1696388, 0.1582050], [0.1785895, 0.3910969, 0.2444784]], [[-0.1269561, 0.0821966, 0.1710853], [-0.1051408, -0.2444743, -0.0922245], [0.0760436, -0.5404816, 0.1443373]], [[0.8598639, 0.9520561, 0.9452971], [0.8955361, 1.0062302, 0.8434545], [0.9398568, 0.8733187, 0.9699113]]]]]

Expected (Unparsed): [[[[[0.26093613428810125,0.2866911083949517,0.2428037085716191],[0.11033305699961266,0.09797528915462161,0.378912394602665],[0.5102858611790881,0.4253406438894537,0.5081492377666365]],[[-0.2727986283350805,-0.03999984194565999,-0.551058776679971],[-0.04096602301630198,-0.05484851945236832,-0.10056413958432314],[-0.35589020041029096,-0.0917668066779514,-0.050271872275643184]],[[0.9858768675991413,0.9361285735647994,0.9110832305319609],[0.9592210139003937,1.0340273321539197,1.0399240838046893],[0.8453439605156398,0.9546938303749641,1.0349657399435046]]],[[[0.257182606671632,0.7601553072785093,0.37215604489302045],[0.30771086304717943,0.1696387933398265,0.1582049707542741],[0.17858951304063786,0.3910969227115113,0.24447835935435153]],[[-0.1269561383034383,0.08219663767290208,0.1710852961719644],[-0.10514078676104832,-0.24447426747468134,-0.09222447139562392],[0.07604358980197161,-0.5404816365382062,0.14433733600577056]],[[0.8598639188815131,0.9520561436150501,0.9452970712927701],[0.8955360960720825,1.0062301716840556,0.8434544637501243],[0.9398568423504472,0.8733186575943443,0.9699112539898533]]]]]

Actual:   [[[[[0.261, 0.2867, 0.2429], [0.1104, 0.098, 0.379], [0.5103, 0.4254, 0.5082]], [[-0.2727, -0.0399, -0.551], [-0.0409, -0.0548, -0.1005], [-0.3558, -0.0917, -0.0502]], [[0.9859, 0.9362, 0.9111], [0.9593, 1.0341, 1.04], [0.8454, 0.9547, 1.035]]], [[[0.2572, 0.7602, 0.3722], [0.3078, 0.1697, 0.1583], [0.1786, 0.3911, 0.2445]], [[-0.1269, 0.0822, 0.1711], [-0.1051, -0.2444, -0.0922], [0.0761, -0.5404, 0.1444]], [[0.8599, 0.9521, 0.9453], [0.8956, 1.0063, 0.8435], [0.9399, 0.8734, 0.97]]]]]

Expected: [[[[[0.261, 0.2867, 0.2429], [0.1104, 0.098, 0.379], [0.5103, 0.4254, 0.5082]], [[-0.2727, -0.0399, -0.551], [-0.0409, -0.0548, -0.1005], [-0.3558, -0.0917, -0.0502]], [[0.9859, 0.9362, 0.9111], [0.9593, 1.0341, 1.04], [0.8454, 0.9547, 1.035]]], [[[0.2572, 0.7602, 0.3722], [0.3078, 0.1697, 0.1583], [0.1786, 0.3911, 0.2445]], [[-0.1269, 0.0822, 0.1711], [-0.1051, -0.2444, -0.0922], [0.0761, -0.5404, 0.1444]], [[0.8599, 0.9521, 0.9453], [0.8956, 1.0063, 0.8435], [0.9399, 0.8734, 0.97]]]]]