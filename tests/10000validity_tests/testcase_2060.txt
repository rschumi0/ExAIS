import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Dot19807 = tf.keras.layers.Input(shape=([3, 2]))
in1Dot19807 = tf.keras.layers.Input(shape=([3, 2]))
in0Con58754 = tf.keras.layers.Input(shape=([1, 2, 1]))
in0Glo36252 = tf.keras.layers.Input(shape=([1, 2, 1]))
in0Con95203 = tf.keras.layers.Input(shape=([7]))
in0Con64221 = tf.keras.layers.Input(shape=([8, 1, 1]))
in0Min8616 = tf.keras.layers.Input(shape=([2, 1, 2]))
in1Min8616 = tf.keras.layers.Input(shape=([2, 1, 2]))
in0Con92118 = tf.keras.layers.Input(shape=([214]))

Dot19807 = keras.layers.Dot(axes=(2, 2), name = 'Dot19807', )([in0Dot19807,in1Dot19807])
ReL93220 = keras.layers.ReLU(max_value=1.279417962787954, negative_slope=2.9012070273086916, threshold=7.616601726297128, name = 'ReL93220', )(Dot19807)
Res75378 = keras.layers.Reshape((3, 3, 1), name = 'Res75378', )(ReL93220)
Res36216 = keras.layers.Reshape((3, 3, 1, 1), name = 'Res36216', )(Res75378)
Con26545 = keras.layers.Conv3DTranspose(3, (2, 2, 1),strides=(1, 1, 8), padding='same', name = 'Con26545', )(Res36216)
Res81463 = keras.layers.Reshape((3, 3, 24), name = 'Res81463', )(Con26545)
Res886 = keras.layers.Reshape((3, 72), name = 'Res886', )(Res81463)
Fla99171 = keras.layers.Flatten(name = 'Fla99171', )(Res886)
Con58754 = keras.layers.Conv2D(4, (1, 1),strides=(1, 1), padding='same', dilation_rate=(1, 1), name = 'Con58754', )(in0Con58754)
Res56568 = keras.layers.Reshape((1, 8), name = 'Res56568', )(Con58754)
Fla93763 = keras.layers.Flatten(name = 'Fla93763', )(Res56568)
Glo36252 = keras.layers.GlobalMaxPool2D(name = 'Glo36252', )(in0Glo36252)
Con95203 = keras.layers.Concatenate(axis=1, name = 'Con95203', )([Glo36252,in0Con95203])
Ave1320 = keras.layers.Average(name = 'Ave1320', )([Fla93763,Con95203])
Res91517 = keras.layers.Reshape((8, 1), name = 'Res91517', )(Ave1320)
Res45758 = keras.layers.Reshape((8, 1, 1), name = 'Res45758', )(Res91517)
Con64221 = keras.layers.Concatenate(axis=3, name = 'Con64221', )([Res45758,in0Con64221])
Min8616 = keras.layers.Minimum(name = 'Min8616', )([in0Min8616,in1Min8616])
Zer35447 = keras.layers.ZeroPadding2D(padding=((6, 0), (0, 0)), name = 'Zer35447', )(Min8616)
Max51366 = keras.layers.Maximum(name = 'Max51366', )([Con64221,Zer35447])
Res78258 = keras.layers.Reshape((8, 2), name = 'Res78258', )(Max51366)
Glo87201 = keras.layers.GlobalAveragePooling1D(name = 'Glo87201', )(Res78258)
Con92118 = keras.layers.Concatenate(axis=1, name = 'Con92118', )([Glo87201,in0Con92118])
Max39231 = keras.layers.Maximum(name = 'Max39231', )([Fla99171,Con92118])
model = tf.keras.models.Model(inputs=[in0Dot19807,in1Dot19807,in0Con58754,in0Glo36252,in0Con95203,in0Con64221,in0Min8616,in1Min8616,in0Con92118], outputs=Max39231)
w = model.get_layer('Con26545').get_weights() 
w[0] = np.array([[[[[0.9176], [0.1299], [0.4553]]], [[[0.1139], [0.942], [0.8219]]]], [[[[0.6341], [0.222], [0.8633]]], [[[0.227], [0.3807], [0.8161]]]]])
w[1] = np.array([0, 0, 0])
model.get_layer('Con26545').set_weights(w) 
w = model.get_layer('Con58754').get_weights() 
w[0] = np.array([[[[0.5522, 0.2776, 0.5822, 0.3546]]]])
w[1] = np.array([0, 0, 0, 0])
model.get_layer('Con58754').set_weights(w) 
in0Dot19807 = tf.constant([[[0.7822, 0.6021], [0.3496, 0.408], [0.4135, 0.8436]]])
in1Dot19807 = tf.constant([[[0.003, 0.0193], [0.2646, 0.162], [0.8191, 0.4991]]])
in0Con58754 = tf.constant([[[[0.0639], [0.3396]]]])
in0Glo36252 = tf.constant([[[[1.9066], [1.1239]]]])
in0Con95203 = tf.constant([[0.2381, 0.7831, 0.5034, 0.3285, 0.9377, 0.0954, 0.2928]])
in0Con64221 = tf.constant([[[[0.537]], [[0.3128]], [[0.2273]], [[0.4405]], [[0.598]], [[0.1163]], [[0.318]], [[0.4179]]]])
in0Min8616 = tf.constant([[[[0.1807, 0.8189]], [[0.2867, 0.6609]]]])
in1Min8616 = tf.constant([[[[0.917, 0.8334]], [[0.4866, 0.6308]]]])
in0Con92118 = tf.constant([[0.4323, 0.3051, 0.2269, 0.8256, 0.5344, 0.8053, 0.1767, 0.7814, 0.4296, 0.3898, 0.0242, 0.3443, 0.0728, 0.222, 0.2336, 0.1073, 0.7704, 0.1354, 0.9163, 0.0154, 0.4618, 0.3712, 0.3686, 0.8269, 0.5809, 0.8946, 0.9701, 0.0189, 0.3972, 0.356, 0.399, 0.6596, 0.6739, 0.8069, 0.8281, 0.1996, 0.1989, 0.3486, 0.7471, 0.0944, 0.2478, 0.9955, 0.326, 0.6683, 0.318, 0.169, 0.7806, 0.649, 0.5382, 0.4023, 0.0319, 0.361, 0.001, 0.0335, 0.0597, 0.653, 0.8938, 0.8901, 0.7431, 0.2496, 0.7085, 0.2301, 0.6543, 0.3715, 0.0004, 0.491, 0.2246, 0.145, 0.1369, 0.7225, 0.9383, 0.5355, 0.2247, 0.9296, 0.7254, 0.2838, 0.9405, 0.9371, 0.5236, 0.5952, 0.3008, 0.8005, 0.0573, 0.1406, 0.5011, 0.2374, 0.2429, 0.2634, 0.8332, 0.0274, 0.2713, 0.1556, 0.3434, 0.8393, 0.3406, 0.1644, 0.336, 0.4855, 0.2167, 0.3221, 0.0433, 0.1713, 0.2843, 0.6558, 0.7854, 0.9135, 0.5187, 0.8216, 0.7279, 0.9332, 0.8546, 0.139, 0.9033, 0.4948, 0.759, 0.1508, 0.5131, 0.358, 0.425, 0.4247, 0.6485, 0.7384, 0.8945, 0.43, 0.4778, 0.4302, 0.5912, 0.9701, 0.6363, 0.5355, 0.1996, 0.836, 0.0224, 0.0641, 0.56, 0.0545, 0.4593, 0.5235, 0.2674, 0.3494, 0.7075, 0.4977, 0.9589, 0.2091, 0.1325, 0.1348, 0.8545, 0.2581, 0.9233, 0.4034, 0.9752, 0.4675, 0.1351, 0.9594, 0.605, 0.1091, 0.9851, 0.4723, 0.9857, 0.2537, 0.7717, 0.3004, 0.9379, 0.0495, 0.503, 0.474, 0.0864, 0.9737, 0.8255, 0.0519, 0.6348, 0.7478, 0.124, 0.0461, 0.1836, 0.6274, 0.7262, 0.277, 0.9116, 0.3375, 0.9942, 0.7185, 0.5553, 0.0064, 0.7055, 0.116, 0.0091, 0.074, 0.5383, 0.9249, 0.0431, 0.2956, 0.662, 0.9022, 0.5899, 0.6465, 0.4034, 0.0065, 0.2862, 0.1429, 0.71, 0.6168, 0.4734, 0.336, 0.6669, 0.4605, 0.6557, 0.5158, 0.6319, 0.0583, 0.4976, 0.4607, 0.6538, 0.9396]])
print (np.array2string(model.predict([in0Dot19807,in1Dot19807,in0Con58754,in0Glo36252,in0Con95203,in0Con64221,in0Min8616,in1Min8616,in0Con92118],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Max39231.png')

LDot19807 = dot_layer([[[0.7822, 0.6021], [0.3496, 0.408], [0.4135, 0.8436]]], [[[0.003, 0.0193], [0.2646, 0.162], [0.8191, 0.4991]]], 2, 2, Dot19807), 
LReL93220 = relu_layer(Dot19807, 1.279417962787954, 2.9012070273086916, 7.616601726297128, ReL93220), 
LRes75378 = reshape_layer(ReL93220, [3, 3, 1], Res75378), 
LRes36216 = reshape_layer(Res75378, [3, 3, 1, 1], Res36216), 
LCon26545 = conv3D_transpose_layer(Res36216, 2, 2, 1,[[[[[0.9176], [0.1299], [0.4553]]], [[[0.1139], [0.942], [0.8219]]]], [[[[0.6341], [0.222], [0.8633]]], [[[0.227], [0.3807], [0.8161]]]]],[0, 0, 0], 1, 1, 8, true, Con26545), 
LRes81463 = reshape_layer(Con26545, [3, 3, 24], Res81463), 
LRes886 = reshape_layer(Res81463, [3, 72], Res886), 
LFla99171 = flatten_layer(Res886, Fla99171), 
LCon58754 = conv2D_layer([[[[0.0639], [0.3396]]]], 1, 1,[[[[0.5522, 0.2776, 0.5822, 0.3546]]]],[0, 0, 0, 0], 1, 1, true, 1, 1, Con58754), 
LRes56568 = reshape_layer(Con58754, [1, 8], Res56568), 
LFla93763 = flatten_layer(Res56568, Fla93763), 
LGlo36252 = global_max_pool2D_layer([[[[1.9066], [1.1239]]]], Glo36252), 
LCon95203 = concatenate_layer([Glo36252,[[0.2381, 0.7831, 0.5034, 0.3285, 0.9377, 0.0954, 0.2928]]], 1, Con95203), 
LAve1320 = average_layer([Fla93763,Con95203], Ave1320), 
LRes91517 = reshape_layer(Ave1320, [8, 1], Res91517), 
LRes45758 = reshape_layer(Res91517, [8, 1, 1], Res45758), 
LCon64221 = concatenate_layer([Res45758,[[[[0.537]], [[0.3128]], [[0.2273]], [[0.4405]], [[0.598]], [[0.1163]], [[0.318]], [[0.4179]]]]], 3, Con64221), 
LMin8616 = minimum_layer([[[[[0.1807, 0.8189]], [[0.2867, 0.6609]]]], [[[[0.917, 0.8334]], [[0.4866, 0.6308]]]]], Min8616), 
LZer35447 = zero_padding2D_layer(Min8616, 6, 0, 0, 0, Zer35447), 
LMax51366 = maximum_layer([Con64221,Zer35447], Max51366), 
LRes78258 = reshape_layer(Max51366, [8, 2], Res78258), 
LGlo87201 = global_average_pooling1D_layer(Res78258, Glo87201), 
LCon92118 = concatenate_layer([Glo87201,[[0.4323, 0.3051, 0.2269, 0.8256, 0.5344, 0.8053, 0.1767, 0.7814, 0.4296, 0.3898, 0.0242, 0.3443, 0.0728, 0.222, 0.2336, 0.1073, 0.7704, 0.1354, 0.9163, 0.0154, 0.4618, 0.3712, 0.3686, 0.8269, 0.5809, 0.8946, 0.9701, 0.0189, 0.3972, 0.356, 0.399, 0.6596, 0.6739, 0.8069, 0.8281, 0.1996, 0.1989, 0.3486, 0.7471, 0.0944, 0.2478, 0.9955, 0.326, 0.6683, 0.318, 0.169, 0.7806, 0.649, 0.5382, 0.4023, 0.0319, 0.361, 0.001, 0.0335, 0.0597, 0.653, 0.8938, 0.8901, 0.7431, 0.2496, 0.7085, 0.2301, 0.6543, 0.3715, 0.0004, 0.491, 0.2246, 0.145, 0.1369, 0.7225, 0.9383, 0.5355, 0.2247, 0.9296, 0.7254, 0.2838, 0.9405, 0.9371, 0.5236, 0.5952, 0.3008, 0.8005, 0.0573, 0.1406, 0.5011, 0.2374, 0.2429, 0.2634, 0.8332, 0.0274, 0.2713, 0.1556, 0.3434, 0.8393, 0.3406, 0.1644, 0.336, 0.4855, 0.2167, 0.3221, 0.0433, 0.1713, 0.2843, 0.6558, 0.7854, 0.9135, 0.5187, 0.8216, 0.7279, 0.9332, 0.8546, 0.139, 0.9033, 0.4948, 0.759, 0.1508, 0.5131, 0.358, 0.425, 0.4247, 0.6485, 0.7384, 0.8945, 0.43, 0.4778, 0.4302, 0.5912, 0.9701, 0.6363, 0.5355, 0.1996, 0.836, 0.0224, 0.0641, 0.56, 0.0545, 0.4593, 0.5235, 0.2674, 0.3494, 0.7075, 0.4977, 0.9589, 0.2091, 0.1325, 0.1348, 0.8545, 0.2581, 0.9233, 0.4034, 0.9752, 0.4675, 0.1351, 0.9594, 0.605, 0.1091, 0.9851, 0.4723, 0.9857, 0.2537, 0.7717, 0.3004, 0.9379, 0.0495, 0.503, 0.474, 0.0864, 0.9737, 0.8255, 0.0519, 0.6348, 0.7478, 0.124, 0.0461, 0.1836, 0.6274, 0.7262, 0.277, 0.9116, 0.3375, 0.9942, 0.7185, 0.5553, 0.0064, 0.7055, 0.116, 0.0091, 0.074, 0.5383, 0.9249, 0.0431, 0.2956, 0.662, 0.9022, 0.5899, 0.6465, 0.4034, 0.0065, 0.2862, 0.1429, 0.71, 0.6168, 0.4734, 0.336, 0.6669, 0.4605, 0.6557, 0.5158, 0.6319, 0.0583, 0.4976, 0.4607, 0.6538, 0.9396]]], 1, Con92118), 
LMax39231 = maximum_layer([Fla99171,Con92118], Max39231), 
exec_layers([LDot19807,LReL93220,LRes75378,LRes36216,LCon26545,LRes81463,LRes886,LFla99171,LCon58754,LRes56568,LFla93763,LGlo36252,LCon95203,LAve1320,LRes91517,LRes45758,LCon64221,LMin8616,LZer35447,LMax51366,LRes78258,LGlo87201,LCon92118,LMax39231],["Dot19807","ReL93220","Res75378","Res36216","Con26545","Res81463","Res886","Fla99171","Con58754","Res56568","Fla93763","Glo36252","Con95203","Ave1320","Res91517","Res45758","Con64221","Min8616","Zer35447","Max51366","Res78258","Glo87201","Con92118","Max39231"],Max39231,"Max39231")

Actual (Unparsed): [[0.3766804, 0.4602000, 0.4323000, 0.3051000, 0.2269000, 0.8256000, 0.5344000, 0.8053000, 0.1767000, 0.7814000, 0.4296000, 0.3898000, 0.0242000, 0.3443000, 0.0728000, 0.2220000, 0.2336000, 0.1073000, 0.7704000, 0.1354000, 0.9163000, 0.0154000, 0.4618000, 0.3712000, 0.3686000, 0.8269000, 0.5809000, 0.8946000, 0.9701000, 0.0189000, 0.3972000, 0.3560000, 0.3990000, 0.6596000, 0.6739000, 0.8069000, 0.8281000, 0.1996000, 0.1989000, 0.3486000, 0.7471000, 0.0944000, 0.2478000, 0.9955000, 0.3260000, 0.6683000, 0.3180000, 0.1690000, 0.7806000, 0.6490000, 0.5382000, 0.4023000, 0.0319000, 0.3610000, 0.0010000, 0.0335000, 0.0597000, 0.6530000, 0.8938000, 0.8901000, 0.7431000, 0.2496000, 0.7085000, 0.2301000, 0.6543000, 0.3715000, 0.0004000, 0.4910000, 0.2246000, 0.1450000, 0.1369000, 0.7225000, 0.9383000, 0.5355000, 0.2247000, 0.9296000, 0.7254000, 0.2838000, 0.9405000, 0.9371000, 0.5236000, 0.5952000, 0.3008000, 0.8005000, 0.0573000, 0.1406000, 0.5011000, 0.2374000, 0.2429000, 0.2634000, 0.8332000, 0.0274000, 0.2713000, 0.1556000, 0.3434000, 0.8393000, 0.3406000, 0.1644000, 0.3360000, 0.4855000, 0.2167000, 0.3221000, 0.0433000, 0.1713000, 0.2843000, 0.6558000, 0.7854000, 0.9135000, 0.5187000, 0.8216000, 0.7279000, 0.9332000, 0.8546000, 0.1390000, 0.9033000, 0.4948000, 0.7590000, 0.1508000, 0.5131000, 0.3580000, 0.4250000, 0.4247000, 0.6485000, 0.7384000, 0.8945000, 0.4300000, 0.4778000, 0.4302000, 0.5912000, 0.9701000, 0.6363000, 0.5355000, 0.1996000, 0.8360000, 0.0224000, 0.0641000, 0.5600000, 0.0545000, 0.4593000, 0.5235000, 0.2674000, 0.3494000, 0.7075000, 0.4977000, 0.9589000, 0.2091000, 0.1325000, 0.1348000, 0.8545000, 0.2581000, 0.9233000, 0.4034000, 0.9752000, 0.4675000, 0.1351000, 0.9594000, 0.6050000, 0.1091000, 0.9851000, 0.4723000, 0.9857000, 0.2537000, 0.7717000, 0.3004000, 0.9379000, 0.0495000, 0.5030000, 0.4740000, 0.0864000, 0.9737000, 0.8255000, 0.0519000, 0.6348000, 0.7478000, 0.1240000, 0.0461000, 0.1836000, 0.6274000, 0.7262000, 0.2770000, 0.9116000, 0.3375000, 0.9942000, 0.7185000, 0.5553000, 0.0064000, 0.7055000, 0.1160000, 0.0091000, 0.0740000, 0.5383000, 0.9249000, 0.0431000, 0.2956000, 0.6620000, 0.9022000, 0.5899000, 0.6465000, 0.4034000, 0.0065000, 0.2862000, 0.1429000, 0.7100000, 0.6168000, 0.4734000, 0.3360000, 0.6669000, 0.4605000, 0.6557000, 0.5158000, 0.6319000, 0.0583000, 0.4976000, 0.4607000, 0.6538000, 0.9396000]]

Expected (Unparsed): [[0.37668036375,0.46019999999999994,0.4323,0.3051,0.2269,0.8256,0.5344,0.8053,0.1767,0.7814,0.4296,0.3898,0.0242,0.3443,0.0728,0.222,0.2336,0.1073,0.7704,0.1354,0.9163,0.0154,0.4618,0.3712,0.3686,0.8269,0.5809,0.8946,0.9701,0.0189,0.3972,0.356,0.399,0.6596,0.6739,0.8069,0.8281,0.1996,0.1989,0.3486,0.7471,0.0944,0.2478,0.9955,0.326,0.6683,0.318,0.169,0.7806,0.649,0.5382,0.4023,0.0319,0.361,0.001,0.0335,0.0597,0.653,0.8938,0.8901,0.7431,0.2496,0.7085,0.2301,0.6543,0.3715,0.0004,0.491,0.2246,0.145,0.1369,0.7225,0.9383,0.5355,0.2247,0.9296,0.7254,0.2838,0.9405,0.9371,0.5236,0.5952,0.3008,0.8005,0.0573,0.1406,0.5011,0.2374,0.2429,0.2634,0.8332,0.0274,0.2713,0.1556,0.3434,0.8393,0.3406,0.1644,0.336,0.4855,0.2167,0.3221,0.0433,0.1713,0.2843,0.6558,0.7854,0.9135,0.5187,0.8216,0.7279,0.9332,0.8546,0.139,0.9033,0.4948,0.759,0.1508,0.5131,0.358,0.425,0.4247,0.6485,0.7384,0.8945,0.43,0.4778,0.4302,0.5912,0.9701,0.6363,0.5355,0.1996,0.836,0.0224,0.0641,0.56,0.0545,0.4593,0.5235,0.2674,0.3494,0.7075,0.4977,0.9589,0.2091,0.1325,0.1348,0.8545,0.2581,0.9233,0.4034,0.9752,0.4675,0.1351,0.9594,0.605,0.1091,0.9851,0.4723,0.9857,0.2537,0.7717,0.3004,0.9379,0.0495,0.503,0.474,0.0864,0.9737,0.8255,0.0519,0.6348,0.7478,0.124,0.0461,0.1836,0.6274,0.7262,0.277,0.9116,0.3375,0.9942,0.7185,0.5553,0.0064,0.7055,0.116,0.0091,0.074,0.5383,0.9249,0.0431,0.2956,0.662,0.9022,0.5899,0.6465,0.4034,0.0065,0.2862,0.1429,0.71,0.6168,0.4734,0.336,0.6669,0.4605,0.6557,0.5158,0.6319,0.0583,0.4976,0.4607,0.6538,0.9396]]

Actual:   [[0.3767, 0.4602, 0.4323, 0.3051, 0.2269, 0.8256, 0.5344, 0.8053, 0.1767, 0.7814, 0.4296, 0.3898, 0.0242, 0.3443, 0.0728, 0.222, 0.2336, 0.1073, 0.7704, 0.1354, 0.9163, 0.0154, 0.4618, 0.3712, 0.3686, 0.8269, 0.5809, 0.8946, 0.9701, 0.0189, 0.3972, 0.356, 0.399, 0.6596, 0.6739, 0.8069, 0.8281, 0.1996, 0.1989, 0.3486, 0.7471, 0.0944, 0.2478, 0.9955, 0.326, 0.6683, 0.318, 0.169, 0.7806, 0.649, 0.5382, 0.4023, 0.0319, 0.361, 0.001, 0.0335, 0.0597, 0.653, 0.8938, 0.8901, 0.7431, 0.2496, 0.7085, 0.2301, 0.6543, 0.3715, 0.0004, 0.491, 0.2246, 0.145, 0.1369, 0.7225, 0.9383, 0.5355, 0.2247, 0.9296, 0.7254, 0.2838, 0.9405, 0.9371, 0.5236, 0.5952, 0.3008, 0.8005, 0.0573, 0.1406, 0.5011, 0.2374, 0.2429, 0.2634, 0.8332, 0.0274, 0.2713, 0.1556, 0.3434, 0.8393, 0.3406, 0.1644, 0.336, 0.4855, 0.2167, 0.3221, 0.0433, 0.1713, 0.2843, 0.6558, 0.7854, 0.9135, 0.5187, 0.8216, 0.7279, 0.9332, 0.8546, 0.139, 0.9033, 0.4948, 0.759, 0.1508, 0.5131, 0.358, 0.425, 0.4247, 0.6485, 0.7384, 0.8945, 0.43, 0.4778, 0.4302, 0.5912, 0.9701, 0.6363, 0.5355, 0.1996, 0.836, 0.0224, 0.0641, 0.56, 0.0545, 0.4593, 0.5235, 0.2674, 0.3494, 0.7075, 0.4977, 0.9589, 0.2091, 0.1325, 0.1348, 0.8545, 0.2581, 0.9233, 0.4034, 0.9752, 0.4675, 0.1351, 0.9594, 0.605, 0.1091, 0.9851, 0.4723, 0.9857, 0.2537, 0.7717, 0.3004, 0.9379, 0.0495, 0.503, 0.474, 0.0864, 0.9737, 0.8255, 0.0519, 0.6348, 0.7478, 0.124, 0.0461, 0.1836, 0.6274, 0.7262, 0.277, 0.9116, 0.3375, 0.9942, 0.7185, 0.5553, 0.0064, 0.7055, 0.116, 0.0091, 0.074, 0.5383, 0.9249, 0.0431, 0.2956, 0.662, 0.9022, 0.5899, 0.6465, 0.4034, 0.0065, 0.2862, 0.1429, 0.71, 0.6168, 0.4734, 0.336, 0.6669, 0.4605, 0.6557, 0.5158, 0.6319, 0.0583, 0.4976, 0.4607, 0.6538, 0.9396]]

Expected: [[0.3767, 0.4602, 0.4323, 0.3051, 0.2269, 0.8256, 0.5344, 0.8053, 0.1767, 0.7814, 0.4296, 0.3898, 0.0242, 0.3443, 0.0728, 0.222, 0.2336, 0.1073, 0.7704, 0.1354, 0.9163, 0.0154, 0.4618, 0.3712, 0.3686, 0.8269, 0.5809, 0.8946, 0.9701, 0.0189, 0.3972, 0.356, 0.399, 0.6596, 0.6739, 0.8069, 0.8281, 0.1996, 0.1989, 0.3486, 0.7471, 0.0944, 0.2478, 0.9955, 0.326, 0.6683, 0.318, 0.169, 0.7806, 0.649, 0.5382, 0.4023, 0.0319, 0.361, 0.001, 0.0335, 0.0597, 0.653, 0.8938, 0.8901, 0.7431, 0.2496, 0.7085, 0.2301, 0.6543, 0.3715, 0.0004, 0.491, 0.2246, 0.145, 0.1369, 0.7225, 0.9383, 0.5355, 0.2247, 0.9296, 0.7254, 0.2838, 0.9405, 0.9371, 0.5236, 0.5952, 0.3008, 0.8005, 0.0573, 0.1406, 0.5011, 0.2374, 0.2429, 0.2634, 0.8332, 0.0274, 0.2713, 0.1556, 0.3434, 0.8393, 0.3406, 0.1644, 0.336, 0.4855, 0.2167, 0.3221, 0.0433, 0.1713, 0.2843, 0.6558, 0.7854, 0.9135, 0.5187, 0.8216, 0.7279, 0.9332, 0.8546, 0.139, 0.9033, 0.4948, 0.759, 0.1508, 0.5131, 0.358, 0.425, 0.4247, 0.6485, 0.7384, 0.8945, 0.43, 0.4778, 0.4302, 0.5912, 0.9701, 0.6363, 0.5355, 0.1996, 0.836, 0.0224, 0.0641, 0.56, 0.0545, 0.4593, 0.5235, 0.2674, 0.3494, 0.7075, 0.4977, 0.9589, 0.2091, 0.1325, 0.1348, 0.8545, 0.2581, 0.9233, 0.4034, 0.9752, 0.4675, 0.1351, 0.9594, 0.605, 0.1091, 0.9851, 0.4723, 0.9857, 0.2537, 0.7717, 0.3004, 0.9379, 0.0495, 0.503, 0.474, 0.0864, 0.9737, 0.8255, 0.0519, 0.6348, 0.7478, 0.124, 0.0461, 0.1836, 0.6274, 0.7262, 0.277, 0.9116, 0.3375, 0.9942, 0.7185, 0.5553, 0.0064, 0.7055, 0.116, 0.0091, 0.074, 0.5383, 0.9249, 0.0431, 0.2956, 0.662, 0.9022, 0.5899, 0.6465, 0.4034, 0.0065, 0.2862, 0.1429, 0.71, 0.6168, 0.4734, 0.336, 0.6669, 0.4605, 0.6557, 0.5158, 0.6319, 0.0583, 0.4976, 0.4607, 0.6538, 0.9396]]