import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0ELU43839 = tf.keras.layers.Input(shape=([1, 1, 2]))
in0Lea5058 = tf.keras.layers.Input(shape=([1, 1, 2, 1]))
in0Con55346 = tf.keras.layers.Input(shape=([3, 1]))
in0Dot74263 = tf.keras.layers.Input(shape=([3, 2]))
in1Dot74263 = tf.keras.layers.Input(shape=([3, 2]))

ELU43839 = keras.layers.ELU(alpha=7.952401799039102, name = 'ELU43839', input_shape=(1, 1, 2))(in0ELU43839)
Res90457 = keras.layers.Reshape((1, 1, 2, 1), name = 'Res90457', )(ELU43839)
Lea5058 = keras.layers.LeakyReLU(alpha=5.228502955839586, name = 'Lea5058', input_shape=(1, 1, 2, 1))(in0Lea5058)
Mul28079 = keras.layers.Multiply(name = 'Mul28079', )([Res90457,Lea5058])
Res35388 = keras.layers.Reshape((1, 1, 2), name = 'Res35388', )(Mul28079)
Res15234 = keras.layers.Reshape((1, 2), name = 'Res15234', )(Res35388)
Zer79162 = keras.layers.ZeroPadding1D(padding=((2, 0)), name = 'Zer79162', )(Res15234)
Con55346 = keras.layers.Concatenate(axis=2, name = 'Con55346', )([Zer79162,in0Con55346])
Dot74263 = keras.layers.Dot(axes=(2, 2), name = 'Dot74263', )([in0Dot74263,in1Dot74263])
Sub72029 = keras.layers.Subtract(name = 'Sub72029', )([Con55346,Dot74263])
model = tf.keras.models.Model(inputs=[in0ELU43839,in0Lea5058,in0Con55346,in0Dot74263,in1Dot74263], outputs=Sub72029)
in0ELU43839 = tf.constant([[[[0.9253, 0.7652]]]])
in0Lea5058 = tf.constant([[[[[0.1466], [0.0694]]]]])
in0Con55346 = tf.constant([[[0.6138], [0.809], [0.2106]]])
in0Dot74263 = tf.constant([[[0.5202, 0.5325], [0.9405, 0.4702], [0.7507, 0.644]]])
in1Dot74263 = tf.constant([[[0.6882, 0.9827], [0.3899, 0.9483], [0.2921, 0.4335]]])
print (np.array2string(model.predict([in0ELU43839,in0Lea5058,in0Con55346,in0Dot74263,in1Dot74263],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Sub72029.png')

LELU43839 = elu_layer([[[[0.9253, 0.7652]]]], 7.952401799039102, ELU43839), 
LRes90457 = reshape_layer(ELU43839, [1, 1, 2, 1], Res90457), 
LLea5058 = leaky_relu_layer([[[[[0.1466], [0.0694]]]]], 5.228502955839586, Lea5058), 
LMul28079 = multiply_layer([Res90457,Lea5058], Mul28079), 
LRes35388 = reshape_layer(Mul28079, [1, 1, 2], Res35388), 
LRes15234 = reshape_layer(Res35388, [1, 2], Res15234), 
LZer79162 = zero_padding1D_layer(Res15234, 2, 0, Zer79162), 
LCon55346 = concatenate_layer([Zer79162,[[[0.6138], [0.809], [0.2106]]]], 2, Con55346), 
LDot74263 = dot_layer([[[0.5202, 0.5325], [0.9405, 0.4702], [0.7507, 0.644]]], [[[0.6882, 0.9827], [0.3899, 0.9483], [0.2921, 0.4335]]], 2, 2, Dot74263), 
LSub72029 = subtract_layer(Con55346,Dot74263, Sub72029), 
exec_layers([LELU43839,LRes90457,LLea5058,LMul28079,LRes35388,LRes15234,LZer79162,LCon55346,LDot74263,LSub72029],["ELU43839","Res90457","Lea5058","Mul28079","Res35388","Res15234","Zer79162","Con55346","Dot74263","Sub72029"],Sub72029,"Sub72029")

Actual (Unparsed): [[[-0.8812894, -0.7077958, 0.2310108], [-1.1093176, -0.8125916, 0.3304483], [-1.0138415, -0.8502982, -0.2878535]]]

Expected (Unparsed): [[[-0.88128939,-0.70779573,0.23101083],[-1.10931764,-0.81259161,0.33044825],[-1.01384156,-0.85029825,-0.28785347]]]

Actual:   [[[-0.8812, -0.7077, 0.2311], [-1.1093, -0.8125, 0.3305], [-1.0138, -0.8502, -0.2878]]]

Expected: [[[-0.8812, -0.7077, 0.2311], [-1.1093, -0.8125, 0.3305], [-1.0138, -0.8502, -0.2878]]]