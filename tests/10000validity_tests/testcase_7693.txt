import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Ave40819 = tf.keras.layers.Input(shape=([1, 2]))
in1Ave40819 = tf.keras.layers.Input(shape=([1, 2]))
in0Con10501 = tf.keras.layers.Input(shape=([3, 1]))
in0Zer25829 = tf.keras.layers.Input(shape=([1, 3]))

Ave40819 = keras.layers.Average(name = 'Ave40819', )([in0Ave40819,in1Ave40819])
ELU49538 = keras.layers.ELU(alpha=5.7012594787745385, name = 'ELU49538', )(Ave40819)
ReL11895 = keras.layers.ReLU(max_value=2.6875712204899, negative_slope=3.095710182766566, threshold=5.348134579682075, name = 'ReL11895', )(ELU49538)
Zer76410 = keras.layers.ZeroPadding1D(padding=((2, 0)), name = 'Zer76410', )(ReL11895)
Con10501 = keras.layers.Concatenate(axis=2, name = 'Con10501', )([Zer76410,in0Con10501])
Zer25829 = keras.layers.ZeroPadding1D(padding=((1, 1)), name = 'Zer25829', )(in0Zer25829)
Max7227 = keras.layers.Maximum(name = 'Max7227', )([Con10501,Zer25829])
Bat22532 = keras.layers.BatchNormalization(axis=1, epsilon=0.8004016018534506,  name = 'Bat22532', )(Max7227)
Res37587 = keras.layers.Reshape((3, 3, 1), name = 'Res37587', )(Bat22532)
Res30229 = keras.layers.Reshape((3, 3, 1, 1), name = 'Res30229', )(Res37587)
Up_31194 = keras.layers.UpSampling3D(size=(2, 2, 2), name = 'Up_31194', )(Res30229)
model = tf.keras.models.Model(inputs=[in0Ave40819,in1Ave40819,in0Con10501,in0Zer25829], outputs=Up_31194)
w = model.get_layer('Bat22532').get_weights() 
w[0] = np.array([0.1034, 0.0969, 0.6373])
w[1] = np.array([0.1287, 0.1576, 0.2409])
w[2] = np.array([0.1077, 0.596, 0.7453])
w[3] = np.array([0.0808, 0.7446, 0.7909])
model.get_layer('Bat22532').set_weights(w) 
in0Ave40819 = tf.constant([[[0.6893, 0.716]]])
in1Ave40819 = tf.constant([[[0.993, 0.33]]])
in0Con10501 = tf.constant([[[0.5777], [0.9698], [0.6332]]])
in0Zer25829 = tf.constant([[[1.2838, 1.2868, 1.8186]]])
print (np.array2string(model.predict([in0Ave40819,in1Ave40819,in0Con10501,in0Zer25829],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Up_31194.png')

LAve40819 = average_layer([[[[0.6893, 0.716]]], [[[0.993, 0.33]]]], Ave40819), 
LELU49538 = elu_layer(Ave40819, 5.7012594787745385, ELU49538), 
LReL11895 = relu_layer(ELU49538, 2.6875712204899, 3.095710182766566, 5.348134579682075, ReL11895), 
LZer76410 = zero_padding1D_layer(ReL11895, 2, 0, Zer76410), 
LCon10501 = concatenate_layer([Zer76410,[[[0.5777], [0.9698], [0.6332]]]], 2, Con10501), 
LZer25829 = zero_padding1D_layer([[[1.2838, 1.2868, 1.8186]]], 1, 1, Zer25829), 
LMax7227 = maximum_layer([Con10501,Zer25829], Max7227), 
LBat22532 = batch_normalization_layer(Max7227, 1, 0.8004016018534506, [0.1034, 0.0969, 0.6373], [0.1287, 0.1576, 0.2409], [0.1077, 0.596, 0.7453], [0.0808, 0.7446, 0.7909], Bat22532), 
LRes37587 = reshape_layer(Bat22532, [3, 3, 1], Res37587), 
LRes30229 = reshape_layer(Res37587, [3, 3, 1, 1], Res30229), 
LUp_31194 = up_sampling3D_layer(Res30229, 2, 2, 2, Up_31194), 
exec_layers([LAve40819,LELU49538,LReL11895,LZer76410,LCon10501,LZer25829,LMax7227,LBat22532,LRes37587,LRes30229,LUp_31194],["Ave40819","ELU49538","ReL11895","Zer76410","Con10501","Zer25829","Max7227","Bat22532","Res37587","Res30229","Up_31194"],Up_31194,"Up_31194")

Actual (Unparsed): [[[[[0.1168369], [0.1168369]], [[0.1168369], [0.1168369]], [[0.1168369], [0.1168369]], [[0.1168369], [0.1168369]], [[0.1804703], [0.1804703]], [[0.1804703], [0.1804703]]], [[[0.1168369], [0.1168369]], [[0.1168369], [0.1168369]], [[0.1168369], [0.1168369]], [[0.1168369], [0.1168369]], [[0.1804703], [0.1804703]], [[0.1804703], [0.1804703]]], [[[0.2112193], [0.2112193]], [[0.2112193], [0.2112193]], [[0.2114532], [0.2114532]], [[0.2114532], [0.2114532]], [[0.2529112], [0.2529112]], [[0.2529112], [0.2529112]]], [[[0.2112193], [0.2112193]], [[0.2112193], [0.2112193]], [[0.2114532], [0.2114532]], [[0.2114532], [0.2114532]], [[0.2529112], [0.2529112]], [[0.2529112], [0.2529112]]], [[[-0.1356293], [-0.1356293]], [[-0.1356293], [-0.1356293]], [[-0.1356293], [-0.1356293]], [[-0.1356293], [-0.1356293]], [[0.1842665], [0.1842665]], [[0.1842665], [0.1842665]]], [[[-0.1356293], [-0.1356293]], [[-0.1356293], [-0.1356293]], [[-0.1356293], [-0.1356293]], [[-0.1356293], [-0.1356293]], [[0.1842665], [0.1842665]], [[0.1842665], [0.1842665]]]]]

Expected (Unparsed): [[[[[0.11683688875183777],[0.11683688875183777]],[[0.11683688875183777],[0.11683688875183777]],[[0.11683688875183777],[0.11683688875183777]],[[0.11683688875183777],[0.11683688875183777]],[[0.18047030906811745],[0.18047030906811745]],[[0.18047030906811745],[0.18047030906811745]]],[[[0.11683688875183777],[0.11683688875183777]],[[0.11683688875183777],[0.11683688875183777]],[[0.11683688875183777],[0.11683688875183777]],[[0.11683688875183777],[0.11683688875183777]],[[0.18047030906811745],[0.18047030906811745]],[[0.18047030906811745],[0.18047030906811745]]],[[[0.21121934216873692],[0.21121934216873692]],[[0.21121934216873692],[0.21121934216873692]],[[0.21145321542623358],[0.21145321542623358]],[[0.21145321542623358],[0.21145321542623358]],[[0.2529111482051436],[0.2529111482051436]],[[0.2529111482051436],[0.2529111482051436]]],[[[0.21121934216873692],[0.21121934216873692]],[[0.21121934216873692],[0.21121934216873692]],[[0.21145321542623358],[0.21145321542623358]],[[0.21145321542623358],[0.21145321542623358]],[[0.2529111482051436],[0.2529111482051436]],[[0.2529111482051436],[0.2529111482051436]]],[[[-0.13562931112113397],[-0.13562931112113397]],[[-0.13562931112113397],[-0.13562931112113397]],[[-0.13562931112113397],[-0.13562931112113397]],[[-0.13562931112113397],[-0.13562931112113397]],[[0.18426651579675418],[0.18426651579675418]],[[0.18426651579675418],[0.18426651579675418]]],[[[-0.13562931112113397],[-0.13562931112113397]],[[-0.13562931112113397],[-0.13562931112113397]],[[-0.13562931112113397],[-0.13562931112113397]],[[-0.13562931112113397],[-0.13562931112113397]],[[0.18426651579675418],[0.18426651579675418]],[[0.18426651579675418],[0.18426651579675418]]]]]

Actual:   [[[[[0.1169], [0.1169]], [[0.1169], [0.1169]], [[0.1169], [0.1169]], [[0.1169], [0.1169]], [[0.1805], [0.1805]], [[0.1805], [0.1805]]], [[[0.1169], [0.1169]], [[0.1169], [0.1169]], [[0.1169], [0.1169]], [[0.1169], [0.1169]], [[0.1805], [0.1805]], [[0.1805], [0.1805]]], [[[0.2113], [0.2113]], [[0.2113], [0.2113]], [[0.2115], [0.2115]], [[0.2115], [0.2115]], [[0.253], [0.253]], [[0.253], [0.253]]], [[[0.2113], [0.2113]], [[0.2113], [0.2113]], [[0.2115], [0.2115]], [[0.2115], [0.2115]], [[0.253], [0.253]], [[0.253], [0.253]]], [[[-0.1356], [-0.1356]], [[-0.1356], [-0.1356]], [[-0.1356], [-0.1356]], [[-0.1356], [-0.1356]], [[0.1843], [0.1843]], [[0.1843], [0.1843]]], [[[-0.1356], [-0.1356]], [[-0.1356], [-0.1356]], [[-0.1356], [-0.1356]], [[-0.1356], [-0.1356]], [[0.1843], [0.1843]], [[0.1843], [0.1843]]]]]

Expected: [[[[[0.1169], [0.1169]], [[0.1169], [0.1169]], [[0.1169], [0.1169]], [[0.1169], [0.1169]], [[0.1805], [0.1805]], [[0.1805], [0.1805]]], [[[0.1169], [0.1169]], [[0.1169], [0.1169]], [[0.1169], [0.1169]], [[0.1169], [0.1169]], [[0.1805], [0.1805]], [[0.1805], [0.1805]]], [[[0.2113], [0.2113]], [[0.2113], [0.2113]], [[0.2115], [0.2115]], [[0.2115], [0.2115]], [[0.253], [0.253]], [[0.253], [0.253]]], [[[0.2113], [0.2113]], [[0.2113], [0.2113]], [[0.2115], [0.2115]], [[0.2115], [0.2115]], [[0.253], [0.253]], [[0.253], [0.253]]], [[[-0.1356], [-0.1356]], [[-0.1356], [-0.1356]], [[-0.1356], [-0.1356]], [[-0.1356], [-0.1356]], [[0.1843], [0.1843]], [[0.1843], [0.1843]]], [[[-0.1356], [-0.1356]], [[-0.1356], [-0.1356]], [[-0.1356], [-0.1356]], [[-0.1356], [-0.1356]], [[0.1843], [0.1843]], [[0.1843], [0.1843]]]]]