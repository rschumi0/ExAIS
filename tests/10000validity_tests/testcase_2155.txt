import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Max57843 = tf.keras.layers.Input(shape=([2, 1, 2]))
in0Con43291 = tf.keras.layers.Input(shape=([28, 1, 1]))
in0Glo98519 = tf.keras.layers.Input(shape=([2, 1, 1, 1]))
in0Con33576 = tf.keras.layers.Input(shape=([28, 1, 2]))
in0Fla62119 = tf.keras.layers.Input(shape=([4]))

Max57843 = keras.layers.MaxPool2D(pool_size=(2, 1), name = 'Max57843', )(in0Max57843)
Zer10856 = keras.layers.ZeroPadding2D(padding=((27, 0), (0, 0)), name = 'Zer10856', )(Max57843)
Con43291 = keras.layers.Concatenate(axis=3, name = 'Con43291', )([Zer10856,in0Con43291])
Glo98519 = keras.layers.GlobalMaxPool3D(name = 'Glo98519', )(in0Glo98519)
Res82352 = keras.layers.Reshape((1, 1), name = 'Res82352', )(Glo98519)
Res32729 = keras.layers.Reshape((1, 1, 1), name = 'Res32729', )(Res82352)
Zer37970 = keras.layers.ZeroPadding2D(padding=((27, 0), (0, 0)), name = 'Zer37970', )(Res32729)
Con33576 = keras.layers.Concatenate(axis=3, name = 'Con33576', )([Zer37970,in0Con33576])
Fla62119 = keras.layers.Flatten(name = 'Fla62119', )(in0Fla62119)
Res58176 = keras.layers.Reshape((4, 1), name = 'Res58176', )(Fla62119)
Res33682 = keras.layers.Reshape((4, 1, 1), name = 'Res33682', )(Res58176)
Con29551 = keras.layers.Conv2DTranspose(3, (3, 1),strides=(7, 1), padding='valid', name = 'Con29551', )(Res33682)
Ave99912 = keras.layers.Average(name = 'Ave99912', )([Con33576,Con29551])
Sub45657 = keras.layers.Subtract(name = 'Sub45657', )([Con43291,Ave99912])
Bat4627 = keras.layers.BatchNormalization(axis=1, epsilon=0.386778827763106,  name = 'Bat4627', )(Sub45657)
model = tf.keras.models.Model(inputs=[in0Max57843,in0Con43291,in0Glo98519,in0Con33576,in0Fla62119], outputs=Bat4627)
w = model.get_layer('Con29551').get_weights() 
w[0] = np.array([[[[0.0168], [0.55], [0.8345]]], [[[0.4413], [0.2718], [0.2296]]], [[[0.7519], [0.6702], [0.9376]]]])
w[1] = np.array([0, 0, 0])
model.get_layer('Con29551').set_weights(w) 
w = model.get_layer('Bat4627').get_weights() 
w[0] = np.array([0.527, 0.6475, 0.7647, 0.8808, 0.1766, 0.6864, 0.6475, 0.9954, 0.0597, 0.788, 0.7372, 0.9963, 0.633, 0.8623, 0.0054, 0.6483, 0.7459, 0.5101, 0.6458, 0.8213, 0.7196, 0.8417, 0.6543, 0.4785, 0.6828, 0.9513, 0.7581, 0.4886])
w[1] = np.array([0.1467, 0.1019, 0.1932, 0.9616, 0.4968, 0.6965, 0.4811, 0.9113, 0.9403, 0.5363, 0.6656, 0.0467, 0.4578, 0.7018, 0.4374, 0.4107, 0.4772, 0.6302, 0.195, 0.3982, 0.3149, 0.6093, 0.3692, 0.412, 0.9078, 0.9587, 0.2729, 0.0428])
w[2] = np.array([0.9997, 0.6286, 0.3294, 0.1965, 0.7385, 0.8083, 0.7786, 0.0356, 0.5911, 0.4107, 0.8643, 0.9472, 0.5899, 0.9719, 0.0653, 0.0082, 0.1858, 0.5904, 0.1481, 0.5776, 0.4036, 0.0198, 0.6932, 0.7126, 0.5698, 0.9226, 0.2725, 0.0815])
w[3] = np.array([0.6143, 0.5958, 0.6416, 0.923, 0.8349, 0.1882, 0.1521, 0.3907, 0.059, 0.5395, 0.0831, 0.3105, 0.3346, 0.2965, 0.7497, 0.9815, 0.6415, 0.8861, 0.0498, 0.1523, 0.6248, 0.2574, 0.5805, 0.7162, 0.0223, 0.045, 0.5765, 0.9901])
model.get_layer('Bat4627').set_weights(w) 
in0Max57843 = tf.constant([[[[1.8383, 1.5736]], [[1.6066, 1.7946]]]])
in0Con43291 = tf.constant([[[[0.5678]], [[0.3208]], [[0.0749]], [[0.4711]], [[0.1703]], [[0.4189]], [[0.7121]], [[0.5164]], [[0.2991]], [[0.5378]], [[0.9101]], [[0.2217]], [[0.1613]], [[0.9044]], [[0.6509]], [[0.8345]], [[0.1975]], [[0.1951]], [[0.4946]], [[0.9669]], [[0.9637]], [[0.4209]], [[0.2946]], [[0.9395]], [[0.1464]], [[0.8231]], [[0.7897]], [[0.8495]]]])
in0Glo98519 = tf.constant([[[[[1.5828]]], [[[1.3518]]]]])
in0Con33576 = tf.constant([[[[0.5629, 0.3008]], [[0.9299, 0.6964]], [[0.9935, 0.9972]], [[0.0663, 0.9988]], [[0.3373, 0.2311]], [[0.1463, 0.7586]], [[0.3866, 0.3971]], [[0.4215, 0.8049]], [[0.0026, 0.136]], [[0.5368, 0.5763]], [[0.056, 0.4923]], [[0.597, 0.4622]], [[0.6929, 0.6655]], [[0.9204, 0.9771]], [[0.1362, 0.3553]], [[0.185, 0.3637]], [[0.5795, 0.8082]], [[0.3529, 0.7822]], [[0.987, 0.9352]], [[0.8407, 0.4573]], [[0.7731, 0.2885]], [[0.8924, 0.0271]], [[0.4039, 0.2631]], [[0.0073, 0.8676]], [[0.934, 0.6164]], [[0.578, 0.3484]], [[0.3754, 0.2903]], [[0.5987, 0.819]]]])
in0Fla62119 = tf.constant([[1.5673, 1.5714, 1.56, 1.7137]])
print (np.array2string(model.predict([in0Max57843,in0Con43291,in0Glo98519,in0Con33576,in0Fla62119],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Bat4627.png')

LMax57843 = max_pool2D_layer([[[[1.8383, 1.5736]], [[1.6066, 1.7946]]]], 2, 1, Max57843), 
LZer10856 = zero_padding2D_layer(Max57843, 27, 0, 0, 0, Zer10856), 
LCon43291 = concatenate_layer([Zer10856,[[[[0.5678]], [[0.3208]], [[0.0749]], [[0.4711]], [[0.1703]], [[0.4189]], [[0.7121]], [[0.5164]], [[0.2991]], [[0.5378]], [[0.9101]], [[0.2217]], [[0.1613]], [[0.9044]], [[0.6509]], [[0.8345]], [[0.1975]], [[0.1951]], [[0.4946]], [[0.9669]], [[0.9637]], [[0.4209]], [[0.2946]], [[0.9395]], [[0.1464]], [[0.8231]], [[0.7897]], [[0.8495]]]]], 3, Con43291), 
LGlo98519 = global_max_pool3D_layer([[[[[1.5828]]], [[[1.3518]]]]], Glo98519), 
LRes82352 = reshape_layer(Glo98519, [1, 1], Res82352), 
LRes32729 = reshape_layer(Res82352, [1, 1, 1], Res32729), 
LZer37970 = zero_padding2D_layer(Res32729, 27, 0, 0, 0, Zer37970), 
LCon33576 = concatenate_layer([Zer37970,[[[[0.5629, 0.3008]], [[0.9299, 0.6964]], [[0.9935, 0.9972]], [[0.0663, 0.9988]], [[0.3373, 0.2311]], [[0.1463, 0.7586]], [[0.3866, 0.3971]], [[0.4215, 0.8049]], [[0.0026, 0.136]], [[0.5368, 0.5763]], [[0.056, 0.4923]], [[0.597, 0.4622]], [[0.6929, 0.6655]], [[0.9204, 0.9771]], [[0.1362, 0.3553]], [[0.185, 0.3637]], [[0.5795, 0.8082]], [[0.3529, 0.7822]], [[0.987, 0.9352]], [[0.8407, 0.4573]], [[0.7731, 0.2885]], [[0.8924, 0.0271]], [[0.4039, 0.2631]], [[0.0073, 0.8676]], [[0.934, 0.6164]], [[0.578, 0.3484]], [[0.3754, 0.2903]], [[0.5987, 0.819]]]]], 3, Con33576), 
LFla62119 = flatten_layer([[1.5673, 1.5714, 1.56, 1.7137]], Fla62119), 
LRes58176 = reshape_layer(Fla62119, [4, 1], Res58176), 
LRes33682 = reshape_layer(Res58176, [4, 1, 1], Res33682), 
LCon29551 = conv2D_transpose_layer(Res33682, 3, 1,[[[[0.0168], [0.55], [0.8345]]], [[[0.4413], [0.2718], [0.2296]]], [[[0.7519], [0.6702], [0.9376]]]],[0, 0, 0], 7, 1, false, Con29551), 
LAve99912 = average_layer([Con33576,Con29551], Ave99912), 
LSub45657 = subtract_layer(Con43291,Ave99912, Sub45657), 
LBat4627 = batch_normalization_layer(Sub45657, 1, 0.386778827763106, [0.527, 0.6475, 0.7647, 0.8808, 0.1766, 0.6864, 0.6475, 0.9954, 0.0597, 0.788, 0.7372, 0.9963, 0.633, 0.8623, 0.0054, 0.6483, 0.7459, 0.5101, 0.6458, 0.8213, 0.7196, 0.8417, 0.6543, 0.4785, 0.6828, 0.9513, 0.7581, 0.4886], [0.1467, 0.1019, 0.1932, 0.9616, 0.4968, 0.6965, 0.4811, 0.9113, 0.9403, 0.5363, 0.6656, 0.0467, 0.4578, 0.7018, 0.4374, 0.4107, 0.4772, 0.6302, 0.195, 0.3982, 0.3149, 0.6093, 0.3692, 0.412, 0.9078, 0.9587, 0.2729, 0.0428], [0.9997, 0.6286, 0.3294, 0.1965, 0.7385, 0.8083, 0.7786, 0.0356, 0.5911, 0.4107, 0.8643, 0.9472, 0.5899, 0.9719, 0.0653, 0.0082, 0.1858, 0.5904, 0.1481, 0.5776, 0.4036, 0.0198, 0.6932, 0.7126, 0.5698, 0.9226, 0.2725, 0.0815], [0.6143, 0.5958, 0.6416, 0.923, 0.8349, 0.1882, 0.1521, 0.3907, 0.059, 0.5395, 0.0831, 0.3105, 0.3346, 0.2965, 0.7497, 0.9815, 0.6415, 0.8861, 0.0498, 0.1523, 0.6248, 0.2574, 0.5805, 0.7162, 0.0223, 0.045, 0.5765, 0.9901], Bat4627), 
exec_layers([LMax57843,LZer10856,LCon43291,LGlo98519,LRes82352,LRes32729,LZer37970,LCon33576,LFla62119,LRes58176,LRes33682,LCon29551,LAve99912,LSub45657,LBat4627],["Max57843","Zer10856","Con43291","Glo98519","Res82352","Res32729","Zer37970","Con33576","Fla62119","Res58176","Res33682","Con29551","Ave99912","Sub45657","Bat4627"],Bat4627,"Bat4627")

Actual (Unparsed): [[[[-0.3867923, -0.7551207, -0.5044557]], [[-0.5346087, -0.7515553, -0.4441393]], [[-0.4995132, -0.8258209, -0.9287506]], [[0.8103690, 0.7848560, 0.7885886]], [[0.3788053, 0.3518590, 0.3875530]], [[-0.0351846, -0.1014010, 0.0006619]], [[-0.2056657, -0.3761664, 0.2473121]], [[0.8562102, 0.1453620, 0.2595704]], [[0.8564432, 0.8682350, 0.8919800]], [[-0.2836594, -0.4508565, -0.1987170]], [[-0.2639161, -0.2940288, 0.4501325]], [[-1.0834305, -1.4395790, -1.0946459]], [[0.0181567, -0.2400470, -0.1096222]], [[-0.3120670, -0.7921386, 0.1217395]], [[0.4370029, 0.4345512, 0.4361693]], [[0.2153822, 0.2373906, 0.6686168]], [[-0.0908690, -0.2571259, -0.3493828]], [[0.3632634, 0.2834853, 0.2746462]], [[0.0502489, -0.4320917, 0.0766384]], [[-0.2479047, -0.7181091, 0.5779034]], [[0.0261364, -0.2504284, 0.6124281]], [[0.5734394, -0.3736189, 0.2658576]], [[-0.3435275, -0.3812578, -0.1143768]], [[-0.2062088, -0.1759766, -0.0483003]], [[0.2995075, -0.1990404, 0.1267772]], [[-0.3769734, -0.7953667, 0.5624569]], [[0.0624170, -0.0825652, 0.5602768]], [[0.4447878, 0.6314786, 0.1920776]]]]

Expected (Unparsed): [[[[-0.386792328045133,-0.7551206790365128,-0.5044557247194912]],[[-0.534608746627197,-0.7515553393616764,-0.4441393311270396]],[[-0.4995132034595535,-0.8258208953060573,-0.9287506284058391]],[[0.810368981254155,0.7848559620611537,0.788588636060733]],[[0.37880525252015235,0.3518589909595886,0.38755300123311326]],[[-0.03518462137410594,-0.10140103861215599,0.0006618601382218925]],[[-0.20566572410294998,-0.37616638486470216,0.24731210483754573]],[[0.8562102201387539,0.14536202634690065,0.2595704185690241]],[[0.8564431662935595,0.8682349871326988,0.8919799565023816]],[[-0.28365940548844526,-0.4508565130214952,-0.19871697982740777]],[[-0.2639160672940686,-0.29402881736260256,0.45013251870604354]],[[-1.0834304600232225,-1.4395790477733614,-1.0946458594364596]],[[0.018156749355427493,-0.24004702109009235,-0.10962225610823062]],[[-0.3120669970794119,-0.792138601211468,0.12173955784956592]],[[0.4370028529827632,0.43455122337515956,0.4361693122888136]],[[0.215382178799513,0.23739058561613022,0.6686168021403833]],[[-0.09086897745724148,-0.2571258921086194,-0.34938279734613925]],[[0.36326338989094803,0.2834853328893522,0.27464622257832244]],[[0.05024893624210855,-0.43209171172898814,0.07663839418581594]],[[-0.24790465457102728,-0.7181091066986783,0.5779034500637734]],[[0.026136400516879876,-0.2504284393745971,0.6124281041750632]],[[0.5734393854471994,-0.3736188453533953,0.2658576049182827]],[[-0.34352746806628465,-0.3812578137248822,-0.11437678979325466]],[[-0.20620876615003897,-0.17597657087328472,-0.048300249343799284]],[[0.2995075082177773,-0.19904039220745628,0.12677720781348867]],[[-0.3769733953604574,-0.795366644069727,0.5624569821047506]],[[0.06241701419121279,-0.08256521126313354,0.5602768619088413]],[[0.44478782165283964,0.6314785610748933,0.19207764042111355]]]]

Actual:   [[[[-0.3867, -0.7551, -0.5044]], [[-0.5346, -0.7515, -0.4441]], [[-0.4995, -0.8258, -0.9287]], [[0.8104, 0.7849, 0.7886]], [[0.3789, 0.3519, 0.3876]], [[-0.0351, -0.1014, 0.0007]], [[-0.2056, -0.3761, 0.2474]], [[0.8563, 0.1454, 0.2596]], [[0.8565, 0.8683, 0.892]], [[-0.2836, -0.4508, -0.1987]], [[-0.2639, -0.294, 0.4502]], [[-1.0834, -1.4395, -1.0946]], [[0.0182, -0.24, -0.1096]], [[-0.312, -0.7921, 0.1218]], [[0.4371, 0.4346, 0.4362]], [[0.2154, 0.2374, 0.6687]], [[-0.0908, -0.2571, -0.3493]], [[0.3633, 0.2835, 0.2747]], [[0.0503, -0.432, 0.0767]], [[-0.2479, -0.7181, 0.578]], [[0.0262, -0.2504, 0.6125]], [[0.5735, -0.3736, 0.2659]], [[-0.3435, -0.3812, -0.1143]], [[-0.2062, -0.1759, -0.0483]], [[0.2996, -0.199, 0.1268]], [[-0.3769, -0.7953, 0.5625]], [[0.0625, -0.0825, 0.5603]], [[0.4448, 0.6315, 0.1921]]]]

Expected: [[[[-0.3867, -0.7551, -0.5044]], [[-0.5346, -0.7515, -0.4441]], [[-0.4995, -0.8258, -0.9287]], [[0.8104, 0.7849, 0.7886]], [[0.3789, 0.3519, 0.3876]], [[-0.0351, -0.1014, 0.0007]], [[-0.2056, -0.3761, 0.2474]], [[0.8563, 0.1454, 0.2596]], [[0.8565, 0.8683, 0.892]], [[-0.2836, -0.4508, -0.1987]], [[-0.2639, -0.294, 0.4502]], [[-1.0834, -1.4395, -1.0946]], [[0.0182, -0.24, -0.1096]], [[-0.312, -0.7921, 0.1218]], [[0.4371, 0.4346, 0.4362]], [[0.2154, 0.2374, 0.6687]], [[-0.0908, -0.2571, -0.3493]], [[0.3633, 0.2835, 0.2747]], [[0.0503, -0.432, 0.0767]], [[-0.2479, -0.7181, 0.578]], [[0.0262, -0.2504, 0.6125]], [[0.5735, -0.3736, 0.2659]], [[-0.3435, -0.3812, -0.1143]], [[-0.2062, -0.1759, -0.0483]], [[0.2996, -0.199, 0.1268]], [[-0.3769, -0.7953, 0.5625]], [[0.0625, -0.0825, 0.5603]], [[0.4448, 0.6315, 0.1921]]]]