import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0ELU44236 = tf.keras.layers.Input(shape=([2, 2]))
in0Con86626 = tf.keras.layers.Input(shape=([3, 1]))
in0Dot93667 = tf.keras.layers.Input(shape=([3, 3]))
in1Dot93667 = tf.keras.layers.Input(shape=([3, 3]))

ELU44236 = keras.layers.ELU(alpha=8.039659916501169, name = 'ELU44236', input_shape=(2, 2))(in0ELU44236)
Zer1690 = keras.layers.ZeroPadding1D(padding=((1, 0)), name = 'Zer1690', )(ELU44236)
Con86626 = keras.layers.Concatenate(axis=2, name = 'Con86626', )([Zer1690,in0Con86626])
Dot93667 = keras.layers.Dot(axes=(1, 1), name = 'Dot93667', )([in0Dot93667,in1Dot93667])
Mul43323 = keras.layers.Multiply(name = 'Mul43323', )([Con86626,Dot93667])
GRU55165 = keras.layers.GRU(2,reset_after=True, recurrent_activation='sigmoid', name = 'GRU55165', )(Mul43323)
Lay87929 = keras.layers.LayerNormalization(axis=1, epsilon=2.402497551782335, name = 'Lay87929', )(GRU55165)
model = tf.keras.models.Model(inputs=[in0ELU44236,in0Con86626,in0Dot93667,in1Dot93667], outputs=Lay87929)
w = model.get_layer('GRU55165').get_weights() 
w[0] = np.array([[9, 3, 5, 2, 1, 3], [3, 1, 2, 3, 7, 10], [10, 3, 9, 1, 5, 8]])
w[1] = np.array([[6, 5, 3, 6, 6, 8], [1, 10, 8, 5, 10, 1]])
w[2] = np.array([[5, 5, 4, 6, 1, 8], [9, 6, 7, 4, 7, 2]])
model.get_layer('GRU55165').set_weights(w) 
in0ELU44236 = tf.constant([[[0.2231, 0.3912], [0.976, 0.2384]]])
in0Con86626 = tf.constant([[[0.1398], [0.6153], [0.1938]]])
in0Dot93667 = tf.constant([[[0.3936, 0.4359, 0.8759], [0.0314, 0.4112, 0.6294], [0.4374, 0.6398, 0.5454]]])
in1Dot93667 = tf.constant([[[0.341, 0.5692, 0.911], [0.8667, 0.7836, 0.8618], [0.9715, 0.154, 0.8593]]])
print (np.array2string(model.predict([in0ELU44236,in0Con86626,in0Dot93667,in1Dot93667],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Lay87929.png')

LELU44236 = elu_layer([[[0.2231, 0.3912], [0.976, 0.2384]]], 8.039659916501169, ELU44236), 
LZer1690 = zero_padding1D_layer(ELU44236, 1, 0, Zer1690), 
LCon86626 = concatenate_layer([Zer1690,[[[0.1398], [0.6153], [0.1938]]]], 2, Con86626), 
LDot93667 = dot_layer([[[0.3936, 0.4359, 0.8759], [0.0314, 0.4112, 0.6294], [0.4374, 0.6398, 0.5454]]], [[[0.341, 0.5692, 0.911], [0.8667, 0.7836, 0.8618], [0.9715, 0.154, 0.8593]]], 1, 1, Dot93667), 
LMul43323 = multiply_layer([Con86626,Dot93667], Mul43323), 
LGRU55165 = gru_layer(Mul43323,[[9, 3, 5, 2, 1, 3], [3, 1, 2, 3, 7, 10], [10, 3, 9, 1, 5, 8]],[[6, 5, 3, 6, 6, 8], [1, 10, 8, 5, 10, 1]],[[5, 5, 4, 6, 1, 8], [9, 6, 7, 4, 7, 2]], true, GRU55165), 
LLay87929 = layer_normalization_layer(GRU55165, 1, 2.402497551782335, Lay87929), 
exec_layers([LELU44236,LZer1690,LCon86626,LDot93667,LMul43323,LGRU55165,LLay87929],["ELU44236","Zer1690","Con86626","Dot93667","Mul43323","GRU55165","Lay87929"],Lay87929,"Lay87929")

Actual (Unparsed): [[-0.0000040, 0.0000040]]

Expected (Unparsed): [[-4.024834548181233e-6,4.024834548181233e-6]]

Actual:   [[-0, 0]]

Expected: [[-0, 0]]