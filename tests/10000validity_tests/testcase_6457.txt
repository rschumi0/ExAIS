import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Zer30009 = tf.keras.layers.Input(shape=([3, 1, 4, 1]))

Zer30009 = keras.layers.ZeroPadding3D(padding=((1, 1), (1, 1), (1, 1)), name = 'Zer30009', )(in0Zer30009)
Bat17070 = keras.layers.BatchNormalization(axis=2, epsilon=0.8774213393033855,  name = 'Bat17070', )(Zer30009)
PRe2444 = keras.layers.PReLU(name = 'PRe2444', )(Bat17070)
Bat70936 = keras.layers.BatchNormalization(axis=3, epsilon=0.1284820120290777,  name = 'Bat70936', )(PRe2444)
model = tf.keras.models.Model(inputs=[in0Zer30009], outputs=Bat70936)
w = model.get_layer('Bat17070').get_weights() 
w[0] = np.array([0.8999, 0.8901, 0.3922])
w[1] = np.array([0.4531, 0.8586, 0.5273])
w[2] = np.array([0.4271, 0.625, 0.4455])
w[3] = np.array([0.8575, 0.8584, 0.8181])
model.get_layer('Bat17070').set_weights(w) 
w = model.get_layer('PRe2444').get_weights() 
w[0] = np.array([[[[0.2159], [0.8297], [0.3663], [0.98], [0.0631], [0.3209]], [[0.1259], [0.5183], [0.377], [0.1614], [0.0277], [0.2898]], [[0.2365], [0.5601], [0.1902], [0.7995], [0.7639], [0.5944]]], [[[0.8718], [0.5242], [0.3021], [0.8129], [0.6909], [0.5511]], [[0.388], [0.4157], [0.1393], [0.5249], [0.5915], [0.919]], [[0.7358], [0.2378], [0.1633], [0.6952], [0.8071], [0.06]]], [[[0.0951], [0.2806], [0.8093], [0.8128], [0.6316], [0.8897]], [[0.3015], [0.6121], [0.4937], [0.1421], [0.1753], [0.6777]], [[0.0752], [0.612], [0.505], [0.0898], [0.6557], [0.4443]]], [[[0.4339], [0.0068], [0.2361], [0.9455], [0.8412], [0.9058]], [[0.9486], [0.7556], [0.1429], [0.0015], [0.1182], [0.9315]], [[0.473], [0.1117], [0.9609], [0.0356], [0.664], [0.891]]], [[[0.5916], [0.1091], [0.899], [0.5317], [0.4835], [0.2336]], [[0.6439], [0.7465], [0.7748], [0.2361], [0.4843], [0.0861]], [[0.8469], [0.9763], [0.7901], [0.4247], [0.4148], [0.5701]]]])
model.get_layer('PRe2444').set_weights(w) 
w = model.get_layer('Bat70936').get_weights() 
w[0] = np.array([0.4794, 0.9182, 0.9692, 0.4293, 0.2763, 0.9395])
w[1] = np.array([0.1084, 0.838, 0.0742, 0.8845, 0.6087, 0.5127])
w[2] = np.array([0.1098, 0.8608, 0.779, 0.6496, 0.1518, 0.1681])
w[3] = np.array([0.2048, 0.2228, 0.3214, 0.82, 0.7263, 0.8825])
model.get_layer('Bat70936').set_weights(w) 
in0Zer30009 = tf.constant([[[[[1.878], [1.3547], [1.7157], [1.0101]]], [[[1.4096], [1.5201], [1.9328], [1.1839]]], [[[1.4363], [1.7331], [1.4653], [1.4879]]]]])
print (np.array2string(model.predict([in0Zer30009],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Bat70936.png')

LZer30009 = zero_padding3D_layer([[[[[1.878], [1.3547], [1.7157], [1.0101]]], [[[1.4096], [1.5201], [1.9328], [1.1839]]], [[[1.4363], [1.7331], [1.4653], [1.4879]]]]], 1, 1, 1, 1, 1, 1, Zer30009), 
LBat17070 = batch_normalization_layer(Zer30009, 2, 0.8774213393033855, [0.8999, 0.8901, 0.3922], [0.4531, 0.8586, 0.5273], [0.4271, 0.625, 0.4455], [0.8575, 0.8584, 0.8181], Bat17070), 
LPRe2444 = prelu_layer(Bat17070, [[[[0.2159], [0.8297], [0.3663], [0.98], [0.0631], [0.3209]], [[0.1259], [0.5183], [0.377], [0.1614], [0.0277], [0.2898]], [[0.2365], [0.5601], [0.1902], [0.7995], [0.7639], [0.5944]]], [[[0.8718], [0.5242], [0.3021], [0.8129], [0.6909], [0.5511]], [[0.388], [0.4157], [0.1393], [0.5249], [0.5915], [0.919]], [[0.7358], [0.2378], [0.1633], [0.6952], [0.8071], [0.06]]], [[[0.0951], [0.2806], [0.8093], [0.8128], [0.6316], [0.8897]], [[0.3015], [0.6121], [0.4937], [0.1421], [0.1753], [0.6777]], [[0.0752], [0.612], [0.505], [0.0898], [0.6557], [0.4443]]], [[[0.4339], [0.0068], [0.2361], [0.9455], [0.8412], [0.9058]], [[0.9486], [0.7556], [0.1429], [0.0015], [0.1182], [0.9315]], [[0.473], [0.1117], [0.9609], [0.0356], [0.664], [0.891]]], [[[0.5916], [0.1091], [0.899], [0.5317], [0.4835], [0.2336]], [[0.6439], [0.7465], [0.7748], [0.2361], [0.4843], [0.0861]], [[0.8469], [0.9763], [0.7901], [0.4247], [0.4148], [0.5701]]]], PRe2444), 
LBat70936 = batch_normalization_layer(PRe2444, 3, 0.1284820120290777, [0.4794, 0.9182, 0.9692, 0.4293, 0.2763, 0.9395], [0.1084, 0.838, 0.0742, 0.8845, 0.6087, 0.5127], [0.1098, 0.8608, 0.779, 0.6496, 0.1518, 0.1681], [0.2048, 0.2228, 0.3214, 0.82, 0.7263, 0.8825], Bat70936), 
exec_layers([LZer30009,LBat17070,LPRe2444,LBat70936],["Zer30009","Bat17070","PRe2444","Bat70936"],Bat70936,"Bat70936")

Actual (Unparsed): [[[[[0.1511668], [-0.2456686], [-0.8183675], [0.6692554], [0.6115393], [0.5063470]], [[0.3795728], [0.1804443], [-0.4209203], [0.7904998], [0.6937387], [0.7633514]], [[0.3436674], [0.1134595], [-0.4833989], [0.7714402], [0.6808170], [0.7229503]]], [[[0.1511668], [-0.2456686], [-0.8183675], [0.6692554], [0.6115393], [0.5063470]], [[0.3795728], [2.1460265], [0.9015731], [1.3014446], [0.8976794], [0.7633514]], [[0.3436674], [0.1134595], [-0.4833989], [0.7714402], [0.6808170], [0.7229503]]], [[[0.1511668], [-0.2456686], [-0.8183675], [0.6692554], [0.6115393], [0.5063470]], [[0.3795728], [1.6557823], [1.0630409], [1.3660982], [0.9327699], [0.7633514]], [[0.3436674], [0.1134595], [-0.4833989], [0.7714402], [0.6808170], [0.7229503]]], [[[0.1511668], [-0.2456686], [-0.8183675], [0.6692554], [0.6115393], [0.5063470]], [[0.3795728], [1.6837275], [1.2709771], [1.2268741], [0.9941479], [0.7633514]], [[0.3436674], [0.1134595], [-0.4833989], [0.7714402], [0.6808170], [0.7229503]]], [[[0.1511668], [-0.2456686], [-0.8183675], [0.6692554], [0.6115393], [0.5063470]], [[0.3795728], [0.1804443], [-0.4209203], [0.7904998], [0.6937387], [0.7633514]], [[0.3436674], [0.1134595], [-0.4833989], [0.7714402], [0.6808170], [0.7229503]]]]]

Expected (Unparsed): [[[[[0.15116682603112686],[-0.2456685877053263],[-0.8183674613762332],[0.6692554066590589],[0.6115393474175725],[0.5063470465403215]],[[0.37957279290746626],[0.18044431553460194],[-0.4209202594669346],[0.7904998075278636],[0.6937387290613184],[0.7633513662446726]],[[0.34366742793492866],[0.11345946605989188],[-0.4833988673467075],[0.7714402158560401],[0.6808170053480604],[0.7229503468461755]]],[[[0.15116682603112686],[-0.2456685877053263],[-0.8183674613762332],[0.6692554066590589],[0.6115393474175725],[0.5063470465403215]],[[0.37957279290746626],[2.146026513173413],[0.9015731345881248],[1.301444603060509],[0.897679406867983],[0.7633513662446726]],[[0.34366742793492866],[0.11345946605989188],[-0.4833988673467075],[0.7714402158560401],[0.6808170053480604],[0.7229503468461755]]],[[[0.15116682603112686],[-0.2456685877053263],[-0.8183674613762332],[0.6692554066590589],[0.6115393474175725],[0.5063470465403215]],[[0.37957279290746626],[1.655782263240496],[1.0630409188774188],[1.3660981643533558],[0.9327698828632294],[0.7633513662446726]],[[0.34366742793492866],[0.11345946605989188],[-0.4833988673467075],[0.7714402158560401],[0.6808170053480604],[0.7229503468461755]]],[[[0.15116682603112686],[-0.2456685877053263],[-0.8183674613762332],[0.6692554066590589],[0.6115393474175725],[0.5063470465403215]],[[0.37957279290746626],[1.6837274414497374],[1.2709770618860021],[1.2268741205744247],[0.9941479306339709],[0.7633513662446726]],[[0.34366742793492866],[0.11345946605989188],[-0.4833988673467075],[0.7714402158560401],[0.6808170053480604],[0.7229503468461755]]],[[[0.15116682603112686],[-0.2456685877053263],[-0.8183674613762332],[0.6692554066590589],[0.6115393474175725],[0.5063470465403215]],[[0.37957279290746626],[0.18044431553460194],[-0.4209202594669346],[0.7904998075278636],[0.6937387290613184],[0.7633513662446726]],[[0.34366742793492866],[0.11345946605989188],[-0.4833988673467075],[0.7714402158560401],[0.6808170053480604],[0.7229503468461755]]]]]

Actual:   [[[[[0.1512], [-0.2456], [-0.8183], [0.6693], [0.6116], [0.5064]], [[0.3796], [0.1805], [-0.4209], [0.7905], [0.6938], [0.7634]], [[0.3437], [0.1135], [-0.4833], [0.7715], [0.6809], [0.723]]], [[[0.1512], [-0.2456], [-0.8183], [0.6693], [0.6116], [0.5064]], [[0.3796], [2.1461], [0.9016], [1.3015], [0.8977], [0.7634]], [[0.3437], [0.1135], [-0.4833], [0.7715], [0.6809], [0.723]]], [[[0.1512], [-0.2456], [-0.8183], [0.6693], [0.6116], [0.5064]], [[0.3796], [1.6558], [1.0631], [1.3661], [0.9328], [0.7634]], [[0.3437], [0.1135], [-0.4833], [0.7715], [0.6809], [0.723]]], [[[0.1512], [-0.2456], [-0.8183], [0.6693], [0.6116], [0.5064]], [[0.3796], [1.6838], [1.271], [1.2269], [0.9942], [0.7634]], [[0.3437], [0.1135], [-0.4833], [0.7715], [0.6809], [0.723]]], [[[0.1512], [-0.2456], [-0.8183], [0.6693], [0.6116], [0.5064]], [[0.3796], [0.1805], [-0.4209], [0.7905], [0.6938], [0.7634]], [[0.3437], [0.1135], [-0.4833], [0.7715], [0.6809], [0.723]]]]]

Expected: [[[[[0.1512], [-0.2456], [-0.8183], [0.6693], [0.6116], [0.5064]], [[0.3796], [0.1805], [-0.4209], [0.7905], [0.6938], [0.7634]], [[0.3437], [0.1135], [-0.4833], [0.7715], [0.6809], [0.723]]], [[[0.1512], [-0.2456], [-0.8183], [0.6693], [0.6116], [0.5064]], [[0.3796], [2.1461], [0.9016], [1.3015], [0.8977], [0.7634]], [[0.3437], [0.1135], [-0.4833], [0.7715], [0.6809], [0.723]]], [[[0.1512], [-0.2456], [-0.8183], [0.6693], [0.6116], [0.5064]], [[0.3796], [1.6558], [1.0631], [1.3661], [0.9328], [0.7634]], [[0.3437], [0.1135], [-0.4833], [0.7715], [0.6809], [0.723]]], [[[0.1512], [-0.2456], [-0.8183], [0.6693], [0.6116], [0.5064]], [[0.3796], [1.6838], [1.271], [1.2269], [0.9942], [0.7634]], [[0.3437], [0.1135], [-0.4833], [0.7715], [0.6809], [0.723]]], [[[0.1512], [-0.2456], [-0.8183], [0.6693], [0.6116], [0.5064]], [[0.3796], [0.1805], [-0.4209], [0.7905], [0.6938], [0.7634]], [[0.3437], [0.1135], [-0.4833], [0.7715], [0.6809], [0.723]]]]]