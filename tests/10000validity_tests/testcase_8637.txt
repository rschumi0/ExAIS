import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Max6407 = tf.keras.layers.Input(shape=([1, 1, 1]))
in1Max6407 = tf.keras.layers.Input(shape=([1, 1, 1]))
in0Con32337 = tf.keras.layers.Input(shape=([1, 2, 1]))
in0Mul95566 = tf.keras.layers.Input(shape=([1, 2, 2]))
in1Mul95566 = tf.keras.layers.Input(shape=([1, 2, 2]))
in0Sub29439 = tf.keras.layers.Input(shape=([3]))
in1Sub29439 = tf.keras.layers.Input(shape=([3]))
in0Con22456 = tf.keras.layers.Input(shape=([1]))

Max6407 = keras.layers.Maximum(name = 'Max6407', )([in0Max6407,in1Max6407])
Max68003 = keras.layers.MaxPool2D(pool_size=(1, 1), strides=(1, 6), padding='same', name = 'Max68003', )(Max6407)
Zer40905 = keras.layers.ZeroPadding2D(padding=((0, 0), (1, 0)), name = 'Zer40905', )(Max68003)
Con32337 = keras.layers.Concatenate(axis=3, name = 'Con32337', )([Zer40905,in0Con32337])
Mul95566 = keras.layers.Multiply(name = 'Mul95566', )([in0Mul95566,in1Mul95566])
Bat48579 = keras.layers.BatchNormalization(axis=1, epsilon=0.5203611414665702,  name = 'Bat48579', )(Mul95566)
Mul84829 = keras.layers.Multiply(name = 'Mul84829', )([Con32337,Bat48579])
Res36343 = keras.layers.Reshape((1, 4), name = 'Res36343', )(Mul84829)
Fla34285 = keras.layers.Flatten(name = 'Fla34285', )(Res36343)
Sub29439 = keras.layers.Subtract(name = 'Sub29439', )([in0Sub29439,in1Sub29439])
Con22456 = keras.layers.Concatenate(axis=1, name = 'Con22456', )([Sub29439,in0Con22456])
Max67864 = keras.layers.Maximum(name = 'Max67864', )([Fla34285,Con22456])
model = tf.keras.models.Model(inputs=[in0Max6407,in1Max6407,in0Con32337,in0Mul95566,in1Mul95566,in0Sub29439,in1Sub29439,in0Con22456], outputs=Max67864)
w = model.get_layer('Bat48579').get_weights() 
w[0] = np.array([0.7145])
w[1] = np.array([0.3878])
w[2] = np.array([0.761])
w[3] = np.array([0.8599])
model.get_layer('Bat48579').set_weights(w) 
in0Max6407 = tf.constant([[[[0.6796]]]])
in1Max6407 = tf.constant([[[[0.3607]]]])
in0Con32337 = tf.constant([[[[0.7031], [0.1583]]]])
in0Mul95566 = tf.constant([[[[0.3324, 0.9393], [0.8176, 0.7198]]]])
in1Mul95566 = tf.constant([[[[0.5271, 0.4328], [0.1912, 0.4022]]]])
in0Sub29439 = tf.constant([[0.2698, 0.7799, 0.8899]])
in1Sub29439 = tf.constant([[0.6088, 0.1158, 0.8729]])
in0Con22456 = tf.constant([[0.0044]])
print (np.array2string(model.predict([in0Max6407,in1Max6407,in0Con32337,in0Mul95566,in1Mul95566,in0Sub29439,in1Sub29439,in0Con22456],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Max67864.png')

LMax6407 = maximum_layer([[[[[0.6796]]]], [[[[0.3607]]]]], Max6407), 
LMax68003 = max_pool2D_layer(Max6407, 1, 1, 1, 6, true, Max68003), 
LZer40905 = zero_padding2D_layer(Max68003, 0, 0, 1, 0, Zer40905), 
LCon32337 = concatenate_layer([Zer40905,[[[[0.7031], [0.1583]]]]], 3, Con32337), 
LMul95566 = multiply_layer([[[[[0.3324, 0.9393], [0.8176, 0.7198]]]], [[[[0.5271, 0.4328], [0.1912, 0.4022]]]]], Mul95566), 
LBat48579 = batch_normalization_layer(Mul95566, 1, 0.5203611414665702, [0.7145], [0.3878], [0.761], [0.8599], Bat48579), 
LMul84829 = multiply_layer([Con32337,Bat48579], Mul84829), 
LRes36343 = reshape_layer(Mul84829, [1, 4], Res36343), 
LFla34285 = flatten_layer(Res36343, Fla34285), 
LSub29439 = subtract_layer([[0.2698, 0.7799, 0.8899]], [[0.6088, 0.1158, 0.8729]], Sub29439), 
LCon22456 = concatenate_layer([Sub29439,[[0.0044]]], 1, Con22456), 
LMax67864 = maximum_layer([Fla34285,Con22456], Max67864), 
exec_layers([LMax6407,LMax68003,LZer40905,LCon32337,LMul95566,LBat48579,LMul84829,LRes36343,LFla34285,LSub29439,LCon22456,LMax67864],["Max6407","Max68003","Zer40905","Con32337","Mul95566","Bat48579","Mul84829","Res36343","Fla34285","Sub29439","Con22456","Max67864"],Max67864,"Max67864")

Actual (Unparsed): [[0.0000000, 0.6641000, 0.0170000, 0.0159966]]

Expected (Unparsed): [[0.0,0.6641,0.017000000000000015,0.015996571014658005]]

Actual:   [[0, 0.6641, 0.017, 0.016]]

Expected: [[0, 0.6641, 0.0171, 0.016]]