import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Sub76732 = tf.keras.layers.Input(shape=([3, 2]))
in1Sub76732 = tf.keras.layers.Input(shape=([3, 2]))
in0Con54797 = tf.keras.layers.Input(shape=([2, 1]))
in0Add11504 = tf.keras.layers.Input(shape=([2, 2]))
in1Add11504 = tf.keras.layers.Input(shape=([2, 2]))

Sub76732 = keras.layers.Subtract(name = 'Sub76732', )([in0Sub76732,in1Sub76732])
Glo74463 = keras.layers.GlobalAveragePooling1D(name = 'Glo74463', )(Sub76732)
Res90948 = keras.layers.Reshape((2, 1), name = 'Res90948', )(Glo74463)
Con54797 = keras.layers.Concatenate(axis=2, name = 'Con54797', )([Res90948,in0Con54797])
Add11504 = keras.layers.Add(name = 'Add11504', )([in0Add11504,in1Add11504])
Max89888 = keras.layers.MaxPool1D(pool_size=(1), name = 'Max89888', )(Add11504)
Mul452 = keras.layers.Multiply(name = 'Mul452', )([Con54797,Max89888])
Bat53094 = keras.layers.BatchNormalization(axis=1, epsilon=0.4232820217798253,  name = 'Bat53094', )(Mul452)
model = tf.keras.models.Model(inputs=[in0Sub76732,in1Sub76732,in0Con54797,in0Add11504,in1Add11504], outputs=Bat53094)
w = model.get_layer('Bat53094').get_weights() 
w[0] = np.array([0.1915, 0.2834])
w[1] = np.array([0.8126, 0.3053])
w[2] = np.array([0.8049, 0.6268])
w[3] = np.array([0.8085, 0.2611])
model.get_layer('Bat53094').set_weights(w) 
in0Sub76732 = tf.constant([[[0.8822, 0.3438], [0.3801, 0.4915], [0.8513, 0.9984]]])
in1Sub76732 = tf.constant([[[0.6922, 0.0305], [0.8452, 0.4174], [0.9317, 0.6605]]])
in0Con54797 = tf.constant([[[0.0802], [0.4352]]])
in0Add11504 = tf.constant([[[0.6205, 0.8956], [0.0922, 0.6754]]])
in1Add11504 = tf.constant([[[0.9804, 0.269], [0.0866, 0.8478]]])
print (np.array2string(model.predict([in0Sub76732,in1Sub76732,in0Con54797,in0Add11504,in1Add11504],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Bat53094.png')

LSub76732 = subtract_layer([[[0.8822, 0.3438], [0.3801, 0.4915], [0.8513, 0.9984]]], [[[0.6922, 0.0305], [0.8452, 0.4174], [0.9317, 0.6605]]], Sub76732), 
LGlo74463 = global_average_pooling1D_layer(Sub76732, Glo74463), 
LRes90948 = reshape_layer(Glo74463, [2, 1], Res90948), 
LCon54797 = concatenate_layer([Res90948,[[[0.0802], [0.4352]]]], 2, Con54797), 
LAdd11504 = add_layer([[[[0.6205, 0.8956], [0.0922, 0.6754]]], [[[0.9804, 0.269], [0.0866, 0.8478]]]], Add11504), 
LMax89888 = max_pool1D_layer(Add11504, 1, Max89888), 
LMul452 = multiply_layer([Con54797,Max89888], Mul452), 
LBat53094 = batch_normalization_layer(Mul452, 1, 0.4232820217798253, [0.1915, 0.2834], [0.8126, 0.3053], [0.8049, 0.6268], [0.8085, 0.2611], Bat53094), 
exec_layers([LSub76732,LGlo74463,LRes90948,LCon54797,LAdd11504,LMax89888,LMul452,LBat53094],["Sub76732","Glo74463","Res90948","Con54797","Add11504","Max89888","Mul452","Bat53094"],Bat53094,"Bat53094")

Actual (Unparsed): [[[0.6409858, 0.6898345], [0.1053851, 0.3176657]]]

Expected (Unparsed): [[[0.6409858054492737,0.6898345413759472],[0.10538513632746704,0.31766566075952446]]]

Actual:   [[[0.641, 0.6899], [0.1054, 0.3177]]]

Expected: [[[0.641, 0.6899], [0.1054, 0.3177]]]