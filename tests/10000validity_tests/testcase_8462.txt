import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Zer27088 = tf.keras.layers.Input(shape=([2, 4, 3, 2]))
in0Con11816 = tf.keras.layers.Input(shape=([4, 6, 2]))
in0Up_10300 = tf.keras.layers.Input(shape=([1, 1, 3, 2]))
in0ELU12901 = tf.keras.layers.Input(shape=([2, 2, 1]))
in0Con25971 = tf.keras.layers.Input(shape=([2, 3, 11]))
in0PRe18213 = tf.keras.layers.Input(shape=([2, 2]))
in0Bat44488 = tf.keras.layers.Input(shape=([3]))
in0Con69469 = tf.keras.layers.Input(shape=([1]))
in0Con19244 = tf.keras.layers.Input(shape=([285]))

Zer27088 = keras.layers.ZeroPadding3D(padding=((1, 1), (1, 1), (1, 1)), name = 'Zer27088', )(in0Zer27088)
Res18781 = keras.layers.Reshape((4, 6, 10), name = 'Res18781', )(Zer27088)
Con11816 = keras.layers.Concatenate(axis=3, name = 'Con11816', )([Res18781,in0Con11816])
Up_10300 = keras.layers.UpSampling3D(size=(2, 1, 2), name = 'Up_10300', )(in0Up_10300)
Res72158 = keras.layers.Reshape((2, 1, 12), name = 'Res72158', )(Up_10300)
Zer75318 = keras.layers.ZeroPadding2D(padding=((0, 0), (2, 0)), name = 'Zer75318', )(Res72158)
ELU12901 = keras.layers.ELU(alpha=7.538685352295804, name = 'ELU12901', input_shape=(2, 2, 1))(in0ELU12901)
Max34838 = keras.layers.MaxPool2D(pool_size=(2, 1), strides=(8, 1), padding='same', name = 'Max34838', )(ELU12901)
Zer89989 = keras.layers.ZeroPadding2D(padding=((1, 0), (1, 0)), name = 'Zer89989', )(Max34838)
Con25971 = keras.layers.Concatenate(axis=3, name = 'Con25971', )([Zer89989,in0Con25971])
Mul4406 = keras.layers.Multiply(name = 'Mul4406', )([Zer75318,Con25971])
Zer95609 = keras.layers.ZeroPadding2D(padding=((2, 0), (3, 0)), name = 'Zer95609', )(Mul4406)
Max51734 = keras.layers.Maximum(name = 'Max51734', )([Con11816,Zer95609])
Res92734 = keras.layers.Reshape((4, 72), name = 'Res92734', )(Max51734)
Fla38989 = keras.layers.Flatten(name = 'Fla38989', )(Res92734)
PRe18213 = keras.layers.PReLU(name = 'PRe18213', input_shape=(2, 2))(in0PRe18213)
Fla42979 = keras.layers.Flatten(name = 'Fla42979', )(PRe18213)
Bat44488 = keras.layers.BatchNormalization(axis=1, epsilon=0.19807959370240807,  name = 'Bat44488', )(in0Bat44488)
Con69469 = keras.layers.Concatenate(axis=1, name = 'Con69469', )([Bat44488,in0Con69469])
Mul43543 = keras.layers.Multiply(name = 'Mul43543', )([Fla42979,Con69469])
Res70824 = keras.layers.Reshape((4, 1), name = 'Res70824', )(Mul43543)
GRU38827 = keras.layers.GRU(3,reset_after=False, recurrent_activation='sigmoid', name = 'GRU38827', )(Res70824)
Con19244 = keras.layers.Concatenate(axis=1, name = 'Con19244', )([GRU38827,in0Con19244])
Mul2652 = keras.layers.Multiply(name = 'Mul2652', )([Fla38989,Con19244])
model = tf.keras.models.Model(inputs=[in0Zer27088,in0Con11816,in0Up_10300,in0ELU12901,in0Con25971,in0PRe18213,in0Bat44488,in0Con69469,in0Con19244], outputs=Mul2652)
w = model.get_layer('PRe18213').get_weights() 
w[0] = np.array([[0.3971, 0.2498], [0.3442, 0.8539]])
model.get_layer('PRe18213').set_weights(w) 
w = model.get_layer('Bat44488').get_weights() 
w[0] = np.array([0.4716, 0.5535, 0.7348])
w[1] = np.array([0.145, 0.7637, 0.5044])
w[2] = np.array([0.503, 0.4935, 0.5541])
w[3] = np.array([0.6218, 0.3062, 0.5026])
model.get_layer('Bat44488').set_weights(w) 
w = model.get_layer('GRU38827').get_weights() 
w[0] = np.array([[10, 9, 10, 2, 2, 7, 9, 9, 8]])
w[1] = np.array([[10, 1, 4, 5, 3, 1, 5, 1, 5], [9, 10, 5, 10, 7, 6, 4, 8, 4], [8, 3, 8, 7, 5, 4, 8, 7, 9]])
w[2] = np.array([7, 2, 2, 3, 5, 1, 7, 4, 5])
model.get_layer('GRU38827').set_weights(w) 
in0Zer27088 = tf.constant([[[[[1.9851, 1.8621], [1.1283, 1.053], [1.7704, 1.5217]], [[1.0143, 1.824], [1.2069, 1.3765], [1.6483, 1.3824]], [[1.3217, 1.2691], [1.8814, 1.201], [1.2377, 1.5258]], [[1.6267, 1.8157], [1.5891, 1.5099], [1.8661, 1.1994]]], [[[1.6964, 1.8947], [1.1919, 1.3613], [1.5028, 1.8957]], [[1.9834, 1.9647], [1.9174, 1.5686], [1.9762, 1.2667]], [[1.1325, 1.6779], [1.4206, 1.2204], [1.6843, 1.5744]], [[1.2469, 1.1551], [1.517, 1.9779], [1.6529, 1.0497]]]]])
in0Con11816 = tf.constant([[[[0.7912, 0.6397], [0.1635, 0.26], [0.9669, 0.0509], [0.3659, 0.3423], [0.7076, 0.2847], [0.6474, 0.2019]], [[0.2598, 0.0858], [0.5045, 0.4251], [0.8618, 0.3621], [0.7381, 0.8315], [0.7879, 0.8263], [0.8975, 0.1405]], [[0.5574, 0.9752], [0.4057, 0.2225], [0.0972, 0.3145], [0.4365, 0.4711], [0.0593, 0.408], [0.3498, 0.955]], [[0.8103, 0.1534], [0.6414, 0.7865], [0.8356, 0.6691], [0.9113, 0.5772], [0.4291, 0.3151], [0.315, 0.9435]]]])
in0Up_10300 = tf.constant([[[[[1.0696, 1.5586], [1.3144, 1.0483], [1.4601, 1.7326]]]]])
in0ELU12901 = tf.constant([[[[0.9813], [0.5353]], [[0.1649], [0.1429]]]])
in0Con25971 = tf.constant([[[[0.6655, 0.5584, 0.5873, 0.1279, 0.0636, 0.9097, 0.1957, 0.6258, 0.7502, 0.1542, 0.4383], [0.4805, 0.4142, 0.7664, 0.7765, 0.4742, 0.6948, 0.8151, 0.8788, 0.9924, 0.1702, 0.2628], [0.141, 0.6826, 0.331, 0.1661, 0.6355, 0.7711, 0.875, 0.71, 0.6209, 0.7368, 0.3546]], [[0.2101, 0.2654, 0.1982, 0.8028, 0.571, 0.1646, 0.6773, 0.4734, 0.206, 0.566, 0.3706], [0.2378, 0.0875, 0.9526, 0.1789, 0.502, 0.4253, 0.7837, 0.027, 0.727, 0.4524, 0.7418], [0.9377, 0.7698, 0.9318, 0.7942, 0.4166, 0.2953, 0.296, 0.7393, 0.4489, 0.4531, 0.7788]]]])
in0PRe18213 = tf.constant([[[0.5669, 0.7792], [0.2044, 0.8843]]])
in0Bat44488 = tf.constant([[1.9641, 1.8845, 1.7377]])
in0Con69469 = tf.constant([[0.0701]])
in0Con19244 = tf.constant([[0.4863, 0.8484, 0.758, 0.1095, 0.1125, 0.7617, 0.8126, 0.2857, 0.0959, 0.2946, 0.3564, 0.2515, 0.2011, 0.515, 0.3251, 0.9982, 0.5098, 0.9381, 0.7417, 0.9382, 0.1811, 0.3464, 0.92, 0.3535, 0.6301, 0.7584, 0.0589, 0.8618, 0.1204, 0.2679, 0.0776, 0.7466, 0.0485, 0.6039, 0.0934, 0.0186, 0.2599, 0.4323, 0.7674, 0.6339, 0.5841, 0.6112, 0.8602, 0.3817, 0.4, 0.8433, 0.7239, 0.5696, 0.4499, 0.4252, 0.1374, 0.7877, 0.3105, 0.0856, 0.8393, 0.71, 0.8979, 0.2226, 0.3526, 0.106, 0.1346, 0.2063, 0.2096, 0.9766, 0.4388, 0.7943, 0.4839, 0.0055, 0.011, 0.1238, 0.6169, 0.3189, 0.9847, 0.1896, 0.8835, 0.8873, 0.3874, 0.1956, 0.4859, 0.7694, 0.5129, 0.6763, 0.1869, 0.7967, 0.7457, 0.8223, 0.5969, 0.2308, 0.097, 0.0208, 0.7151, 0.3838, 0.7141, 0.2169, 0.0926, 0.1886, 0.4051, 0.3422, 0.0313, 0.2279, 0.7874, 0.719, 0.5631, 0.7883, 0.0101, 0.0435, 0.5632, 0.0911, 0.1279, 0.2826, 0.2441, 0.9707, 0.1123, 0.5748, 0.4484, 0.9201, 0.1709, 0.0156, 0.6133, 0.6053, 0.5206, 0.0323, 0.0156, 0.6596, 0.0059, 0.9009, 0.9209, 0.8143, 0.99, 0.8462, 0.5607, 0.1962, 0.7574, 0.0776, 0.4387, 0.9269, 0.263, 0.062, 0.3279, 0.9973, 0.4371, 0.3111, 0.4633, 0.7576, 0.4139, 0.9695, 0.8558, 0.2633, 0.9907, 0.5312, 0.3582, 0.1483, 0.3411, 0.082, 0.0996, 0.6574, 0.5117, 0.5673, 0.1704, 0.9654, 0.5079, 0.7218, 0.9884, 0.9311, 0.8448, 0.4585, 0.6062, 0.9951, 0.2769, 0.7135, 0.3285, 0.0112, 0.0519, 0.9697, 0.4021, 0.4261, 0.4379, 0.9409, 0.2124, 0.3643, 0.4629, 0.1897, 0.3745, 0.8629, 0.7752, 0.7701, 0.344, 0.7062, 0.2841, 0.3432, 0.6855, 0.0596, 0.7728, 0.5937, 0.9404, 0.371, 0.9876, 0.1932, 0.1078, 0.8173, 0.8053, 0.9997, 0.1193, 0.909, 0.9525, 0.6641, 0.3251, 0.7347, 0.4114, 0.3181, 0.9338, 0.7259, 0.0829, 0.5638, 0.5532, 0.7544, 0.4986, 0.5508, 0.4629, 0.8341, 0.3737, 0.5341, 0.0164, 0.6967, 0.5758, 0.937, 0.6202, 0.0288, 0.6471, 0.6079, 0.0642, 0.2206, 0.3625, 0.6596, 0.5884, 0.0573, 0.9334, 0.0024, 0.9645, 0.9056, 0.3712, 0.4451, 0.0412, 0.5979, 0.2174, 0.0268, 0.6574, 0.3936, 0.3114, 0.0616, 0.5634, 0.1138, 0.5242, 0.2346, 0.0054, 0.7842, 0.4125, 0.9688, 0.9275, 0.8991, 0.5662, 0.1973, 0.3244, 0.7229, 0.0133, 0.624, 0.138, 0.883, 0.4246, 0.5305, 0.2321, 0.876, 0.0163, 0.1977, 0.4044, 0.0093, 0.988, 0.0502, 0.0968, 0.7305, 0.0532, 0.7025, 0.451, 0.2566, 0.1198]])
print (np.array2string(model.predict([in0Zer27088,in0Con11816,in0Up_10300,in0ELU12901,in0Con25971,in0PRe18213,in0Bat44488,in0Con69469,in0Con19244],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Mul2652.png')

LZer27088 = zero_padding3D_layer([[[[[1.9851, 1.8621], [1.1283, 1.053], [1.7704, 1.5217]], [[1.0143, 1.824], [1.2069, 1.3765], [1.6483, 1.3824]], [[1.3217, 1.2691], [1.8814, 1.201], [1.2377, 1.5258]], [[1.6267, 1.8157], [1.5891, 1.5099], [1.8661, 1.1994]]], [[[1.6964, 1.8947], [1.1919, 1.3613], [1.5028, 1.8957]], [[1.9834, 1.9647], [1.9174, 1.5686], [1.9762, 1.2667]], [[1.1325, 1.6779], [1.4206, 1.2204], [1.6843, 1.5744]], [[1.2469, 1.1551], [1.517, 1.9779], [1.6529, 1.0497]]]]], 1, 1, 1, 1, 1, 1, Zer27088), 
LRes18781 = reshape_layer(Zer27088, [4, 6, 10], Res18781), 
LCon11816 = concatenate_layer([Res18781,[[[[0.7912, 0.6397], [0.1635, 0.26], [0.9669, 0.0509], [0.3659, 0.3423], [0.7076, 0.2847], [0.6474, 0.2019]], [[0.2598, 0.0858], [0.5045, 0.4251], [0.8618, 0.3621], [0.7381, 0.8315], [0.7879, 0.8263], [0.8975, 0.1405]], [[0.5574, 0.9752], [0.4057, 0.2225], [0.0972, 0.3145], [0.4365, 0.4711], [0.0593, 0.408], [0.3498, 0.955]], [[0.8103, 0.1534], [0.6414, 0.7865], [0.8356, 0.6691], [0.9113, 0.5772], [0.4291, 0.3151], [0.315, 0.9435]]]]], 3, Con11816), 
LUp_10300 = up_sampling3D_layer([[[[[1.0696, 1.5586], [1.3144, 1.0483], [1.4601, 1.7326]]]]], 2, 1, 2, Up_10300), 
LRes72158 = reshape_layer(Up_10300, [2, 1, 12], Res72158), 
LZer75318 = zero_padding2D_layer(Res72158, 0, 0, 2, 0, Zer75318), 
LELU12901 = elu_layer([[[[0.9813], [0.5353]], [[0.1649], [0.1429]]]], 7.538685352295804, ELU12901), 
LMax34838 = max_pool2D_layer(ELU12901, 2, 1, 8, 1, true, Max34838), 
LZer89989 = zero_padding2D_layer(Max34838, 1, 0, 1, 0, Zer89989), 
LCon25971 = concatenate_layer([Zer89989,[[[[0.6655, 0.5584, 0.5873, 0.1279, 0.0636, 0.9097, 0.1957, 0.6258, 0.7502, 0.1542, 0.4383], [0.4805, 0.4142, 0.7664, 0.7765, 0.4742, 0.6948, 0.8151, 0.8788, 0.9924, 0.1702, 0.2628], [0.141, 0.6826, 0.331, 0.1661, 0.6355, 0.7711, 0.875, 0.71, 0.6209, 0.7368, 0.3546]], [[0.2101, 0.2654, 0.1982, 0.8028, 0.571, 0.1646, 0.6773, 0.4734, 0.206, 0.566, 0.3706], [0.2378, 0.0875, 0.9526, 0.1789, 0.502, 0.4253, 0.7837, 0.027, 0.727, 0.4524, 0.7418], [0.9377, 0.7698, 0.9318, 0.7942, 0.4166, 0.2953, 0.296, 0.7393, 0.4489, 0.4531, 0.7788]]]]], 3, Con25971), 
LMul4406 = multiply_layer([Zer75318,Con25971], Mul4406), 
LZer95609 = zero_padding2D_layer(Mul4406, 2, 0, 3, 0, Zer95609), 
LMax51734 = maximum_layer([Con11816,Zer95609], Max51734), 
LRes92734 = reshape_layer(Max51734, [4, 72], Res92734), 
LFla38989 = flatten_layer(Res92734, Fla38989), 
LPRe18213 = prelu_layer([[[0.5669, 0.7792], [0.2044, 0.8843]]], [[0.3971, 0.2498], [0.3442, 0.8539]], PRe18213), 
LFla42979 = flatten_layer(PRe18213, Fla42979), 
LBat44488 = batch_normalization_layer([[1.9641, 1.8845, 1.7377]], 1, 0.19807959370240807, [0.4716, 0.5535, 0.7348], [0.145, 0.7637, 0.5044], [0.503, 0.4935, 0.5541], [0.6218, 0.3062, 0.5026], Bat44488), 
LCon69469 = concatenate_layer([Bat44488,[[0.0701]]], 1, Con69469), 
LMul43543 = multiply_layer([Fla42979,Con69469], Mul43543), 
LRes70824 = reshape_layer(Mul43543, [4, 1], Res70824), 
LGRU38827 = gru_layer(Res70824,[[10, 9, 10, 2, 2, 7, 9, 9, 8]],[[10, 1, 4, 5, 3, 1, 5, 1, 5], [9, 10, 5, 10, 7, 6, 4, 8, 4], [8, 3, 8, 7, 5, 4, 8, 7, 9]],[7, 2, 2, 3, 5, 1, 7, 4, 5], false, GRU38827), 
LCon19244 = concatenate_layer([GRU38827,[[0.4863, 0.8484, 0.758, 0.1095, 0.1125, 0.7617, 0.8126, 0.2857, 0.0959, 0.2946, 0.3564, 0.2515, 0.2011, 0.515, 0.3251, 0.9982, 0.5098, 0.9381, 0.7417, 0.9382, 0.1811, 0.3464, 0.92, 0.3535, 0.6301, 0.7584, 0.0589, 0.8618, 0.1204, 0.2679, 0.0776, 0.7466, 0.0485, 0.6039, 0.0934, 0.0186, 0.2599, 0.4323, 0.7674, 0.6339, 0.5841, 0.6112, 0.8602, 0.3817, 0.4, 0.8433, 0.7239, 0.5696, 0.4499, 0.4252, 0.1374, 0.7877, 0.3105, 0.0856, 0.8393, 0.71, 0.8979, 0.2226, 0.3526, 0.106, 0.1346, 0.2063, 0.2096, 0.9766, 0.4388, 0.7943, 0.4839, 0.0055, 0.011, 0.1238, 0.6169, 0.3189, 0.9847, 0.1896, 0.8835, 0.8873, 0.3874, 0.1956, 0.4859, 0.7694, 0.5129, 0.6763, 0.1869, 0.7967, 0.7457, 0.8223, 0.5969, 0.2308, 0.097, 0.0208, 0.7151, 0.3838, 0.7141, 0.2169, 0.0926, 0.1886, 0.4051, 0.3422, 0.0313, 0.2279, 0.7874, 0.719, 0.5631, 0.7883, 0.0101, 0.0435, 0.5632, 0.0911, 0.1279, 0.2826, 0.2441, 0.9707, 0.1123, 0.5748, 0.4484, 0.9201, 0.1709, 0.0156, 0.6133, 0.6053, 0.5206, 0.0323, 0.0156, 0.6596, 0.0059, 0.9009, 0.9209, 0.8143, 0.99, 0.8462, 0.5607, 0.1962, 0.7574, 0.0776, 0.4387, 0.9269, 0.263, 0.062, 0.3279, 0.9973, 0.4371, 0.3111, 0.4633, 0.7576, 0.4139, 0.9695, 0.8558, 0.2633, 0.9907, 0.5312, 0.3582, 0.1483, 0.3411, 0.082, 0.0996, 0.6574, 0.5117, 0.5673, 0.1704, 0.9654, 0.5079, 0.7218, 0.9884, 0.9311, 0.8448, 0.4585, 0.6062, 0.9951, 0.2769, 0.7135, 0.3285, 0.0112, 0.0519, 0.9697, 0.4021, 0.4261, 0.4379, 0.9409, 0.2124, 0.3643, 0.4629, 0.1897, 0.3745, 0.8629, 0.7752, 0.7701, 0.344, 0.7062, 0.2841, 0.3432, 0.6855, 0.0596, 0.7728, 0.5937, 0.9404, 0.371, 0.9876, 0.1932, 0.1078, 0.8173, 0.8053, 0.9997, 0.1193, 0.909, 0.9525, 0.6641, 0.3251, 0.7347, 0.4114, 0.3181, 0.9338, 0.7259, 0.0829, 0.5638, 0.5532, 0.7544, 0.4986, 0.5508, 0.4629, 0.8341, 0.3737, 0.5341, 0.0164, 0.6967, 0.5758, 0.937, 0.6202, 0.0288, 0.6471, 0.6079, 0.0642, 0.2206, 0.3625, 0.6596, 0.5884, 0.0573, 0.9334, 0.0024, 0.9645, 0.9056, 0.3712, 0.4451, 0.0412, 0.5979, 0.2174, 0.0268, 0.6574, 0.3936, 0.3114, 0.0616, 0.5634, 0.1138, 0.5242, 0.2346, 0.0054, 0.7842, 0.4125, 0.9688, 0.9275, 0.8991, 0.5662, 0.1973, 0.3244, 0.7229, 0.0133, 0.624, 0.138, 0.883, 0.4246, 0.5305, 0.2321, 0.876, 0.0163, 0.1977, 0.4044, 0.0093, 0.988, 0.0502, 0.0968, 0.7305, 0.0532, 0.7025, 0.451, 0.2566, 0.1198]]], 1, Con19244), 
LMul2652 = multiply_layer([Fla38989,Con19244], Mul2652), 
exec_layers([LZer27088,LRes18781,LCon11816,LUp_10300,LRes72158,LZer75318,LELU12901,LMax34838,LZer89989,LCon25971,LMul4406,LZer95609,LMax51734,LRes92734,LFla38989,LPRe18213,LFla42979,LBat44488,LCon69469,LMul43543,LRes70824,LGRU38827,LCon19244,LMul2652],["Zer27088","Res18781","Con11816","Up_10300","Res72158","Zer75318","ELU12901","Max34838","Zer89989","Con25971","Mul4406","Zer95609","Max51734","Res92734","Fla38989","PRe18213","Fla42979","Bat44488","Con69469","Mul43543","Res70824","GRU38827","Con19244","Mul2652"],Mul2652,"Mul2652")

Actual (Unparsed): [[0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.2260458, 0.0613472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.1533957, 0.0470860, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.7218875, 0.0024687, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.1396640, 0.1369200, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.5023960, 0.2556321, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0035607, 0.0022209, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.1998901, 0.0440068, 0.0000000, 0.0000000, 1.5815292, 1.3885680, 0.9278011, 0.6285357, 0.4086083, 0.1476049, 0.0000000, 0.0000000, 0.1936271, 0.3035639, 0.0000000, 0.0000000, 0.1912970, 0.7389024, 0.4130012, 0.0430845, 0.3756476, 1.0885018, 0.0000000, 0.0000000, 0.6793569, 0.0036572, 0.0000000, 0.0000000, 0.1204069, 0.1623179, 0.5316836, 0.2931641, 1.2014354, 0.1713473, 0.0000000, 0.0000000, 0.6791258, 0.1421034, 0.0000000, 0.0000000, 0.9846416, 0.9452535, 0.0513279, 0.0235544, 1.2308796, 0.0070765, 0.0000000, 0.0000000, 0.6415869, 0.8180370, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.8950768, 0.0614125, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0826624, 0.3326407, 0.0000000, 0.0000000, 1.1152134, 0.9695180, 0.6761649, 0.2319655, 1.4508031, 0.9628260, 0.0000000, 0.0000000, 0.3777473, 0.1879680, 0.0000000, 0.0000000, 1.9736814, 0.5440254, 1.3680649, 0.5152851, 0.0221334, 0.0657417, 0.0000000, 0.0000000, 0.0414169, 0.1377196, 0.0000000, 0.0000000, 0.4125698, 0.7766999, 0.2694878, 0.4570398, 1.4533825, 1.2204749, 0.0000000, 0.0000000, 0.3082563, 0.1338395, 0.0000000, 0.0000000, 0.0743152, 0.8926613, 0.9006429, 1.8600172, 0.6132259, 1.0366838, 0.0000000, 0.0000000, 0.0484659, 0.3285624, 0.0000000, 0.0262177, 0.6636690, 0.4913915, 0.1449875, 0.2165799, 0.7446433, 0.3773618, 0.3297651, 1.0045552, 0.7809245, 0.0791695, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.5645360, 0.0883277, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0367522, 0.7341191, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.3288922, 0.2083577, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.8193498, 0.3268106, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.3758916, 0.0051361, 0.1131945, 0.5910302, 0.0076574, 1.4348758, 0.0524036, 0.0422747, 0.2835380, 0.0165078, 0.7583150, 0.3507716, 0.1697592, 0.1616520]]

Expected (Unparsed): [[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22604584,0.06134723,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15339570000000002,0.047086,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.72188754,0.00246865,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13966403,0.13692000000000001,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.502396,0.25563213,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0035606999999999995,0.0022209,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19989011999999998,0.04400682,0.0,0.0,1.58152917,1.3885679700000002,0.9278010900000001,0.6285356999999999,0.40860832,0.1476049,0.0,0.0,0.19362709999999997,0.30356391,0.0,0.0,0.19129697999999998,0.7389024000000001,0.41300118,0.04308445,0.37564757,1.08850176,0.0,0.0,0.67935694,0.0036572099999999997,0.0,0.0,0.12040687000000001,0.16231789,0.53168364,0.29316410000000004,1.20143539,0.17134734,0.0,0.0,0.67912581,0.14210335,0.0,0.0,0.9846415099999999,0.94525342,0.05132793,0.02355444,1.23087956,0.00707646,0.0,0.0,0.6415869700000001,0.818037,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89507675,0.06141255,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08266242,0.33264072,0.0,0.0,1.1152133599999998,0.9695179900000002,0.67616487,0.23196551999999998,1.45080312,0.96282603,0.0,0.0,0.37774727,0.187968,0.0,0.0,1.97368134,0.5440254299999999,1.3680649,0.5152851,0.02213344,0.06574173,0.0,0.0,0.041416919999999996,0.13771955,0.0,0.0,0.41256975,0.7766999099999999,0.26948782000000004,0.4570398,1.45338247,1.22047488,0.0,0.0,0.30825630000000004,0.13383951000000002,0.0,0.0,0.07431523999999999,0.8926612800000001,0.9006428999999999,1.86001716,0.6132259,1.03668372,0.0,0.0,0.04846589,0.3285624,0.0,0.02621767818,0.6636690446400001,0.49139151150000004,0.144987533944,0.216579880715,0.7446433122480001,0.37736179249999996,0.3297650451,1.004555277292,0.7809244395119999,0.0791695,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56453601,0.08832772,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.036752219999999995,0.7341191,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.32889216,0.20835774,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8193498300000001,0.32681064000000004,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3758916,0.005136129999999999,0.113194495176,0.5910302845679999,0.0076574161440000005,1.4348758382399998,0.05240360329600001,0.042274668304,0.28353796476000004,0.01650778976,0.7583149808249999,0.35077162714000004,0.169759198146,0.161651995824]]

Actual:   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2261, 0.0614, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1534, 0.0471, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7219, 0.0025, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1397, 0.137, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5024, 0.2557, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0036, 0.0023, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1999, 0.0441, 0, 0, 1.5816, 1.3886, 0.9279, 0.6286, 0.4087, 0.1477, 0, 0, 0.1937, 0.3036, 0, 0, 0.1913, 0.739, 0.4131, 0.0431, 0.3757, 1.0886, 0, 0, 0.6794, 0.0037, 0, 0, 0.1205, 0.1624, 0.5317, 0.2932, 1.2015, 0.1714, 0, 0, 0.6792, 0.1422, 0, 0, 0.9847, 0.9453, 0.0514, 0.0236, 1.2309, 0.0071, 0, 0, 0.6416, 0.8181, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.8951, 0.0615, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0827, 0.3327, 0, 0, 1.1153, 0.9696, 0.6762, 0.232, 1.4509, 0.9629, 0, 0, 0.3778, 0.188, 0, 0, 1.9737, 0.5441, 1.3681, 0.5153, 0.0222, 0.0658, 0, 0, 0.0415, 0.1378, 0, 0, 0.4126, 0.7767, 0.2695, 0.4571, 1.4534, 1.2205, 0, 0, 0.3083, 0.1339, 0, 0, 0.0744, 0.8927, 0.9007, 1.8601, 0.6133, 1.0367, 0, 0, 0.0485, 0.3286, 0, 0.0263, 0.6637, 0.4914, 0.145, 0.2166, 0.7447, 0.3774, 0.3298, 1.0046, 0.781, 0.0792, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5646, 0.0884, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0368, 0.7342, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3289, 0.2084, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.8194, 0.3269, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3759, 0.0052, 0.1132, 0.5911, 0.0077, 1.4349, 0.0525, 0.0423, 0.2836, 0.0166, 0.7584, 0.3508, 0.1698, 0.1617]]

Expected: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2261, 0.0614, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1534, 0.0471, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7219, 0.0025, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1397, 0.137, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5024, 0.2557, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0036, 0.0023, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1999, 0.0441, 0, 0, 1.5816, 1.3886, 0.9279, 0.6286, 0.4087, 0.1477, 0, 0, 0.1937, 0.3036, 0, 0, 0.1913, 0.739, 0.4131, 0.0431, 0.3757, 1.0886, 0, 0, 0.6794, 0.0037, 0, 0, 0.1205, 0.1624, 0.5317, 0.2932, 1.2015, 0.1714, 0, 0, 0.6792, 0.1422, 0, 0, 0.9847, 0.9453, 0.0514, 0.0236, 1.2309, 0.0071, 0, 0, 0.6416, 0.8181, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.8951, 0.0615, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0827, 0.3327, 0, 0, 1.1153, 0.9696, 0.6762, 0.232, 1.4509, 0.9629, 0, 0, 0.3778, 0.188, 0, 0, 1.9737, 0.5441, 1.3681, 0.5153, 0.0222, 0.0658, 0, 0, 0.0415, 0.1378, 0, 0, 0.4126, 0.7767, 0.2695, 0.4571, 1.4534, 1.2205, 0, 0, 0.3083, 0.1339, 0, 0, 0.0744, 0.8927, 0.9007, 1.8601, 0.6133, 1.0367, 0, 0, 0.0485, 0.3286, 0, 0.0263, 0.6637, 0.4914, 0.145, 0.2166, 0.7447, 0.3774, 0.3298, 1.0046, 0.781, 0.0792, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5646, 0.0884, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0368, 0.7342, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3289, 0.2084, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.8194, 0.3269, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3759, 0.0052, 0.1132, 0.5911, 0.0077, 1.4349, 0.0525, 0.0423, 0.2836, 0.0166, 0.7584, 0.3508, 0.1698, 0.1617]]