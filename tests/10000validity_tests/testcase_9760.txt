import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Dot48483 = tf.keras.layers.Input(shape=([3]))
in1Dot48483 = tf.keras.layers.Input(shape=([3]))
in0PRe2001 = tf.keras.layers.Input(shape=([1, 2, 2, 2]))
in0Sub64828 = tf.keras.layers.Input(shape=([3, 3, 2]))
in1Sub64828 = tf.keras.layers.Input(shape=([3, 3, 2]))
in0Con86089 = tf.keras.layers.Input(shape=([6, 2]))
in0Sep61983 = tf.keras.layers.Input(shape=([2, 2]))
in0Con56346 = tf.keras.layers.Input(shape=([6, 5]))
in0Con22669 = tf.keras.layers.Input(shape=([6, 8, 1]))
in0PRe11374 = tf.keras.layers.Input(shape=([1, 1, 2]))

Dot48483 = keras.layers.Dot(axes=(1, 1), name = 'Dot48483', )([in0Dot48483,in1Dot48483])
Res12161 = keras.layers.Reshape((1, 1), name = 'Res12161', )(Dot48483)
PRe2001 = keras.layers.PReLU(name = 'PRe2001', input_shape=(1, 2, 2, 2))(in0PRe2001)
Res43570 = keras.layers.Reshape((1, 2, 4), name = 'Res43570', )(PRe2001)
Res69962 = keras.layers.Reshape((1, 8), name = 'Res69962', )(Res43570)
Dot13407 = keras.layers.Dot(axes=(1, 1), name = 'Dot13407', )([Res12161,Res69962])
Zer49397 = keras.layers.ZeroPadding1D(padding=((5, 0)), name = 'Zer49397', )(Dot13407)
Sub64828 = keras.layers.Subtract(name = 'Sub64828', )([in0Sub64828,in1Sub64828])
Res72460 = keras.layers.Reshape((3, 6), name = 'Res72460', )(Sub64828)
Up_12897 = keras.layers.UpSampling1D(size=(2), name = 'Up_12897', )(Res72460)
Con86089 = keras.layers.Concatenate(axis=2, name = 'Con86089', )([Up_12897,in0Con86089])
Min64456 = keras.layers.Minimum(name = 'Min64456', )([Zer49397,Con86089])
Sep61983 = keras.layers.SeparableConv1D(3, (1),strides=(1), padding='valid', name = 'Sep61983', )(in0Sep61983)
Zer72440 = keras.layers.ZeroPadding1D(padding=((4, 0)), name = 'Zer72440', )(Sep61983)
Con56346 = keras.layers.Concatenate(axis=2, name = 'Con56346', )([Zer72440,in0Con56346])
Sub4527 = keras.layers.Subtract(name = 'Sub4527', )([Min64456,Con56346])
Res81965 = keras.layers.Reshape((6, 8, 1), name = 'Res81965', )(Sub4527)
Con22669 = keras.layers.Concatenate(axis=3, name = 'Con22669', )([Res81965,in0Con22669])
PRe11374 = keras.layers.PReLU(name = 'PRe11374', input_shape=(1, 1, 2))(in0PRe11374)
Zer47254 = keras.layers.ZeroPadding2D(padding=((5, 0), (7, 0)), name = 'Zer47254', )(PRe11374)
Sub57563 = keras.layers.Subtract(name = 'Sub57563', )([Con22669,Zer47254])
model = tf.keras.models.Model(inputs=[in0Dot48483,in1Dot48483,in0PRe2001,in0Sub64828,in1Sub64828,in0Con86089,in0Sep61983,in0Con56346,in0Con22669,in0PRe11374], outputs=Sub57563)
w = model.get_layer('PRe2001').get_weights() 
w[0] = np.array([[[[0.7982, 0.4332], [0.7095, 0.6096]], [[0.8441, 0.1684], [0.0231, 0.3452]]]])
model.get_layer('PRe2001').set_weights(w) 
w = model.get_layer('Sep61983').get_weights() 
w[0] = np.array([[[0.9716], [0.4301]]])
w[1] = np.array([[[0.9364, 0.3447, 0.3914], [0.591, 0.8206, 0.0506]]])
w[2] = np.array([0, 0, 0])
model.get_layer('Sep61983').set_weights(w) 
w = model.get_layer('PRe11374').get_weights() 
w[0] = np.array([[[0.0183, 0.3774]]])
model.get_layer('PRe11374').set_weights(w) 
in0Dot48483 = tf.constant([[0.9199, 0.1603, 0.6715]])
in1Dot48483 = tf.constant([[0.4663, 0.7995, 0.702]])
in0PRe2001 = tf.constant([[[[[0.1492, 0.8231], [0.5381, 0.8755]], [[0.4728, 0.6327], [0.2598, 0.5576]]]]])
in0Sub64828 = tf.constant([[[[0.8378, 0.9158], [0.909, 0.7966], [0.1952, 0.0664]], [[0.2378, 0.235], [0.9909, 0.3181], [0.1491, 0.9149]], [[0.3774, 0.3849], [0.2839, 0.607], [0.538, 0.8777]]]])
in1Sub64828 = tf.constant([[[[0.1925, 0.9815], [0.2361, 0.9166], [0.1681, 0.6521]], [[0.9522, 0.2141], [0.836, 0.3685], [0.0024, 0.7863]], [[0.889, 0.4049], [0.8485, 0.3654], [0.9405, 0.5]]]])
in0Con86089 = tf.constant([[[0.8655, 0.3493], [0.4483, 0.7164], [0.9617, 0.4817], [0.6058, 0.5484], [0.3189, 0.4093], [0.1511, 0.0725]]])
in0Sep61983 = tf.constant([[[0.4003, 0.5316], [0.3258, 0.54]]])
in0Con56346 = tf.constant([[[0.9637, 0.9643, 0.4085, 0.0695, 0.7591], [0.3451, 0.5443, 0.5543, 0.4877, 0.8687], [0.0649, 0.2948, 0.9568, 0.5756, 0.9486], [0.5433, 0.6607, 0.665, 0.0706, 0.2418], [0.6044, 0.9669, 0.3195, 0.6394, 0.4144], [0.1204, 0.6871, 0.7121, 0.2089, 0.1673]]])
in0Con22669 = tf.constant([[[[0.4659], [0.0741], [0.4143], [0.8965], [0.1812], [0.4054], [0.6853], [0.3246]], [[0.7129], [0.8813], [0.5817], [0.2888], [0.9454], [0.1112], [0.475], [0.314]], [[0.6815], [0.3662], [0.2906], [0.8057], [0.9614], [0.9733], [0.7381], [0.9605]], [[0.0738], [0.2266], [0.0991], [0.4081], [0.6784], [0.8795], [0.7735], [0.9837]], [[0.4007], [0.1428], [0.6178], [0.4014], [0.2409], [0.2547], [0.4785], [0.7601]], [[0.1442], [0.1113], [0.2259], [0.7495], [0.1237], [0.1519], [0.0175], [0.1454]]]])
in0PRe11374 = tf.constant([[[[0.4916, 0.5625]]]])
print (np.array2string(model.predict([in0Dot48483,in1Dot48483,in0PRe2001,in0Sub64828,in1Sub64828,in0Con86089,in0Sep61983,in0Con56346,in0Con22669,in0PRe11374],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Sub57563.png')

LDot48483 = dot_layer([[0.9199, 0.1603, 0.6715]], [[0.4663, 0.7995, 0.702]], 1, 1, Dot48483), 
LRes12161 = reshape_layer(Dot48483, [1, 1], Res12161), 
LPRe2001 = prelu_layer([[[[[0.1492, 0.8231], [0.5381, 0.8755]], [[0.4728, 0.6327], [0.2598, 0.5576]]]]], [[[[0.7982, 0.4332], [0.7095, 0.6096]], [[0.8441, 0.1684], [0.0231, 0.3452]]]], PRe2001), 
LRes43570 = reshape_layer(PRe2001, [1, 2, 4], Res43570), 
LRes69962 = reshape_layer(Res43570, [1, 8], Res69962), 
LDot13407 = dot_layer(Res12161,Res69962, 1, 1, Dot13407), 
LZer49397 = zero_padding1D_layer(Dot13407, 5, 0, Zer49397), 
LSub64828 = subtract_layer([[[[0.8378, 0.9158], [0.909, 0.7966], [0.1952, 0.0664]], [[0.2378, 0.235], [0.9909, 0.3181], [0.1491, 0.9149]], [[0.3774, 0.3849], [0.2839, 0.607], [0.538, 0.8777]]]], [[[[0.1925, 0.9815], [0.2361, 0.9166], [0.1681, 0.6521]], [[0.9522, 0.2141], [0.836, 0.3685], [0.0024, 0.7863]], [[0.889, 0.4049], [0.8485, 0.3654], [0.9405, 0.5]]]], Sub64828), 
LRes72460 = reshape_layer(Sub64828, [3, 6], Res72460), 
LUp_12897 = up_sampling1D_layer(Res72460, 2, Up_12897), 
LCon86089 = concatenate_layer([Up_12897,[[[0.8655, 0.3493], [0.4483, 0.7164], [0.9617, 0.4817], [0.6058, 0.5484], [0.3189, 0.4093], [0.1511, 0.0725]]]], 2, Con86089), 
LMin64456 = minimum_layer([Zer49397,Con86089], Min64456), 
LSep61983 = separable_conv1D_layer([[[0.4003, 0.5316], [0.3258, 0.54]]], 1,[[[[0.9716], [0.4301]]],[[[0.9364, 0.3447, 0.3914], [0.591, 0.8206, 0.0506]]]],[0, 0, 0], 1, false, Sep61983), 
LZer72440 = zero_padding1D_layer(Sep61983, 4, 0, Zer72440), 
LCon56346 = concatenate_layer([Zer72440,[[[0.9637, 0.9643, 0.4085, 0.0695, 0.7591], [0.3451, 0.5443, 0.5543, 0.4877, 0.8687], [0.0649, 0.2948, 0.9568, 0.5756, 0.9486], [0.5433, 0.6607, 0.665, 0.0706, 0.2418], [0.6044, 0.9669, 0.3195, 0.6394, 0.4144], [0.1204, 0.6871, 0.7121, 0.2089, 0.1673]]]], 2, Con56346), 
LSub4527 = subtract_layer(Min64456,Con56346, Sub4527), 
LRes81965 = reshape_layer(Sub4527, [6, 8, 1], Res81965), 
LCon22669 = concatenate_layer([Res81965,[[[[0.4659], [0.0741], [0.4143], [0.8965], [0.1812], [0.4054], [0.6853], [0.3246]], [[0.7129], [0.8813], [0.5817], [0.2888], [0.9454], [0.1112], [0.475], [0.314]], [[0.6815], [0.3662], [0.2906], [0.8057], [0.9614], [0.9733], [0.7381], [0.9605]], [[0.0738], [0.2266], [0.0991], [0.4081], [0.6784], [0.8795], [0.7735], [0.9837]], [[0.4007], [0.1428], [0.6178], [0.4014], [0.2409], [0.2547], [0.4785], [0.7601]], [[0.1442], [0.1113], [0.2259], [0.7495], [0.1237], [0.1519], [0.0175], [0.1454]]]]], 3, Con22669), 
LPRe11374 = prelu_layer([[[[0.4916, 0.5625]]]], [[[0.0183, 0.3774]]], PRe11374), 
LZer47254 = zero_padding2D_layer(PRe11374, 5, 0, 7, 0, Zer47254), 
LSub57563 = subtract_layer(Con22669,Zer47254, Sub57563), 
exec_layers([LDot48483,LRes12161,LPRe2001,LRes43570,LRes69962,LDot13407,LZer49397,LSub64828,LRes72460,LUp_12897,LCon86089,LMin64456,LSep61983,LZer72440,LCon56346,LSub4527,LRes81965,LCon22669,LPRe11374,LZer47254,LSub57563],["Dot48483","Res12161","PRe2001","Res43570","Res69962","Dot13407","Zer49397","Sub64828","Res72460","Up_12897","Con86089","Min64456","Sep61983","Zer72440","Con56346","Sub4527","Res81965","Con22669","PRe11374","Zer47254","Sub57563"],Sub57563,"Sub57563")

Actual (Unparsed): [[[[0.0000000, 0.4659000], [-0.0657001, 0.0741000], [0.0000000, 0.4143000], [-1.0837000, 0.8965000], [-0.9643000, 0.1812000], [-0.9942000, 0.4054000], [-0.0695000, 0.6853000], [-0.7591000, 0.3246000]], [[0.0000000, 0.7129000], [-0.0657001, 0.8813000], [0.0000000, 0.5817000], [-0.4651000, 0.2888000], [-0.5443000, 0.9454000], [-1.1400000, 0.1112000], [-0.4877000, 0.4750000], [-0.8687000, 0.3140000]], [[-0.7144000, 0.6815000], [0.0000000, 0.3662000], [0.0000000, 0.2906000], [-0.1153000, 0.8057000], [-0.2948000, 0.9614000], [-0.9568000, 0.9733000], [-0.5756000, 0.7381000], [-0.9486000, 0.9605000]], [[-0.7144000, 0.0738000], [0.0000000, 0.2266000], [0.0000000, 0.0991000], [-0.5937000, 0.4081000], [-0.6607000, 0.6784000], [-0.6650000, 0.8795000], [-0.0706000, 0.7735000], [-0.2418000, 0.9837000]], [[-1.0109223, 0.4007000], [-0.3416876, 0.1428000], [-0.7283970, 0.6178000], [-0.6044000, 0.4014000], [-1.3694000, 0.2409000], [-0.3195000, 0.2547000], [-0.6394000, 0.4785000], [-0.4144000, 0.7601000]], [[-0.9452770, 0.1442000], [-0.3197015, 0.1113000], [-0.7002487, 0.2259000], [0.1212000, 0.7495000], [-1.0896000, 0.1237000], [-0.3344001, 0.1519000], [-0.0578000, 0.0175000], [-0.5864000, -0.4171000]]]]

Expected (Unparsed): [[[[0,0.4659],[-0.06570000000000009,0.0741],[0,0.4143],[-1.0836999999999999,0.8965],[-0.9643,0.1812],[-0.9942,0.4054],[-0.0695,0.6853],[-0.7591,0.3246]],[[0,0.7129],[-0.06570000000000009,0.8813],[0,0.5817],[-0.4651,0.2888],[-0.5443,0.9454],[-1.1400000000000001,0.1112],[-0.4877,0.475],[-0.8687,0.314]],[[-0.7144,0.6815],[0,0.3662],[0,0.2906],[-0.1153,0.8057],[-0.2948,0.9614],[-0.9568,0.9733],[-0.5756,0.7381],[-0.9486,0.9605]],[[-0.7144,0.0738],[0,0.2266],[0,0.0991],[-0.5937,0.4081],[-0.6607,0.6784],[-0.665,0.8795],[-0.0706,0.7735],[-0.2418,0.9837]],[[-1.010922363432,0.4007],[-0.34168761705199996,0.1428],[-0.728397023968,0.6178],[-0.6044,0.4014],[-1.3694,0.2409],[-0.3195,0.2547],[-0.6394,0.4785],[-0.4144,0.7601]],[[-0.945276986992,0.1442],[-0.319701479816,0.1113],[-0.700248657792,0.2259],[0.12119999999999999,0.7495],[-1.0896,0.1237],[-0.3343999999999999,0.1519],[-0.05779999999999999,0.0175],[-0.5864,-0.4171]]]]

Actual:   [[[[0, 0.4659], [-0.0657, 0.0741], [0, 0.4143], [-1.0837, 0.8965], [-0.9643, 0.1812], [-0.9942, 0.4054], [-0.0695, 0.6853], [-0.7591, 0.3246]], [[0, 0.7129], [-0.0657, 0.8813], [0, 0.5817], [-0.4651, 0.2888], [-0.5443, 0.9454], [-1.14, 0.1112], [-0.4877, 0.475], [-0.8687, 0.314]], [[-0.7144, 0.6815], [0, 0.3662], [0, 0.2906], [-0.1153, 0.8057], [-0.2948, 0.9614], [-0.9568, 0.9733], [-0.5756, 0.7381], [-0.9486, 0.9605]], [[-0.7144, 0.0738], [0, 0.2266], [0, 0.0991], [-0.5937, 0.4081], [-0.6607, 0.6784], [-0.665, 0.8795], [-0.0706, 0.7735], [-0.2418, 0.9837]], [[-1.0109, 0.4007], [-0.3416, 0.1428], [-0.7283, 0.6178], [-0.6044, 0.4014], [-1.3694, 0.2409], [-0.3195, 0.2547], [-0.6394, 0.4785], [-0.4144, 0.7601]], [[-0.9452, 0.1442], [-0.3197, 0.1113], [-0.7002, 0.2259], [0.1212, 0.7495], [-1.0896, 0.1237], [-0.3344, 0.1519], [-0.0578, 0.0175], [-0.5864, -0.4171]]]]

Expected: [[[[0, 0.4659], [-0.0657, 0.0741], [0, 0.4143], [-1.0836, 0.8965], [-0.9643, 0.1812], [-0.9942, 0.4054], [-0.0695, 0.6853], [-0.7591, 0.3246]], [[0, 0.7129], [-0.0657, 0.8813], [0, 0.5817], [-0.4651, 0.2888], [-0.5443, 0.9454], [-1.14, 0.1112], [-0.4877, 0.475], [-0.8687, 0.314]], [[-0.7144, 0.6815], [0, 0.3662], [0, 0.2906], [-0.1153, 0.8057], [-0.2948, 0.9614], [-0.9568, 0.9733], [-0.5756, 0.7381], [-0.9486, 0.9605]], [[-0.7144, 0.0738], [0, 0.2266], [0, 0.0991], [-0.5937, 0.4081], [-0.6607, 0.6784], [-0.665, 0.8795], [-0.0706, 0.7735], [-0.2418, 0.9837]], [[-1.0109, 0.4007], [-0.3416, 0.1428], [-0.7283, 0.6178], [-0.6044, 0.4014], [-1.3694, 0.2409], [-0.3195, 0.2547], [-0.6394, 0.4785], [-0.4144, 0.7601]], [[-0.9452, 0.1442], [-0.3197, 0.1113], [-0.7002, 0.2259], [0.1212, 0.7495], [-1.0896, 0.1237], [-0.3343, 0.1519], [-0.0577, 0.0175], [-0.5864, -0.4171]]]]