import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Zer82769 = tf.keras.layers.Input(shape=([1, 3, 1]))
in0Sim95255 = tf.keras.layers.Input(shape=([1, 2]))
in0Con83353 = tf.keras.layers.Input(shape=([14]))

Zer82769 = keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name = 'Zer82769', )(in0Zer82769)
Res81866 = keras.layers.Reshape((3, 5), name = 'Res81866', )(Zer82769)
Fla9547 = keras.layers.Flatten(name = 'Fla9547', )(Res81866)
Sim95255 = keras.layers.SimpleRNN(1,name = 'Sim95255', )(in0Sim95255)
Con83353 = keras.layers.Concatenate(axis=1, name = 'Con83353', )([Sim95255,in0Con83353])
Mul96393 = keras.layers.Multiply(name = 'Mul96393', )([Fla9547,Con83353])
Res65767 = keras.layers.Reshape((15, 1), name = 'Res65767', )(Mul96393)
PRe41186 = keras.layers.PReLU(name = 'PRe41186', )(Res65767)
model = tf.keras.models.Model(inputs=[in0Zer82769,in0Sim95255,in0Con83353], outputs=PRe41186)
w = model.get_layer('Sim95255').get_weights() 
w[0] = np.array([[1], [8]])
w[1] = np.array([[7]])
w[2] = np.array([4])
model.get_layer('Sim95255').set_weights(w) 
w = model.get_layer('PRe41186').get_weights() 
w[0] = np.array([[0.7248], [0.8438], [0.4954], [0.6585], [0.9301], [0.2861], [0.9933], [0.4953], [0.7014], [0.3272], [0.3091], [0.9397], [0.6733], [0.799], [0.3326]])
model.get_layer('PRe41186').set_weights(w) 
in0Zer82769 = tf.constant([[[[1.0439], [1.865], [1.991]]]])
in0Sim95255 = tf.constant([[[3, 9]]])
in0Con83353 = tf.constant([[0.7677, 0.2581, 0.401, 0.866, 0.8704, 0.1277, 0.0897, 0.9896, 0.5885, 0.6437, 0.2382, 0.4253, 0.7479, 0.1749]])
print (np.array2string(model.predict([in0Zer82769,in0Sim95255,in0Con83353],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='PRe41186.png')

LZer82769 = zero_padding2D_layer([[[[1.0439], [1.865], [1.991]]]], 1, 1, 1, 1, Zer82769), 
LRes81866 = reshape_layer(Zer82769, [3, 5], Res81866), 
LFla9547 = flatten_layer(Res81866, Fla9547), 
LSim95255 = simple_rnn_layer([[[3, 9]]],[[1], [8]],[[7]],[4], Sim95255), 
LCon83353 = concatenate_layer([Sim95255,[[0.7677, 0.2581, 0.401, 0.866, 0.8704, 0.1277, 0.0897, 0.9896, 0.5885, 0.6437, 0.2382, 0.4253, 0.7479, 0.1749]]], 1, Con83353), 
LMul96393 = multiply_layer([Fla9547,Con83353], Mul96393), 
LRes65767 = reshape_layer(Mul96393, [15, 1], Res65767), 
LPRe41186 = prelu_layer(Res65767, [[0.7248], [0.8438], [0.4954], [0.6585], [0.9301], [0.2861], [0.9933], [0.4953], [0.7014], [0.3272], [0.3091], [0.9397], [0.6733], [0.799], [0.3326]], PRe41186), 
exec_layers([LZer82769,LRes81866,LFla9547,LSim95255,LCon83353,LMul96393,LRes65767,LPRe41186],["Zer82769","Res81866","Fla9547","Sim95255","Con83353","Mul96393","Res65767","PRe41186"],PRe41186,"PRe41186")

Actual (Unparsed): [[[0.0000000], [0.0000000], [0.0000000], [0.0000000], [0.0000000], [0.0000000], [0.1333060], [0.1672905], [1.9702937], [0.0000000], [0.0000000], [0.0000000], [0.0000000], [0.0000000], [0.0000000]]]

Expected (Unparsed): [[[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.13330603000000002],[0.1672905],[1.9702936000000002],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0]]]

Actual:   [[[0], [0], [0], [0], [0], [0], [0.1334], [0.1673], [1.9703], [0], [0], [0], [0], [0], [0]]]

Expected: [[[0], [0], [0], [0], [0], [0], [0.1334], [0.1673], [1.9703], [0], [0], [0], [0], [0], [0]]]