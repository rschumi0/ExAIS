import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Dot15 = tf.keras.layers.Input(shape=([2]))
in1Dot15 = tf.keras.layers.Input(shape=([2]))
in0Ave28512 = tf.keras.layers.Input(shape=([1, 2, 1]))
in1Ave28512 = tf.keras.layers.Input(shape=([1, 2, 1]))
in0Con1859 = tf.keras.layers.Input(shape=([70]))
in0Max37216 = tf.keras.layers.Input(shape=([2, 1, 1, 2]))
in1Max37216 = tf.keras.layers.Input(shape=([2, 1, 1, 2]))
in0Dot9986 = tf.keras.layers.Input(shape=([3]))
in1Dot9986 = tf.keras.layers.Input(shape=([3]))
in0Con22658 = tf.keras.layers.Input(shape=([71]))

Dot15 = keras.layers.Dot(axes=(1, 1), name = 'Dot15', )([in0Dot15,in1Dot15])
Thr12278 = keras.layers.ThresholdedReLU(theta=3.9827506368805414, name = 'Thr12278', )(Dot15)
Res35326 = keras.layers.Reshape((1, 1), name = 'Res35326', )(Thr12278)
Res17018 = keras.layers.Reshape((1, 1, 1), name = 'Res17018', )(Res35326)
Zer85788 = keras.layers.ZeroPadding2D(padding=((0, 0), (1, 0)), name = 'Zer85788', )(Res17018)
Ave28512 = keras.layers.Average(name = 'Ave28512', )([in0Ave28512,in1Ave28512])
Sub36811 = keras.layers.Subtract(name = 'Sub36811', )([Zer85788,Ave28512])
Res87835 = keras.layers.Reshape((1, 2), name = 'Res87835', )(Sub36811)
Fla83347 = keras.layers.Flatten(name = 'Fla83347', )(Res87835)
Con1859 = keras.layers.Concatenate(axis=1, name = 'Con1859', )([Fla83347,in0Con1859])
Max37216 = keras.layers.Maximum(name = 'Max37216', )([in0Max37216,in1Max37216])
Zer4915 = keras.layers.ZeroPadding3D(padding=((1, 1), (1, 1), (1, 1)), name = 'Zer4915', )(Max37216)
Res91708 = keras.layers.Reshape((4, 3, 6), name = 'Res91708', )(Zer4915)
Res53357 = keras.layers.Reshape((4, 18), name = 'Res53357', )(Res91708)
Fla43207 = keras.layers.Flatten(name = 'Fla43207', )(Res53357)
Dot9986 = keras.layers.Dot(axes=(1, 1), name = 'Dot9986', )([in0Dot9986,in1Dot9986])
Bat5442 = keras.layers.BatchNormalization(axis=1, epsilon=0.7312333910307945,  name = 'Bat5442', )(Dot9986)
Con22658 = keras.layers.Concatenate(axis=1, name = 'Con22658', )([Bat5442,in0Con22658])
Min73416 = keras.layers.Minimum(name = 'Min73416', )([Fla43207,Con22658])
Max41567 = keras.layers.Maximum(name = 'Max41567', )([Con1859,Min73416])
model = tf.keras.models.Model(inputs=[in0Dot15,in1Dot15,in0Ave28512,in1Ave28512,in0Con1859,in0Max37216,in1Max37216,in0Dot9986,in1Dot9986,in0Con22658], outputs=Max41567)
w = model.get_layer('Bat5442').get_weights() 
w[0] = np.array([0.9128])
w[1] = np.array([0.9916])
w[2] = np.array([0.5036])
w[3] = np.array([0.4239])
model.get_layer('Bat5442').set_weights(w) 
in0Dot15 = tf.constant([[0.1144, 0.0388]])
in1Dot15 = tf.constant([[0.6953, 0.836]])
in0Ave28512 = tf.constant([[[[0.975], [0.1975]]]])
in1Ave28512 = tf.constant([[[[0.9044], [0.1379]]]])
in0Con1859 = tf.constant([[0.2988, 0.3249, 0.0901, 0.4331, 0.1335, 0.5504, 0.4512, 0.182, 0.3656, 0.6518, 0.9605, 0.5562, 0.6419, 0.0732, 0.0526, 0.4301, 0.9392, 0.2664, 0.4785, 0.8838, 0.2894, 0.3118, 0.9137, 0.9789, 0.9602, 0.4476, 0.6902, 0.9194, 0.3689, 0.9622, 0.8177, 0.4627, 0.8539, 0.1011, 0.5702, 0.5276, 0.052, 0.8695, 0.3068, 0.5681, 0.039, 0.8673, 0.865, 0.429, 0.5777, 0.5848, 0.8239, 0.0823, 0.7891, 0.9097, 0.1652, 0.8513, 0.3113, 0.9466, 0.3804, 0.477, 0.5448, 0.635, 0.8669, 0.654, 0.8957, 0.067, 0.5381, 0.4657, 0.0225, 0.8515, 0.9311, 0.4928, 0.9686, 0.3501]])
in0Max37216 = tf.constant([[[[[0.7216, 0.6751]]], [[[0.5067, 0.6972]]]]])
in1Max37216 = tf.constant([[[[[0.5385, 0.5739]]], [[[0.2683, 0.6628]]]]])
in0Dot9986 = tf.constant([[0.4159, 0.8607, 0.0956]])
in1Dot9986 = tf.constant([[0.2273, 0.8641, 0.704]])
in0Con22658 = tf.constant([[0.8289, 0.7452, 0.4176, 0.0448, 0.078, 0.5516, 0.8179, 0.415, 0.1238, 0.7408, 0.2634, 0.6585, 0.8541, 0.8085, 0.821, 0.2286, 0.4602, 0.1932, 0.9369, 0.6711, 0.3769, 0.5748, 0.2804, 0.6995, 0.5188, 0.3166, 0.7422, 0.4123, 0.8555, 0.9483, 0.9432, 0.6616, 0.8786, 0.5038, 0.0102, 0.0186, 0.0522, 0.7379, 0.3043, 0.0033, 0.9113, 0.7262, 0.2515, 0.2447, 0.073, 0.1236, 0.4164, 0.0216, 0.0621, 0.4001, 0.485, 0.1968, 0.5144, 0.707, 0.8791, 0.9023, 0.0204, 0.6305, 0.3098, 0.4591, 0.5967, 0.9882, 0.6976, 0.9233, 0.5346, 0.9914, 0.5358, 0.3363, 0.1616, 0.5528, 0.9524]])
print (np.array2string(model.predict([in0Dot15,in1Dot15,in0Ave28512,in1Ave28512,in0Con1859,in0Max37216,in1Max37216,in0Dot9986,in1Dot9986,in0Con22658],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Max41567.png')

LDot15 = dot_layer([[0.1144, 0.0388]], [[0.6953, 0.836]], 1, 1, Dot15), 
LThr12278 = thresholded_relu_layer(Dot15, 3.9827506368805414, Thr12278), 
LRes35326 = reshape_layer(Thr12278, [1, 1], Res35326), 
LRes17018 = reshape_layer(Res35326, [1, 1, 1], Res17018), 
LZer85788 = zero_padding2D_layer(Res17018, 0, 0, 1, 0, Zer85788), 
LAve28512 = average_layer([[[[[0.975], [0.1975]]]], [[[[0.9044], [0.1379]]]]], Ave28512), 
LSub36811 = subtract_layer(Zer85788,Ave28512, Sub36811), 
LRes87835 = reshape_layer(Sub36811, [1, 2], Res87835), 
LFla83347 = flatten_layer(Res87835, Fla83347), 
LCon1859 = concatenate_layer([Fla83347,[[0.2988, 0.3249, 0.0901, 0.4331, 0.1335, 0.5504, 0.4512, 0.182, 0.3656, 0.6518, 0.9605, 0.5562, 0.6419, 0.0732, 0.0526, 0.4301, 0.9392, 0.2664, 0.4785, 0.8838, 0.2894, 0.3118, 0.9137, 0.9789, 0.9602, 0.4476, 0.6902, 0.9194, 0.3689, 0.9622, 0.8177, 0.4627, 0.8539, 0.1011, 0.5702, 0.5276, 0.052, 0.8695, 0.3068, 0.5681, 0.039, 0.8673, 0.865, 0.429, 0.5777, 0.5848, 0.8239, 0.0823, 0.7891, 0.9097, 0.1652, 0.8513, 0.3113, 0.9466, 0.3804, 0.477, 0.5448, 0.635, 0.8669, 0.654, 0.8957, 0.067, 0.5381, 0.4657, 0.0225, 0.8515, 0.9311, 0.4928, 0.9686, 0.3501]]], 1, Con1859), 
LMax37216 = maximum_layer([[[[[[0.7216, 0.6751]]], [[[0.5067, 0.6972]]]]], [[[[[0.5385, 0.5739]]], [[[0.2683, 0.6628]]]]]], Max37216), 
LZer4915 = zero_padding3D_layer(Max37216, 1, 1, 1, 1, 1, 1, Zer4915), 
LRes91708 = reshape_layer(Zer4915, [4, 3, 6], Res91708), 
LRes53357 = reshape_layer(Res91708, [4, 18], Res53357), 
LFla43207 = flatten_layer(Res53357, Fla43207), 
LDot9986 = dot_layer([[0.4159, 0.8607, 0.0956]], [[0.2273, 0.8641, 0.704]], 1, 1, Dot9986), 
LBat5442 = batch_normalization_layer(Dot9986, 1, 0.7312333910307945, [0.9128], [0.9916], [0.5036], [0.4239], Bat5442), 
LCon22658 = concatenate_layer([Bat5442,[[0.8289, 0.7452, 0.4176, 0.0448, 0.078, 0.5516, 0.8179, 0.415, 0.1238, 0.7408, 0.2634, 0.6585, 0.8541, 0.8085, 0.821, 0.2286, 0.4602, 0.1932, 0.9369, 0.6711, 0.3769, 0.5748, 0.2804, 0.6995, 0.5188, 0.3166, 0.7422, 0.4123, 0.8555, 0.9483, 0.9432, 0.6616, 0.8786, 0.5038, 0.0102, 0.0186, 0.0522, 0.7379, 0.3043, 0.0033, 0.9113, 0.7262, 0.2515, 0.2447, 0.073, 0.1236, 0.4164, 0.0216, 0.0621, 0.4001, 0.485, 0.1968, 0.5144, 0.707, 0.8791, 0.9023, 0.0204, 0.6305, 0.3098, 0.4591, 0.5967, 0.9882, 0.6976, 0.9233, 0.5346, 0.9914, 0.5358, 0.3363, 0.1616, 0.5528, 0.9524]]], 1, Con22658), 
LMin73416 = minimum_layer([Fla43207,Con22658], Min73416), 
LMax41567 = maximum_layer([Con1859,Min73416], Max41567), 
exec_layers([LDot15,LThr12278,LRes35326,LRes17018,LZer85788,LAve28512,LSub36811,LRes87835,LFla83347,LCon1859,LMax37216,LZer4915,LRes91708,LRes53357,LFla43207,LDot9986,LBat5442,LCon22658,LMin73416,LMax41567],["Dot15","Thr12278","Res35326","Res17018","Zer85788","Ave28512","Sub36811","Res87835","Fla83347","Con1859","Max37216","Zer4915","Res91708","Res53357","Fla43207","Dot9986","Bat5442","Con22658","Min73416","Max41567"],Max41567,"Max41567")

Actual (Unparsed): [[0.0000000, 0.0000000, 0.2988000, 0.3249000, 0.0901000, 0.4331000, 0.1335000, 0.5504000, 0.4512000, 0.1820000, 0.3656000, 0.6518000, 0.9605000, 0.5562000, 0.6419000, 0.0732000, 0.0526000, 0.4301000, 0.9392000, 0.2664000, 0.4785000, 0.8838000, 0.2894000, 0.3118000, 0.9137000, 0.9789000, 0.9602000, 0.6751000, 0.6902000, 0.9194000, 0.3689000, 0.9622000, 0.8177000, 0.4627000, 0.8539000, 0.1011000, 0.5702000, 0.5276000, 0.0520000, 0.8695000, 0.3068000, 0.5681000, 0.0390000, 0.8673000, 0.8650000, 0.4290000, 0.5777000, 0.5848000, 0.8239000, 0.0823000, 0.7891000, 0.9097000, 0.1652000, 0.8513000, 0.3113000, 0.9466000, 0.3804000, 0.4770000, 0.5448000, 0.6350000, 0.8669000, 0.6540000, 0.8957000, 0.0670000, 0.5381000, 0.4657000, 0.0225000, 0.8515000, 0.9311000, 0.4928000, 0.9686000, 0.3501000]]

Expected (Unparsed): [[0,0,0.2988,0.3249,0.0901,0.4331,0.1335,0.5504,0.4512,0.182,0.3656,0.6518,0.9605,0.5562,0.6419,0.0732,0.0526,0.4301,0.9392,0.2664,0.4785,0.8838,0.2894,0.3118,0.9137,0.9789,0.9602,0.6751,0.6902,0.9194,0.3689,0.9622,0.8177,0.4627,0.8539,0.1011,0.5702,0.5276,0.052,0.8695,0.3068,0.5681,0.039,0.8673,0.865,0.429,0.5777,0.5848,0.8239,0.0823,0.7891,0.9097,0.1652,0.8513,0.3113,0.9466,0.3804,0.477,0.5448,0.635,0.8669,0.654,0.8957,0.067,0.5381,0.4657,0.0225,0.8515,0.9311,0.4928,0.9686,0.3501]]

Actual:   [[0, 0, 0.2988, 0.3249, 0.0901, 0.4331, 0.1335, 0.5504, 0.4512, 0.182, 0.3656, 0.6518, 0.9605, 0.5562, 0.6419, 0.0732, 0.0526, 0.4301, 0.9392, 0.2664, 0.4785, 0.8838, 0.2894, 0.3118, 0.9137, 0.9789, 0.9602, 0.6751, 0.6902, 0.9194, 0.3689, 0.9622, 0.8177, 0.4627, 0.8539, 0.1011, 0.5702, 0.5276, 0.052, 0.8695, 0.3068, 0.5681, 0.039, 0.8673, 0.865, 0.429, 0.5777, 0.5848, 0.8239, 0.0823, 0.7891, 0.9097, 0.1652, 0.8513, 0.3113, 0.9466, 0.3804, 0.477, 0.5448, 0.635, 0.8669, 0.654, 0.8957, 0.067, 0.5381, 0.4657, 0.0225, 0.8515, 0.9311, 0.4928, 0.9686, 0.3501]]

Expected: [[0, 0, 0.2988, 0.3249, 0.0901, 0.4331, 0.1335, 0.5504, 0.4512, 0.182, 0.3656, 0.6518, 0.9605, 0.5562, 0.6419, 0.0732, 0.0526, 0.4301, 0.9392, 0.2664, 0.4785, 0.8838, 0.2894, 0.3118, 0.9137, 0.9789, 0.9602, 0.6751, 0.6902, 0.9194, 0.3689, 0.9622, 0.8177, 0.4627, 0.8539, 0.1011, 0.5702, 0.5276, 0.052, 0.8695, 0.3068, 0.5681, 0.039, 0.8673, 0.865, 0.429, 0.5777, 0.5848, 0.8239, 0.0823, 0.7891, 0.9097, 0.1652, 0.8513, 0.3113, 0.9466, 0.3804, 0.477, 0.5448, 0.635, 0.8669, 0.654, 0.8957, 0.067, 0.5381, 0.4657, 0.0225, 0.8515, 0.9311, 0.4928, 0.9686, 0.3501]]