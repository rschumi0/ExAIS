import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Ave11390 = tf.keras.layers.Input(shape=([1, 2, 1]))
in1Ave11390 = tf.keras.layers.Input(shape=([1, 2, 1]))
in0Con13422 = tf.keras.layers.Input(shape=([1, 2, 1]))
in0Min8893 = tf.keras.layers.Input(shape=([1, 1, 2]))
in1Min8893 = tf.keras.layers.Input(shape=([1, 1, 2]))
in0Sub17069 = tf.keras.layers.Input(shape=([2, 3, 3]))
in1Sub17069 = tf.keras.layers.Input(shape=([2, 3, 3]))

Ave11390 = keras.layers.Average(name = 'Ave11390', )([in0Ave11390,in1Ave11390])
Con13422 = keras.layers.Concatenate(axis=3, name = 'Con13422', )([Ave11390,in0Con13422])
Min8893 = keras.layers.Minimum(name = 'Min8893', )([in0Min8893,in1Min8893])
Zer48601 = keras.layers.ZeroPadding2D(padding=((0, 0), (1, 0)), name = 'Zer48601', )(Min8893)
Min92638 = keras.layers.Minimum(name = 'Min92638', )([Con13422,Zer48601])
Glo98643 = keras.layers.GlobalMaxPool2D(name = 'Glo98643', )(Min92638)
Res72804 = keras.layers.Reshape((2, 1), name = 'Res72804', )(Glo98643)
Res61613 = keras.layers.Reshape((2, 1, 1), name = 'Res61613', )(Res72804)
Res16782 = keras.layers.Reshape((2, 1, 1, 1), name = 'Res16782', )(Res61613)
Zer46326 = keras.layers.ZeroPadding3D(padding=((0, 0), (5, 0), (5, 0)), name = 'Zer46326', )(Res16782)
Sub17069 = keras.layers.Subtract(name = 'Sub17069', )([in0Sub17069,in1Sub17069])
Res22042 = keras.layers.Reshape((2, 3, 3, 1), name = 'Res22042', )(Sub17069)
Up_5964 = keras.layers.UpSampling3D(size=(1, 2, 2), name = 'Up_5964', )(Res22042)
Add87570 = keras.layers.Add(name = 'Add87570', )([Zer46326,Up_5964])
model = tf.keras.models.Model(inputs=[in0Ave11390,in1Ave11390,in0Con13422,in0Min8893,in1Min8893,in0Sub17069,in1Sub17069], outputs=Add87570)
in0Ave11390 = tf.constant([[[[0.6556], [0.2056]]]])
in1Ave11390 = tf.constant([[[[0.1916], [0.6721]]]])
in0Con13422 = tf.constant([[[[0.1473], [0.0473]]]])
in0Min8893 = tf.constant([[[[0.4976, 0.783]]]])
in1Min8893 = tf.constant([[[[0.8542, 0.6664]]]])
in0Sub17069 = tf.constant([[[[0.5623, 0.6234, 0.6405], [0.7143, 0.9089, 0.9108], [0.176, 0.0748, 0.2665]], [[0.3894, 0.7046, 0.8725], [0.9847, 0.7548, 0.605], [0.058, 0.9156, 0.9621]]]])
in1Sub17069 = tf.constant([[[[0.465, 0.037, 0.9459], [0.2063, 0.2118, 0.2342], [0.7079, 0.4258, 0.2353]], [[0.2203, 0.3307, 0.0531], [0.926, 0.6709, 0.9638], [0.3975, 0.5536, 0.35]]]])
print (np.array2string(model.predict([in0Ave11390,in1Ave11390,in0Con13422,in0Min8893,in1Min8893,in0Sub17069,in1Sub17069],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Add87570.png')

LAve11390 = average_layer([[[[[0.6556], [0.2056]]]], [[[[0.1916], [0.6721]]]]], Ave11390), 
LCon13422 = concatenate_layer([Ave11390,[[[[0.1473], [0.0473]]]]], 3, Con13422), 
LMin8893 = minimum_layer([[[[[0.4976, 0.783]]]], [[[[0.8542, 0.6664]]]]], Min8893), 
LZer48601 = zero_padding2D_layer(Min8893, 0, 0, 1, 0, Zer48601), 
LMin92638 = minimum_layer([Con13422,Zer48601], Min92638), 
LGlo98643 = global_max_pool2D_layer(Min92638, Glo98643), 
LRes72804 = reshape_layer(Glo98643, [2, 1], Res72804), 
LRes61613 = reshape_layer(Res72804, [2, 1, 1], Res61613), 
LRes16782 = reshape_layer(Res61613, [2, 1, 1, 1], Res16782), 
LZer46326 = zero_padding3D_layer(Res16782, 0, 0, 5, 0, 5, 0, Zer46326), 
LSub17069 = subtract_layer([[[[0.5623, 0.6234, 0.6405], [0.7143, 0.9089, 0.9108], [0.176, 0.0748, 0.2665]], [[0.3894, 0.7046, 0.8725], [0.9847, 0.7548, 0.605], [0.058, 0.9156, 0.9621]]]], [[[[0.465, 0.037, 0.9459], [0.2063, 0.2118, 0.2342], [0.7079, 0.4258, 0.2353]], [[0.2203, 0.3307, 0.0531], [0.926, 0.6709, 0.9638], [0.3975, 0.5536, 0.35]]]], Sub17069), 
LRes22042 = reshape_layer(Sub17069, [2, 3, 3, 1], Res22042), 
LUp_5964 = up_sampling3D_layer(Res22042, 1, 2, 2, Up_5964), 
LAdd87570 = add_layer([Zer46326,Up_5964], Add87570), 
exec_layers([LAve11390,LCon13422,LMin8893,LZer48601,LMin92638,LGlo98643,LRes72804,LRes61613,LRes16782,LZer46326,LSub17069,LRes22042,LUp_5964,LAdd87570],["Ave11390","Con13422","Min8893","Zer48601","Min92638","Glo98643","Res72804","Res61613","Res16782","Zer46326","Sub17069","Res22042","Up_5964","Add87570"],Add87570,"Add87570")

Actual (Unparsed): [[[[[0.0973000], [0.0973000], [0.5864000], [0.5864000], [-0.3054000], [-0.3054000]], [[0.0973000], [0.0973000], [0.5864000], [0.5864000], [-0.3054000], [-0.3054000]], [[0.5080000], [0.5080000], [0.6971000], [0.6971000], [0.6766000], [0.6766000]], [[0.5080000], [0.5080000], [0.6971000], [0.6971000], [0.6766000], [0.6766000]], [[-0.5319000], [-0.5319000], [-0.3510000], [-0.3510000], [0.0312000], [0.0312000]], [[-0.5319000], [-0.5319000], [-0.3510000], [-0.3510000], [0.0312000], [0.4700500]]], [[[0.1691000], [0.1691000], [0.3739000], [0.3739000], [0.8194000], [0.8194000]], [[0.1691000], [0.1691000], [0.3739000], [0.3739000], [0.8194000], [0.8194000]], [[0.0587000], [0.0587000], [0.0839000], [0.0839000], [-0.3588000], [-0.3588000]], [[0.0587000], [0.0587000], [0.0839000], [0.0839000], [-0.3588000], [-0.3588000]], [[-0.3395000], [-0.3395000], [0.3620000], [0.3620000], [0.6121000], [0.6121000]], [[-0.3395000], [-0.3395000], [0.3620000], [0.3620000], [0.6121000], [0.6594000]]]]]

Expected (Unparsed): [[[[[0.0973],[0.0973],[0.5863999999999999],[0.5863999999999999],[-0.3054],[-0.3054]],[[0.0973],[0.0973],[0.5863999999999999],[0.5863999999999999],[-0.3054],[-0.3054]],[[0.508],[0.508],[0.6971],[0.6971],[0.6766000000000001],[0.6766000000000001]],[[0.508],[0.508],[0.6971],[0.6971],[0.6766000000000001],[0.6766000000000001]],[[-0.5319],[-0.5319],[-0.351],[-0.351],[0.031200000000000006],[0.031200000000000006]],[[-0.5319],[-0.5319],[-0.351],[-0.351],[0.031200000000000006],[0.47005]]],[[[0.16910000000000003],[0.16910000000000003],[0.3739],[0.3739],[0.8194],[0.8194]],[[0.16910000000000003],[0.16910000000000003],[0.3739],[0.3739],[0.8194],[0.8194]],[[0.058699999999999974],[0.058699999999999974],[0.08389999999999997],[0.08389999999999997],[-0.3588],[-0.3588]],[[0.058699999999999974],[0.058699999999999974],[0.08389999999999997],[0.08389999999999997],[-0.3588],[-0.3588]],[[-0.3395],[-0.3395],[0.362],[0.362],[0.6121],[0.6121]],[[-0.3395],[-0.3395],[0.362],[0.362],[0.6121],[0.6594]]]]]

Actual:   [[[[[0.0973], [0.0973], [0.5864], [0.5864], [-0.3054], [-0.3054]], [[0.0973], [0.0973], [0.5864], [0.5864], [-0.3054], [-0.3054]], [[0.508], [0.508], [0.6971], [0.6971], [0.6766], [0.6766]], [[0.508], [0.508], [0.6971], [0.6971], [0.6766], [0.6766]], [[-0.5319], [-0.5319], [-0.351], [-0.351], [0.0312], [0.0312]], [[-0.5319], [-0.5319], [-0.351], [-0.351], [0.0312], [0.4701]]], [[[0.1691], [0.1691], [0.3739], [0.3739], [0.8194], [0.8194]], [[0.1691], [0.1691], [0.3739], [0.3739], [0.8194], [0.8194]], [[0.0587], [0.0587], [0.0839], [0.0839], [-0.3588], [-0.3588]], [[0.0587], [0.0587], [0.0839], [0.0839], [-0.3588], [-0.3588]], [[-0.3395], [-0.3395], [0.362], [0.362], [0.6121], [0.6121]], [[-0.3395], [-0.3395], [0.362], [0.362], [0.6121], [0.6594]]]]]

Expected: [[[[[0.0973], [0.0973], [0.5864], [0.5864], [-0.3054], [-0.3054]], [[0.0973], [0.0973], [0.5864], [0.5864], [-0.3054], [-0.3054]], [[0.508], [0.508], [0.6971], [0.6971], [0.6767], [0.6767]], [[0.508], [0.508], [0.6971], [0.6971], [0.6767], [0.6767]], [[-0.5319], [-0.5319], [-0.351], [-0.351], [0.0313], [0.0313]], [[-0.5319], [-0.5319], [-0.351], [-0.351], [0.0313], [0.4701]]], [[[0.1692], [0.1692], [0.3739], [0.3739], [0.8194], [0.8194]], [[0.1692], [0.1692], [0.3739], [0.3739], [0.8194], [0.8194]], [[0.0587], [0.0587], [0.0839], [0.0839], [-0.3588], [-0.3588]], [[0.0587], [0.0587], [0.0839], [0.0839], [-0.3588], [-0.3588]], [[-0.3395], [-0.3395], [0.362], [0.362], [0.6121], [0.6121]], [[-0.3395], [-0.3395], [0.362], [0.362], [0.6121], [0.6594]]]]]