import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Mul93992 = tf.keras.layers.Input(shape=([1, 1]))
in1Mul93992 = tf.keras.layers.Input(shape=([1, 1]))
in0Min17323 = tf.keras.layers.Input(shape=([1, 1, 1]))
in1Min17323 = tf.keras.layers.Input(shape=([1, 1, 1]))

Mul93992 = keras.layers.Multiply(name = 'Mul93992', )([in0Mul93992,in1Mul93992])
Loc62581 = keras.layers.LocallyConnected1D(4, (1),strides=(1), name = 'Loc62581', )(Mul93992)
Res93845 = keras.layers.Reshape((1, 4, 1), name = 'Res93845', )(Loc62581)
Min17323 = keras.layers.Minimum(name = 'Min17323', )([in0Min17323,in1Min17323])
Max92121 = keras.layers.MaxPool2D(pool_size=(1, 1), strides=(12, 1), padding='same', name = 'Max92121', )(Min17323)
Zer11976 = keras.layers.ZeroPadding2D(padding=((0, 0), (3, 0)), name = 'Zer11976', )(Max92121)
Add42147 = keras.layers.Add(name = 'Add42147', )([Res93845,Zer11976])
Bat20652 = keras.layers.BatchNormalization(axis=1, epsilon=0.9962955472119762,  name = 'Bat20652', )(Add42147)
model = tf.keras.models.Model(inputs=[in0Mul93992,in1Mul93992,in0Min17323,in1Min17323], outputs=Bat20652)
w = model.get_layer('Loc62581').get_weights() 
w[0] = np.array([[[0.0835, 0.918, 0.7455, 0.9881]]])
w[1] = np.array([[0, 0, 0, 0]])
model.get_layer('Loc62581').set_weights(w) 
w = model.get_layer('Bat20652').get_weights() 
w[0] = np.array([0.8996])
w[1] = np.array([0.5073])
w[2] = np.array([0.509])
w[3] = np.array([0.7799])
model.get_layer('Bat20652').set_weights(w) 
in0Mul93992 = tf.constant([[[0.1835]]])
in1Mul93992 = tf.constant([[[0.0775]]])
in0Min17323 = tf.constant([[[[0.7436]]]])
in1Min17323 = tf.constant([[[[0.622]]]])
print (np.array2string(model.predict([in0Mul93992,in1Mul93992,in0Min17323,in1Min17323],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Bat20652.png')

LMul93992 = multiply_layer([[[[0.1835]]], [[[0.0775]]]], Mul93992), 
LLoc62581 = locally_connected1D_layer(Mul93992, 1,[[[0.0835, 0.918, 0.7455, 0.9881]]],[[0, 0, 0, 0]], 1, Loc62581), 
LRes93845 = reshape_layer(Loc62581, [1, 4, 1], Res93845), 
LMin17323 = minimum_layer([[[[[0.7436]]]], [[[[0.622]]]]], Min17323), 
LMax92121 = max_pool2D_layer(Min17323, 1, 1, 12, 1, true, Max92121), 
LZer11976 = zero_padding2D_layer(Max92121, 0, 0, 3, 0, Zer11976), 
LAdd42147 = add_layer([Res93845,Zer11976], Add42147), 
LBat20652 = batch_normalization_layer(Add42147, 1, 0.9962955472119762, [0.8996], [0.5073], [0.509], [0.7799], Bat20652), 
exec_layers([LMul93992,LLoc62581,LRes93845,LMin17323,LMax92121,LZer11976,LAdd42147,LBat20652],["Mul93992","Loc62581","Res93845","Min17323","Max92121","Zer11976","Add42147","Bat20652"],Bat20652,"Bat20652")

Actual (Unparsed): [[[[0.1645263], [0.1725370], [0.1708811], [0.5930602]]]]

Expected (Unparsed): [[[[0.16452631991446243],[0.1725369775388576],[0.17088108965364712],[0.5930601679254108]]]]

Actual:   [[[[0.1646], [0.1726], [0.1709], [0.5931]]]]

Expected: [[[[0.1646], [0.1726], [0.1709], [0.5931]]]]