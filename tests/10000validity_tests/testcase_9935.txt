import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Mul34401 = tf.keras.layers.Input(shape=([1, 2, 1, 2]))
in1Mul34401 = tf.keras.layers.Input(shape=([1, 2, 1, 2]))

Mul34401 = keras.layers.Multiply(name = 'Mul34401', )([in0Mul34401,in1Mul34401])
Zer42118 = keras.layers.ZeroPadding3D(padding=((1, 1), (1, 1), (1, 1)), name = 'Zer42118', )(Mul34401)
Thr39922 = keras.layers.ThresholdedReLU(theta=0.7042391767649797, name = 'Thr39922', )(Zer42118)
Bat98221 = keras.layers.BatchNormalization(axis=1, epsilon=0.7937183710019325,  name = 'Bat98221', )(Thr39922)
model = tf.keras.models.Model(inputs=[in0Mul34401,in1Mul34401], outputs=Bat98221)
w = model.get_layer('Bat98221').get_weights() 
w[0] = np.array([0.9332, 0.1798, 0.9316])
w[1] = np.array([0.0117, 0.6775, 0.5574])
w[2] = np.array([0.1678, 0.0363, 0.4774])
w[3] = np.array([0.0746, 0.5614, 0.3087])
model.get_layer('Bat98221').set_weights(w) 
in0Mul34401 = tf.constant([[[[[0.0757, 0.1618]], [[0.3233, 0.5076]]]]])
in1Mul34401 = tf.constant([[[[[0.7403, 0.2934]], [[0.9675, 0.9865]]]]])
print (np.array2string(model.predict([in0Mul34401,in1Mul34401],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Bat98221.png')

LMul34401 = multiply_layer([[[[[[0.0757, 0.1618]], [[0.3233, 0.5076]]]]], [[[[[0.7403, 0.2934]], [[0.9675, 0.9865]]]]]], Mul34401), 
LZer42118 = zero_padding3D_layer(Mul34401, 1, 1, 1, 1, 1, 1, Zer42118), 
LThr39922 = thresholded_relu_layer(Zer42118, 0.7042391767649797, Thr39922), 
LBat98221 = batch_normalization_layer(Thr39922, 1, 0.7937183710019325, [0.9332, 0.1798, 0.9316], [0.0117, 0.6775, 0.5574], [0.1678, 0.0363, 0.4774], [0.0746, 0.5614, 0.3087], Bat98221), 
exec_layers([LMul34401,LZer42118,LThr39922,LBat98221],["Mul34401","Zer42118","Thr39922","Bat98221"],Bat98221,"Bat98221")

Actual (Unparsed): [[[[[-0.1563456, -0.1563456], [-0.1563456, -0.1563456], [-0.1563456, -0.1563456]], [[-0.1563456, -0.1563456], [-0.1563456, -0.1563456], [-0.1563456, -0.1563456]], [[-0.1563456, -0.1563456], [-0.1563456, -0.1563456], [-0.1563456, -0.1563456]], [[-0.1563456, -0.1563456], [-0.1563456, -0.1563456], [-0.1563456, -0.1563456]]], [[[0.6718933, 0.6718933], [0.6718933, 0.6718933], [0.6718933, 0.6718933]], [[0.6718933, 0.6718933], [0.6718933, 0.6718933], [0.6718933, 0.6718933]], [[0.6718933, 0.6718933], [0.6718933, 0.6718933], [0.6718933, 0.6718933]], [[0.6718933, 0.6718933], [0.6718933, 0.6718933], [0.6718933, 0.6718933]]], [[[0.1338169, 0.1338169], [0.1338169, 0.1338169], [0.1338169, 0.1338169]], [[0.1338169, 0.1338169], [0.1338169, 0.1338169], [0.1338169, 0.1338169]], [[0.1338169, 0.1338169], [0.1338169, 0.1338169], [0.1338169, 0.1338169]], [[0.1338169, 0.1338169], [0.1338169, 0.1338169], [0.1338169, 0.1338169]]]]]

Expected (Unparsed): [[[[[-0.1563456178809194,-0.1563456178809194],[-0.1563456178809194,-0.1563456178809194],[-0.1563456178809194,-0.1563456178809194]],[[-0.1563456178809194,-0.1563456178809194],[-0.1563456178809194,-0.1563456178809194],[-0.1563456178809194,-0.1563456178809194]],[[-0.1563456178809194,-0.1563456178809194],[-0.1563456178809194,-0.1563456178809194],[-0.1563456178809194,-0.1563456178809194]],[[-0.1563456178809194,-0.1563456178809194],[-0.1563456178809194,-0.1563456178809194],[-0.1563456178809194,-0.1563456178809194]]],[[[0.6718932951301898,0.6718932951301898],[0.6718932951301898,0.6718932951301898],[0.6718932951301898,0.6718932951301898]],[[0.6718932951301898,0.6718932951301898],[0.6718932951301898,0.6718932951301898],[0.6718932951301898,0.6718932951301898]],[[0.6718932951301898,0.6718932951301898],[0.6718932951301898,0.6718932951301898],[0.6718932951301898,0.6718932951301898]],[[0.6718932951301898,0.6718932951301898],[0.6718932951301898,0.6718932951301898],[0.6718932951301898,0.6718932951301898]]],[[[0.1338168520145101,0.1338168520145101],[0.1338168520145101,0.1338168520145101],[0.1338168520145101,0.1338168520145101]],[[0.1338168520145101,0.1338168520145101],[0.1338168520145101,0.1338168520145101],[0.1338168520145101,0.1338168520145101]],[[0.1338168520145101,0.1338168520145101],[0.1338168520145101,0.1338168520145101],[0.1338168520145101,0.1338168520145101]],[[0.1338168520145101,0.1338168520145101],[0.1338168520145101,0.1338168520145101],[0.1338168520145101,0.1338168520145101]]]]]

Actual:   [[[[[-0.1563, -0.1563], [-0.1563, -0.1563], [-0.1563, -0.1563]], [[-0.1563, -0.1563], [-0.1563, -0.1563], [-0.1563, -0.1563]], [[-0.1563, -0.1563], [-0.1563, -0.1563], [-0.1563, -0.1563]], [[-0.1563, -0.1563], [-0.1563, -0.1563], [-0.1563, -0.1563]]], [[[0.6719, 0.6719], [0.6719, 0.6719], [0.6719, 0.6719]], [[0.6719, 0.6719], [0.6719, 0.6719], [0.6719, 0.6719]], [[0.6719, 0.6719], [0.6719, 0.6719], [0.6719, 0.6719]], [[0.6719, 0.6719], [0.6719, 0.6719], [0.6719, 0.6719]]], [[[0.1339, 0.1339], [0.1339, 0.1339], [0.1339, 0.1339]], [[0.1339, 0.1339], [0.1339, 0.1339], [0.1339, 0.1339]], [[0.1339, 0.1339], [0.1339, 0.1339], [0.1339, 0.1339]], [[0.1339, 0.1339], [0.1339, 0.1339], [0.1339, 0.1339]]]]]

Expected: [[[[[-0.1563, -0.1563], [-0.1563, -0.1563], [-0.1563, -0.1563]], [[-0.1563, -0.1563], [-0.1563, -0.1563], [-0.1563, -0.1563]], [[-0.1563, -0.1563], [-0.1563, -0.1563], [-0.1563, -0.1563]], [[-0.1563, -0.1563], [-0.1563, -0.1563], [-0.1563, -0.1563]]], [[[0.6719, 0.6719], [0.6719, 0.6719], [0.6719, 0.6719]], [[0.6719, 0.6719], [0.6719, 0.6719], [0.6719, 0.6719]], [[0.6719, 0.6719], [0.6719, 0.6719], [0.6719, 0.6719]], [[0.6719, 0.6719], [0.6719, 0.6719], [0.6719, 0.6719]]], [[[0.1339, 0.1339], [0.1339, 0.1339], [0.1339, 0.1339]], [[0.1339, 0.1339], [0.1339, 0.1339], [0.1339, 0.1339]], [[0.1339, 0.1339], [0.1339, 0.1339], [0.1339, 0.1339]], [[0.1339, 0.1339], [0.1339, 0.1339], [0.1339, 0.1339]]]]]