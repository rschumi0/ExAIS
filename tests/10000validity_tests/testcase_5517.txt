import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Min95737 = tf.keras.layers.Input(shape=([2, 2, 2, 2]))
in1Min95737 = tf.keras.layers.Input(shape=([2, 2, 2, 2]))

Min95737 = keras.layers.Minimum(name = 'Min95737', )([in0Min95737,in1Min95737])
Bat36394 = keras.layers.BatchNormalization(axis=1, epsilon=0.11675976551242373,  name = 'Bat36394', )(Min95737)
Res32103 = keras.layers.Reshape((2, 2, 4), name = 'Res32103', )(Bat36394)
Res98409 = keras.layers.Reshape((2, 8), name = 'Res98409', )(Res32103)
Sim24301 = keras.layers.SimpleRNN(1,name = 'Sim24301', )(Res98409)
Mas71015 = keras.layers.Masking(mask_value=1, name = 'Mas71015', )(Sim24301)
model = tf.keras.models.Model(inputs=[in0Min95737,in1Min95737], outputs=Mas71015)
w = model.get_layer('Bat36394').get_weights() 
w[0] = np.array([0.2172, 0.2053])
w[1] = np.array([0.6666, 0.0559])
w[2] = np.array([0.7579, 0.3348])
w[3] = np.array([0.8776, 0.4086])
model.get_layer('Bat36394').set_weights(w) 
w = model.get_layer('Sim24301').get_weights() 
w[0] = np.array([[6], [9], [5], [8], [3], [9], [4], [9]])
w[1] = np.array([[3]])
w[2] = np.array([9])
model.get_layer('Sim24301').set_weights(w) 
in0Min95737 = tf.constant([[[[[0.8764, 0.4874], [0.8302, 0.8106]], [[0.6426, 0.5135], [0.3759, 0.2079]]], [[[0.484, 0.2696], [0.6145, 0.9992]], [[0.8641, 0.9174], [0.6256, 0.8092]]]]])
in1Min95737 = tf.constant([[[[[0.4918, 0.4781], [0.0444, 0.1727]], [[0.2624, 0.0333], [0.2818, 0.0769]]], [[[0.1071, 0.4514], [0.9427, 0.373]], [[0.1512, 0.1132], [0.55, 0.0714]]]]])
print (np.array2string(model.predict([in0Min95737,in1Min95737],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Mas71015.png')

LMin95737 = minimum_layer([[[[[[0.8764, 0.4874], [0.8302, 0.8106]], [[0.6426, 0.5135], [0.3759, 0.2079]]], [[[0.484, 0.2696], [0.6145, 0.9992]], [[0.8641, 0.9174], [0.6256, 0.8092]]]]], [[[[[0.4918, 0.4781], [0.0444, 0.1727]], [[0.2624, 0.0333], [0.2818, 0.0769]]], [[[0.1071, 0.4514], [0.9427, 0.373]], [[0.1512, 0.1132], [0.55, 0.0714]]]]]], Min95737), 
LBat36394 = batch_normalization_layer(Min95737, 1, 0.11675976551242373, [0.2172, 0.2053], [0.6666, 0.0559], [0.7579, 0.3348], [0.8776, 0.4086], Bat36394), 
LRes32103 = reshape_layer(Bat36394, [2, 2, 4], Res32103), 
LRes98409 = reshape_layer(Res32103, [2, 8], Res98409), 
LSim24301 = simple_rnn_layer(Res98409,[[6], [9], [5], [8], [3], [9], [4], [9]],[[3]],[9], Sim24301), 
LMas71015 = masking_layer(Sim24301, 1, Mas71015), 
exec_layers([LMin95737,LBat36394,LRes32103,LRes98409,LSim24301,LMas71015],["Min95737","Bat36394","Res32103","Res98409","Sim24301","Mas71015"],Mas71015,"Mas71015")

Actual (Unparsed): [[1.0000000]]

Expected (Unparsed): [[0.9999999999976908]]

Actual:   [[1]]

Expected: [[1]]