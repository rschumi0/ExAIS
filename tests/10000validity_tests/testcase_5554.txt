import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Add96866 = tf.keras.layers.Input(shape=([1, 2]))
in1Add96866 = tf.keras.layers.Input(shape=([1, 2]))
in0Con30497 = tf.keras.layers.Input(shape=([2, 4, 1, 1]))
in0ReL91622 = tf.keras.layers.Input(shape=([2, 2, 1, 2]))

Add96866 = keras.layers.Add(name = 'Add96866', )([in0Add96866,in1Add96866])
Fla57630 = keras.layers.Flatten(name = 'Fla57630', )(Add96866)
Res92677 = keras.layers.Reshape((2, 1), name = 'Res92677', )(Fla57630)
Loc19893 = keras.layers.LocallyConnected1D(4, (1),strides=(1), name = 'Loc19893', )(Res92677)
Res48303 = keras.layers.Reshape((2, 4, 1), name = 'Res48303', )(Loc19893)
Res34664 = keras.layers.Reshape((2, 4, 1, 1), name = 'Res34664', )(Res48303)
Con30497 = keras.layers.Concatenate(axis=4, name = 'Con30497', )([Res34664,in0Con30497])
ReL91622 = keras.layers.ReLU(max_value=3.2816167682299704, negative_slope=1.902350493575987, threshold=9.521320852797947, name = 'ReL91622', input_shape=(2, 2, 1, 2))(in0ReL91622)
Zer76913 = keras.layers.ZeroPadding3D(padding=((0, 0), (2, 0), (0, 0)), name = 'Zer76913', )(ReL91622)
Max35518 = keras.layers.Maximum(name = 'Max35518', )([Con30497,Zer76913])
model = tf.keras.models.Model(inputs=[in0Add96866,in1Add96866,in0Con30497,in0ReL91622], outputs=Max35518)
w = model.get_layer('Loc19893').get_weights() 
w[0] = np.array([[[0.4844, 0.6642, 0.7692, 0.7854]], [[0.7968, 0.1751, 0.7595, 0.9819]]])
w[1] = np.array([[0, 0, 0, 0], [0, 0, 0, 0]])
model.get_layer('Loc19893').set_weights(w) 
in0Add96866 = tf.constant([[[0.0426, 0.1414]]])
in1Add96866 = tf.constant([[[0.7947, 0.5372]]])
in0Con30497 = tf.constant([[[[[0.1023]], [[0.3299]], [[0.0474]], [[0.3641]]], [[[0.8343]], [[0.4906]], [[0.0231]], [[0.3823]]]]])
in0ReL91622 = tf.constant([[[[[0.7275, 0.9822]], [[0.1229, 0.0636]]], [[[0.5578, 0.0545]], [[0.0235, 0.9044]]]]])
print (np.array2string(model.predict([in0Add96866,in1Add96866,in0Con30497,in0ReL91622],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Max35518.png')

LAdd96866 = add_layer([[[[0.0426, 0.1414]]], [[[0.7947, 0.5372]]]], Add96866), 
LFla57630 = flatten_layer(Add96866, Fla57630), 
LRes92677 = reshape_layer(Fla57630, [2, 1], Res92677), 
LLoc19893 = locally_connected1D_layer(Res92677, 1,[[[0.4844, 0.6642, 0.7692, 0.7854]], [[0.7968, 0.1751, 0.7595, 0.9819]]],[[0, 0, 0, 0], [0, 0, 0, 0]], 1, Loc19893), 
LRes48303 = reshape_layer(Loc19893, [2, 4, 1], Res48303), 
LRes34664 = reshape_layer(Res48303, [2, 4, 1, 1], Res34664), 
LCon30497 = concatenate_layer([Res34664,[[[[[0.1023]], [[0.3299]], [[0.0474]], [[0.3641]]], [[[0.8343]], [[0.4906]], [[0.0231]], [[0.3823]]]]]], 4, Con30497), 
LReL91622 = relu_layer([[[[[0.7275, 0.9822]], [[0.1229, 0.0636]]], [[[0.5578, 0.0545]], [[0.0235, 0.9044]]]]], 3.2816167682299704, 1.902350493575987, 9.521320852797947, ReL91622), 
LZer76913 = zero_padding3D_layer(ReL91622, 0, 0, 2, 0, 0, 0, Zer76913), 
LMax35518 = maximum_layer([Con30497,Zer76913], Max35518), 
exec_layers([LAdd96866,LFla57630,LRes92677,LLoc19893,LRes48303,LRes34664,LCon30497,LReL91622,LZer76913,LMax35518],["Add96866","Fla57630","Res92677","Loc19893","Res48303","Res34664","Con30497","ReL91622","Zer76913","Max35518"],Max35518,"Max35518")

Actual (Unparsed): [[[[[0.4055881, 0.1023000]], [[0.5561347, 0.3299000]], [[0.6440512, 0.0474000]], [[0.6576154, 0.3641000]]], [[[0.5407085, 0.8343000]], [[0.1188229, 0.4906000]], [[0.5153967, 0.0231000]], [[0.6663173, 0.3823000]]]]]

Expected (Unparsed): [[[[[0.40558811999999994,0.1023]],[[0.55613466,0.3299]],[[0.64405116,0.0474]],[[0.65761542,0.3641]]],[[[0.5407084799999999,0.8343]],[[0.11882286,0.4906]],[[0.5153966999999999,0.0231]],[[0.66631734,0.3823]]]]]

Actual:   [[[[[0.4056, 0.1023]], [[0.5562, 0.3299]], [[0.6441, 0.0474]], [[0.6577, 0.3641]]], [[[0.5408, 0.8343]], [[0.1189, 0.4906]], [[0.5154, 0.0231]], [[0.6664, 0.3823]]]]]

Expected: [[[[[0.4056, 0.1023]], [[0.5562, 0.3299]], [[0.6441, 0.0474]], [[0.6577, 0.3641]]], [[[0.5408, 0.8343]], [[0.1189, 0.4906]], [[0.5154, 0.0231]], [[0.6664, 0.3823]]]]]