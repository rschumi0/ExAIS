import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Lea74964 = tf.keras.layers.Input(shape=([2, 1, 2]))

Lea74964 = keras.layers.LeakyReLU(alpha=5.826683200915442, name = 'Lea74964', input_shape=(2, 1, 2))(in0Lea74964)
Bat52034 = keras.layers.BatchNormalization(axis=1, epsilon=0.7161776434886373,  name = 'Bat52034', )(Lea74964)
model = tf.keras.models.Model(inputs=[in0Lea74964], outputs=Bat52034)
w = model.get_layer('Bat52034').get_weights() 
w[0] = np.array([0.2245, 0.1808])
w[1] = np.array([0.9138, 0.0107])
w[2] = np.array([0.5943, 0.4842])
w[3] = np.array([0.8323, 0.3488])
model.get_layer('Bat52034').set_weights(w) 
in0Lea74964 = tf.constant([[[[0.1318, 0.3797]], [[0.5848, 0.0888]]]])
print (np.array2string(model.predict([in0Lea74964],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Bat52034.png')

LLea74964 = leaky_relu_layer([[[[0.1318, 0.3797]], [[0.5848, 0.0888]]]], 5.826683200915442, Lea74964), 
LBat52034 = batch_normalization_layer(Lea74964, 1, 0.7161776434886373, [0.2245, 0.1808], [0.9138, 0.0107], [0.5943, 0.4842], [0.8323, 0.3488], Bat52034), 
exec_layers([LLea74964,LBat52034],["Lea74964","Bat52034"],Bat52034,"Bat52034")

Actual (Unparsed): [[[[0.8303597, 0.8750837]], [[0.0283249, -0.0585731]]]]

Expected (Unparsed): [[[[0.8303597469210526,0.8750837225713683]],[[0.02832487970075174,-0.058573135523630596]]]]

Actual:   [[[[0.8304, 0.8751]], [[0.0284, -0.0585]]]]

Expected: [[[[0.8304, 0.8751]], [[0.0284, -0.0585]]]]