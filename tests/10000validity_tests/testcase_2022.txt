import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Bat650 = tf.keras.layers.Input(shape=([4, 4]))
in0Sim28271 = tf.keras.layers.Input(shape=([1, 3]))
in0Con37871 = tf.keras.layers.Input(shape=([15]))

Bat650 = keras.layers.BatchNormalization(axis=1, epsilon=0.8111433176076218,  name = 'Bat650', )(in0Bat650)
Fla24977 = keras.layers.Flatten(name = 'Fla24977', )(Bat650)
Sim28271 = keras.layers.SimpleRNN(1,name = 'Sim28271', )(in0Sim28271)
Con37871 = keras.layers.Concatenate(axis=1, name = 'Con37871', )([Sim28271,in0Con37871])
Ave85534 = keras.layers.Average(name = 'Ave85534', )([Fla24977,Con37871])
model = tf.keras.models.Model(inputs=[in0Bat650,in0Sim28271,in0Con37871], outputs=Ave85534)
w = model.get_layer('Bat650').get_weights() 
w[0] = np.array([0.2323, 0.9767, 0.93, 0.7424])
w[1] = np.array([0.3628, 0.9035, 0.0253, 0.6441])
w[2] = np.array([0.5626, 0.6205, 0.4487, 0.6874])
w[3] = np.array([0.3911, 0.9314, 0.4387, 0.3455])
model.get_layer('Bat650').set_weights(w) 
w = model.get_layer('Sim28271').get_weights() 
w[0] = np.array([[8], [4], [9]])
w[1] = np.array([[3]])
w[2] = np.array([7])
model.get_layer('Sim28271').set_weights(w) 
in0Bat650 = tf.constant([[[1.4955, 1.8043, 1.468, 1.5277], [1.1467, 1.152, 1.2307, 1.6386], [1.7693, 1.9349, 1.221, 1.1552], [1.5681, 1.6611, 1.8947, 1.9548]]])
in0Sim28271 = tf.constant([[[2, 1, 8]]])
in0Con37871 = tf.constant([[0.3168, 0.253, 0.1312, 0.6622, 0.0174, 0.0548, 0.0272, 0.556, 0.927, 0.5839, 0.8184, 0.1161, 0.0211, 0.0315, 0.7579]])
print (np.array2string(model.predict([in0Bat650,in0Sim28271,in0Con37871],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Ave85534.png')

LBat650 = batch_normalization_layer([[[1.4955, 1.8043, 1.468, 1.5277], [1.1467, 1.152, 1.2307, 1.6386], [1.7693, 1.9349, 1.221, 1.1552], [1.5681, 1.6611, 1.8947, 1.9548]]], 1, 0.8111433176076218, [0.2323, 0.9767, 0.93, 0.7424], [0.3628, 0.9035, 0.0253, 0.6441], [0.5626, 0.6205, 0.4487, 0.6874], [0.3911, 0.9314, 0.4387, 0.3455], Bat650), 
LFla24977 = flatten_layer(Bat650, Fla24977), 
LSim28271 = simple_rnn_layer([[[2, 1, 8]]],[[8], [4], [9]],[[3]],[7], Sim28271), 
LCon37871 = concatenate_layer([Sim28271,[[0.3168, 0.253, 0.1312, 0.6622, 0.0174, 0.0548, 0.0272, 0.556, 0.927, 0.5839, 0.8184, 0.1161, 0.0211, 0.0315, 0.7579]]], 1, Con37871), 
LAve85534 = average_layer([Fla24977,Con37871], Ave85534), 
exec_layers([LBat650,LFla24977,LSim28271,LCon37871,LAve85534],["Bat650","Fla24977","Sim28271","Con37871","Ave85534"],Ave85534,"Ave85534")

Actual (Unparsed): [[0.7802230, 0.4713345, 0.4038099, 0.3492340, 0.9775161, 0.6570768, 0.7048916, 0.8419929, 0.8399334, 1.0943122, 0.6258263, 0.7157079, 0.6840739, 0.6686728, 0.7545000, 1.1384435]]

Expected (Unparsed): [[0.780223019918423,0.4713345094144129,0.4038099177126596,0.3492339977738986,0.9775160624137171,0.6570767810203167,0.704891602593786,0.8419929459205728,0.8399333813145813,1.0943121697029614,0.625826378456195,0.7157078743743387,0.6840738592656058,0.6686728361154993,0.7544999435578129,1.1384434759091957]]

Actual:   [[0.7803, 0.4714, 0.4039, 0.3493, 0.9776, 0.6571, 0.7049, 0.842, 0.84, 1.0944, 0.6259, 0.7158, 0.6841, 0.6687, 0.7545, 1.1385]]

Expected: [[0.7803, 0.4714, 0.4039, 0.3493, 0.9776, 0.6571, 0.7049, 0.842, 0.84, 1.0944, 0.6259, 0.7158, 0.6841, 0.6687, 0.7545, 1.1385]]