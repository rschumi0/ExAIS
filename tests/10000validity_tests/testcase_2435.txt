import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Min10237 = tf.keras.layers.Input(shape=([1, 1, 1]))
in1Min10237 = tf.keras.layers.Input(shape=([1, 1, 1]))
in0Con47411 = tf.keras.layers.Input(shape=([4, 3]))
in0Bat12947 = tf.keras.layers.Input(shape=([4, 4]))
in0Lay45540 = tf.keras.layers.Input(shape=([3]))
in0Con25350 = tf.keras.layers.Input(shape=([29]))
in0Ave44730 = tf.keras.layers.Input(shape=([2, 1, 2, 1]))
in1Ave44730 = tf.keras.layers.Input(shape=([2, 1, 2, 1]))

Min10237 = keras.layers.Minimum(name = 'Min10237', )([in0Min10237,in1Min10237])
Res35496 = keras.layers.Reshape((1, 1), name = 'Res35496', )(Min10237)
Zer51477 = keras.layers.ZeroPadding1D(padding=((3, 0)), name = 'Zer51477', )(Res35496)
Con47411 = keras.layers.Concatenate(axis=2, name = 'Con47411', )([Zer51477,in0Con47411])
Bat12947 = keras.layers.BatchNormalization(axis=2, epsilon=0.8637641242153098,  name = 'Bat12947', )(in0Bat12947)
Sub29639 = keras.layers.Subtract(name = 'Sub29639', )([Con47411,Bat12947])
Up_63623 = keras.layers.UpSampling1D(size=(2), name = 'Up_63623', )(Sub29639)
Fla78833 = keras.layers.Flatten(name = 'Fla78833', )(Up_63623)
Lay45540 = keras.layers.LayerNormalization(axis=1, epsilon=1.4237765685624455, name = 'Lay45540', )(in0Lay45540)
Con25350 = keras.layers.Concatenate(axis=1, name = 'Con25350', )([Lay45540,in0Con25350])
Mul98430 = keras.layers.Multiply(name = 'Mul98430', )([Fla78833,Con25350])
Res42454 = keras.layers.Reshape((32, 1), name = 'Res42454', )(Mul98430)
Res77048 = keras.layers.Reshape((32, 1, 1), name = 'Res77048', )(Res42454)
Res5672 = keras.layers.Reshape((32, 1, 1, 1), name = 'Res5672', )(Res77048)
Zer58199 = keras.layers.ZeroPadding3D(padding=((0, 0), (0, 0), (2, 0)), name = 'Zer58199', )(Res5672)
Ave44730 = keras.layers.Average(name = 'Ave44730', )([in0Ave44730,in1Ave44730])
Zer81429 = keras.layers.ZeroPadding3D(padding=((30, 0), (0, 0), (1, 0)), name = 'Zer81429', )(Ave44730)
Min68380 = keras.layers.Minimum(name = 'Min68380', )([Zer58199,Zer81429])
model = tf.keras.models.Model(inputs=[in0Min10237,in1Min10237,in0Con47411,in0Bat12947,in0Lay45540,in0Con25350,in0Ave44730,in1Ave44730], outputs=Min68380)
w = model.get_layer('Bat12947').get_weights() 
w[0] = np.array([0.5678, 0.7119, 0.6092, 0.1175])
w[1] = np.array([0.0817, 0.4264, 0.1436, 0.29])
w[2] = np.array([0.6369, 0.0255, 0.2303, 0.7736])
w[3] = np.array([0.6143, 0.7102, 0.7305, 0.0344])
model.get_layer('Bat12947').set_weights(w) 
in0Min10237 = tf.constant([[[[0.1067]]]])
in1Min10237 = tf.constant([[[[0.8117]]]])
in0Con47411 = tf.constant([[[0.6224, 0.3932, 0.5671], [0.4117, 0.6767, 0.4749], [0.858, 0.1275, 0.1792], [0.0335, 0.0502, 0.0263]]])
in0Bat12947 = tf.constant([[[1.7848, 1.1547, 1.7681, 1.9416], [1.0398, 1.758, 1.2482, 1.5141], [1.5814, 1.4184, 1.315, 1.6642], [1.3704, 1.835, 1.8085, 1.4908]]])
in0Lay45540 = tf.constant([[1.8526, 1.8115, 1.4393]])
in0Con25350 = tf.constant([[0.1612, 0.2568, 0.4159, 0.1392, 0.3698, 0.5653, 0.7637, 0.0089, 0.9083, 0.4443, 0.4745, 0.9676, 0.0713, 0.3463, 0.2359, 0.8907, 0.2116, 0.2955, 0.8633, 0.8377, 0.1532, 0.6266, 0.3404, 0.3587, 0.0077, 0.2216, 0.8925, 0.5149, 0.4832]])
in0Ave44730 = tf.constant([[[[[0.6854], [0.6484]]], [[[0.5468], [0.595]]]]])
in1Ave44730 = tf.constant([[[[[0.2516], [0.4653]]], [[[0.6636], [0.5674]]]]])
print (np.array2string(model.predict([in0Min10237,in1Min10237,in0Con47411,in0Bat12947,in0Lay45540,in0Con25350,in0Ave44730,in1Ave44730],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Min68380.png')

LMin10237 = minimum_layer([[[[[0.1067]]]], [[[[0.8117]]]]], Min10237), 
LRes35496 = reshape_layer(Min10237, [1, 1], Res35496), 
LZer51477 = zero_padding1D_layer(Res35496, 3, 0, Zer51477), 
LCon47411 = concatenate_layer([Zer51477,[[[0.6224, 0.3932, 0.5671], [0.4117, 0.6767, 0.4749], [0.858, 0.1275, 0.1792], [0.0335, 0.0502, 0.0263]]]], 2, Con47411), 
LBat12947 = batch_normalization_layer([[[1.7848, 1.1547, 1.7681, 1.9416], [1.0398, 1.758, 1.2482, 1.5141], [1.5814, 1.4184, 1.315, 1.6642], [1.3704, 1.835, 1.8085, 1.4908]]], 2, 0.8637641242153098, [0.5678, 0.7119, 0.6092, 0.1175], [0.0817, 0.4264, 0.1436, 0.29], [0.6369, 0.0255, 0.2303, 0.7736], [0.6143, 0.7102, 0.7305, 0.0344], Bat12947), 
LSub29639 = subtract_layer(Con47411,Bat12947, Sub29639), 
LUp_63623 = up_sampling1D_layer(Sub29639, 2, Up_63623), 
LFla78833 = flatten_layer(Up_63623, Fla78833), 
LLay45540 = layer_normalization_layer([[1.8526, 1.8115, 1.4393]], 1, 1.4237765685624455, Lay45540), 
LCon25350 = concatenate_layer([Lay45540,[[0.1612, 0.2568, 0.4159, 0.1392, 0.3698, 0.5653, 0.7637, 0.0089, 0.9083, 0.4443, 0.4745, 0.9676, 0.0713, 0.3463, 0.2359, 0.8907, 0.2116, 0.2955, 0.8633, 0.8377, 0.1532, 0.6266, 0.3404, 0.3587, 0.0077, 0.2216, 0.8925, 0.5149, 0.4832]]], 1, Con25350), 
LMul98430 = multiply_layer([Fla78833,Con25350], Mul98430), 
LRes42454 = reshape_layer(Mul98430, [32, 1], Res42454), 
LRes77048 = reshape_layer(Res42454, [32, 1, 1], Res77048), 
LRes5672 = reshape_layer(Res77048, [32, 1, 1, 1], Res5672), 
LZer58199 = zero_padding3D_layer(Res5672, 0, 0, 0, 0, 2, 0, Zer58199), 
LAve44730 = average_layer([[[[[[0.6854], [0.6484]]], [[[0.5468], [0.595]]]]], [[[[[0.2516], [0.4653]]], [[[0.6636], [0.5674]]]]]], Ave44730), 
LZer81429 = zero_padding3D_layer(Ave44730, 30, 0, 0, 0, 1, 0, Zer81429), 
LMin68380 = minimum_layer([Zer58199,Zer81429], Min68380), 
exec_layers([LMin10237,LRes35496,LZer51477,LCon47411,LBat12947,LSub29639,LUp_63623,LFla78833,LLay45540,LCon25350,LMul98430,LRes42454,LRes77048,LRes5672,LZer58199,LAve44730,LZer81429,LMin68380],["Min10237","Res35496","Zer51477","Con47411","Bat12947","Sub29639","Up_63623","Fla78833","Lay45540","Con25350","Mul98430","Res42454","Res77048","Res5672","Zer58199","Ave44730","Zer81429","Min68380"],Min68380,"Min68380")

Actual (Unparsed): [[[[[0.0000000], [0.0000000], [-0.0774894]]], [[[0.0000000], [0.0000000], [-0.0406472]]], [[[0.0000000], [0.0000000], [0.0000000]]], [[[0.0000000], [0.0000000], [0.0000000]]], [[[0.0000000], [0.0000000], [-0.1586533]]], [[[0.0000000], [0.0000000], [-0.1849739]]], [[[0.0000000], [0.0000000], [-0.0685363]]], [[[0.0000000], [0.0000000], [0.0000000]]], [[[0.0000000], [0.0000000], [-0.1525565]]], [[[0.0000000], [0.0000000], [-0.7620149]]], [[[0.0000000], [0.0000000], [0.0000000]]], [[[0.0000000], [0.0000000], [0.0000000]]], [[[0.0000000], [0.0000000], [-0.1199024]]], [[[0.0000000], [0.0000000], [-0.4734530]]], [[[0.0000000], [0.0000000], [0.0000000]]], [[[0.0000000], [0.0000000], [0.0000000]]], [[[0.0000000], [0.0000000], [-0.1810505]]], [[[0.0000000], [0.0000000], [-0.0846386]]], [[[0.0000000], [0.0000000], [-0.4804851]]], [[[0.0000000], [0.0000000], [-0.0468099]]], [[[0.0000000], [0.0000000], [-0.1544915]]], [[[0.0000000], [0.0000000], [-0.3097436]]], [[[0.0000000], [0.0000000], [-0.4518944]]], [[[0.0000000], [0.0000000], [-0.0338907]]], [[[0.0000000], [0.0000000], [-0.1989891]]], [[[0.0000000], [0.0000000], [-0.4832613]]], [[[0.0000000], [0.0000000], [-0.3066350]]], [[[0.0000000], [0.0000000], [-0.0027152]]], [[[0.0000000], [0.0000000], [-0.0703734]]], [[[0.0000000], [0.0000000], [-1.2670702]]], [[[0.0000000], [0.0000000], [-0.4401627]]], [[[0.0000000], [0.0000000], [-0.1703861]]]]]

Expected (Unparsed): [[[[[0],[0],[-0.07748937888052779]]],[[[0],[0],[-0.040647191112103014]]],[[[0],[0],[0]]],[[[0],[0],[0]]],[[[0],[0],[-0.15865325866044]]],[[[0],[0],[-0.1849738769712217]]],[[[0],[0],[-0.06853630321334317]]],[[[0],[0],[0]]],[[[0],[0],[-0.15255644132394028]]],[[[0],[0],[-0.762014890758425]]],[[[0],[0],[0]]],[[[0],[0],[0]]],[[[0],[0],[-0.11990240028343652]]],[[[0],[0],[-0.4734530125243847]]],[[[0],[0],[0]]],[[[0],[0],[0]]],[[[0],[0],[-0.1810504428100062]]],[[[0],[0],[-0.08463859526931496]]],[[[0],[0],[-0.48048507710597976]]],[[[0],[0],[-0.04680988099546438]]],[[[0],[0],[-0.15449149826842865]]],[[[0],[0],[-0.30974353241203734]]],[[[0],[0],[-0.4518944078720997]]],[[[0],[0],[-0.03389070779066703]]],[[[0],[0],[-0.19898913501701662]]],[[[0],[0],[-0.48326129731203027]]],[[[0],[0],[-0.30663495037986355]]],[[[0],[0],[-0.0027151753267364216]]],[[[0],[0],[-0.07037343172641379]]],[[[0],[0],[-1.267070234579868]]],[[[0],[0],[-0.4401626315879335]]],[[[0],[0],[-0.17038606725701802]]]]]

Actual:   [[[[[0], [0], [-0.0774]]], [[[0], [0], [-0.0406]]], [[[0], [0], [0]]], [[[0], [0], [0]]], [[[0], [0], [-0.1586]]], [[[0], [0], [-0.1849]]], [[[0], [0], [-0.0685]]], [[[0], [0], [0]]], [[[0], [0], [-0.1525]]], [[[0], [0], [-0.762]]], [[[0], [0], [0]]], [[[0], [0], [0]]], [[[0], [0], [-0.1199]]], [[[0], [0], [-0.4734]]], [[[0], [0], [0]]], [[[0], [0], [0]]], [[[0], [0], [-0.181]]], [[[0], [0], [-0.0846]]], [[[0], [0], [-0.4804]]], [[[0], [0], [-0.0468]]], [[[0], [0], [-0.1544]]], [[[0], [0], [-0.3097]]], [[[0], [0], [-0.4518]]], [[[0], [0], [-0.0338]]], [[[0], [0], [-0.1989]]], [[[0], [0], [-0.4832]]], [[[0], [0], [-0.3066]]], [[[0], [0], [-0.0027]]], [[[0], [0], [-0.0703]]], [[[0], [0], [-1.267]]], [[[0], [0], [-0.4401]]], [[[0], [0], [-0.1703]]]]]

Expected: [[[[[0], [0], [-0.0774]]], [[[0], [0], [-0.0406]]], [[[0], [0], [0]]], [[[0], [0], [0]]], [[[0], [0], [-0.1586]]], [[[0], [0], [-0.1849]]], [[[0], [0], [-0.0685]]], [[[0], [0], [0]]], [[[0], [0], [-0.1525]]], [[[0], [0], [-0.762]]], [[[0], [0], [0]]], [[[0], [0], [0]]], [[[0], [0], [-0.1199]]], [[[0], [0], [-0.4734]]], [[[0], [0], [0]]], [[[0], [0], [0]]], [[[0], [0], [-0.181]]], [[[0], [0], [-0.0846]]], [[[0], [0], [-0.4804]]], [[[0], [0], [-0.0468]]], [[[0], [0], [-0.1544]]], [[[0], [0], [-0.3097]]], [[[0], [0], [-0.4518]]], [[[0], [0], [-0.0338]]], [[[0], [0], [-0.1989]]], [[[0], [0], [-0.4832]]], [[[0], [0], [-0.3066]]], [[[0], [0], [-0.0027]]], [[[0], [0], [-0.0703]]], [[[0], [0], [-1.267]]], [[[0], [0], [-0.4401]]], [[[0], [0], [-0.1703]]]]]