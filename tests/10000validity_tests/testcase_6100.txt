import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Bat59253 = tf.keras.layers.Input(shape=([4]))
in0Mul14486 = tf.keras.layers.Input(shape=([2, 1]))
in1Mul14486 = tf.keras.layers.Input(shape=([2, 1]))

Bat59253 = keras.layers.BatchNormalization(axis=1, epsilon=0.7982580275979518,  name = 'Bat59253', )(in0Bat59253)
Res62506 = keras.layers.Reshape((4, 1), name = 'Res62506', )(Bat59253)
Mul14486 = keras.layers.Multiply(name = 'Mul14486', )([in0Mul14486,in1Mul14486])
Zer73226 = keras.layers.ZeroPadding1D(padding=((2, 0)), name = 'Zer73226', )(Mul14486)
Mul86573 = keras.layers.Multiply(name = 'Mul86573', )([Res62506,Zer73226])
model = tf.keras.models.Model(inputs=[in0Bat59253,in0Mul14486,in1Mul14486], outputs=Mul86573)
w = model.get_layer('Bat59253').get_weights() 
w[0] = np.array([0.6566, 0.1539, 0.8985, 0.4353])
w[1] = np.array([0.9533, 0.119, 0.1356, 0.0175])
w[2] = np.array([0.7505, 0.6561, 0.8066, 0.5179])
w[3] = np.array([0.6952, 0.3816, 0.2735, 0.0439])
model.get_layer('Bat59253').set_weights(w) 
in0Bat59253 = tf.constant([[1.2195, 1.8867, 1.1787, 1.6501]])
in0Mul14486 = tf.constant([[[0.1477], [0.118]]])
in1Mul14486 = tf.constant([[[0.0774], [0.4843]]])
print (np.array2string(model.predict([in0Bat59253,in0Mul14486,in1Mul14486],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Mul86573.png')

LBat59253 = batch_normalization_layer([[1.2195, 1.8867, 1.1787, 1.6501]], 1, 0.7982580275979518, [0.6566, 0.1539, 0.8985, 0.4353], [0.9533, 0.119, 0.1356, 0.0175], [0.7505, 0.6561, 0.8066, 0.5179], [0.6952, 0.3816, 0.2735, 0.0439], Bat59253), 
LRes62506 = reshape_layer(Bat59253, [4, 1], Res62506), 
LMul14486 = multiply_layer([[[[0.1477], [0.118]]], [[[0.0774], [0.4843]]]], Mul14486), 
LZer73226 = zero_padding1D_layer(Mul14486, 2, 0, Zer73226), 
LMul86573 = multiply_layer([Res62506,Zer73226], Mul86573), 
exec_layers([LBat59253,LRes62506,LMul14486,LZer73226,LMul86573],["Bat59253","Res62506","Mul14486","Zer73226","Mul86573"],Mul86573,"Mul86573")

Actual (Unparsed): [[[0.0000000], [0.0000000], [0.0052421], [0.0316911]]]

Expected (Unparsed): [[[0.0],[0.0],[0.005242084188413105],[0.03169111189876867]]]

Actual:   [[[0], [0], [0.0053], [0.0317]]]

Expected: [[[0], [0], [0.0053], [0.0317]]]