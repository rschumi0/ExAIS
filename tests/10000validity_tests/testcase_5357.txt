import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Sim10494 = tf.keras.layers.Input(shape=([3, 2]))
in0Glo86418 = tf.keras.layers.Input(shape=([2, 1, 2, 2]))
in0PRe79569 = tf.keras.layers.Input(shape=([1, 1]))
in0Con44700 = tf.keras.layers.Input(shape=([2, 2]))
in0Sub45030 = tf.keras.layers.Input(shape=([2, 3]))
in1Sub45030 = tf.keras.layers.Input(shape=([2, 3]))

Sim10494 = keras.layers.SimpleRNN(3,name = 'Sim10494', )(in0Sim10494)
Res35718 = keras.layers.Reshape((3, 1), name = 'Res35718', )(Sim10494)
Glo50879 = keras.layers.GlobalMaxPool1D(name = 'Glo50879', )(Res35718)
Res9939 = keras.layers.Reshape((1, 1), name = 'Res9939', )(Glo50879)
Zer10181 = keras.layers.ZeroPadding1D(padding=((1, 0)), name = 'Zer10181', )(Res9939)
Glo86418 = keras.layers.GlobalAveragePooling3D(name = 'Glo86418', )(in0Glo86418)
Res42062 = keras.layers.Reshape((2, 1), name = 'Res42062', )(Glo86418)
PRe79569 = keras.layers.PReLU(name = 'PRe79569', input_shape=(1, 1))(in0PRe79569)
Zer42421 = keras.layers.ZeroPadding1D(padding=((1, 0)), name = 'Zer42421', )(PRe79569)
Mul24570 = keras.layers.Multiply(name = 'Mul24570', )([Res42062,Zer42421])
Ave42331 = keras.layers.Average(name = 'Ave42331', )([Zer10181,Mul24570])
Con44700 = keras.layers.Concatenate(axis=2, name = 'Con44700', )([Ave42331,in0Con44700])
Sub45030 = keras.layers.Subtract(name = 'Sub45030', )([in0Sub45030,in1Sub45030])
Add47809 = keras.layers.Add(name = 'Add47809', )([Con44700,Sub45030])
model = tf.keras.models.Model(inputs=[in0Sim10494,in0Glo86418,in0PRe79569,in0Con44700,in0Sub45030,in1Sub45030], outputs=Add47809)
w = model.get_layer('Sim10494').get_weights() 
w[0] = np.array([[9, 1, 5], [6, 10, 4]])
w[1] = np.array([[6, 9, 2], [10, 3, 8], [4, 8, 8]])
w[2] = np.array([10, 1, 10])
model.get_layer('Sim10494').set_weights(w) 
w = model.get_layer('PRe79569').get_weights() 
w[0] = np.array([[0.2413]])
model.get_layer('PRe79569').set_weights(w) 
in0Sim10494 = tf.constant([[[7, 9], [10, 3], [6, 6]]])
in0Glo86418 = tf.constant([[[[[1.7372, 1.0543], [1.552, 1.1975]]], [[[1.4419, 1.1762], [1.9184, 1.2052]]]]])
in0PRe79569 = tf.constant([[[0.1036]]])
in0Con44700 = tf.constant([[[0.6885, 0.2813], [0.0955, 0.4221]]])
in0Sub45030 = tf.constant([[[0.3004, 0.0747, 0.8231], [0.9485, 0.801, 0.9478]]])
in1Sub45030 = tf.constant([[[0.1691, 0.856, 0.865], [0.3839, 0.0536, 0.4988]]])
print (np.array2string(model.predict([in0Sim10494,in0Glo86418,in0PRe79569,in0Con44700,in0Sub45030,in1Sub45030],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Add47809.png')

LSim10494 = simple_rnn_layer([[[7, 9], [10, 3], [6, 6]]],[[9, 1, 5], [6, 10, 4]],[[6, 9, 2], [10, 3, 8], [4, 8, 8]],[10, 1, 10], Sim10494), 
LRes35718 = reshape_layer(Sim10494, [3, 1], Res35718), 
LGlo50879 = global_max_pool1D_layer(Res35718, Glo50879), 
LRes9939 = reshape_layer(Glo50879, [1, 1], Res9939), 
LZer10181 = zero_padding1D_layer(Res9939, 1, 0, Zer10181), 
LGlo86418 = global_average_pooling3D_layer([[[[[1.7372, 1.0543], [1.552, 1.1975]]], [[[1.4419, 1.1762], [1.9184, 1.2052]]]]], Glo86418), 
LRes42062 = reshape_layer(Glo86418, [2, 1], Res42062), 
LPRe79569 = prelu_layer([[[0.1036]]], [[0.2413]], PRe79569), 
LZer42421 = zero_padding1D_layer(PRe79569, 1, 0, Zer42421), 
LMul24570 = multiply_layer([Res42062,Zer42421], Mul24570), 
LAve42331 = average_layer([Zer10181,Mul24570], Ave42331), 
LCon44700 = concatenate_layer([Ave42331,[[[0.6885, 0.2813], [0.0955, 0.4221]]]], 2, Con44700), 
LSub45030 = subtract_layer([[[0.3004, 0.0747, 0.8231], [0.9485, 0.801, 0.9478]]], [[[0.1691, 0.856, 0.865], [0.3839, 0.0536, 0.4988]]], Sub45030), 
LAdd47809 = add_layer([Con44700,Sub45030], Add47809), 
exec_layers([LSim10494,LRes35718,LGlo50879,LRes9939,LZer10181,LGlo86418,LRes42062,LPRe79569,LZer42421,LMul24570,LAve42331,LCon44700,LSub45030,LAdd47809],["Sim10494","Res35718","Glo50879","Res9939","Zer10181","Glo86418","Res42062","PRe79569","Zer42421","Mul24570","Ave42331","Con44700","Sub45030","Add47809"],Add47809,"Add47809")

Actual (Unparsed): [[[0.1313000, -0.0928000, 0.2394000], [1.1245999, 0.8429000, 0.8711000]]]

Expected (Unparsed): [[[0.1313,-0.0928,0.23940000000000006],[1.12459994,0.8429000000000001,0.8711]]]

Actual:   [[[0.1313, -0.0928, 0.2394], [1.1246, 0.8429, 0.8711]]]

Expected: [[[0.1313, -0.0928, 0.2395], [1.1246, 0.843, 0.8711]]]