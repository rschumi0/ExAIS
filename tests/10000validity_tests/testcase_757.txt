import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Bat68752 = tf.keras.layers.Input(shape=([3]))

Bat68752 = keras.layers.BatchNormalization(axis=1, epsilon=0.22112363855229608,  name = 'Bat68752', )(in0Bat68752)
ELU44322 = keras.layers.ELU(alpha=-6.3653026056277495, name = 'ELU44322', )(Bat68752)
model = tf.keras.models.Model(inputs=[in0Bat68752], outputs=ELU44322)
w = model.get_layer('Bat68752').get_weights() 
w[0] = np.array([0.6166, 0.6932, 0.73])
w[1] = np.array([0.5569, 0.1215, 0.3069])
w[2] = np.array([0.3328, 0.4095, 0.2207])
w[3] = np.array([0.5911, 0.9913, 0.6764])
model.get_layer('Bat68752').set_weights(w) 
in0Bat68752 = tf.constant([[1.9732, 1.5991, 1.0003]])
print (np.array2string(model.predict([in0Bat68752],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='ELU44322.png')

LBat68752 = batch_normalization_layer([[1.9732, 1.5991, 1.0003]], 1, 0.22112363855229608, [0.6166, 0.6932, 0.73], [0.5569, 0.1215, 0.3069], [0.3328, 0.4095, 0.2207], [0.5911, 0.9913, 0.6764], Bat68752), 
LELU44322 = elu_layer(Bat68752, -6.3653026056277495, ELU44322), 
exec_layers([LBat68752,LELU44322],["Bat68752","ELU44322"],ELU44322,"ELU44322")

Actual (Unparsed): [[1.6792168, 0.8704146, 0.9076196]]

Expected (Unparsed): [[1.6792168120315694,0.8704146254735092,0.9076195178475659]]

Actual:   [[1.6793, 0.8705, 0.9077]]

Expected: [[1.6793, 0.8705, 0.9077]]