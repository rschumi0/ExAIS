import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Max33492 = tf.keras.layers.Input(shape=([1, 2]))
in0Con70495 = tf.keras.layers.Input(shape=([2, 1]))
in0Max73621 = tf.keras.layers.Input(shape=([2, 1]))
in1Max73621 = tf.keras.layers.Input(shape=([2, 1]))
in0Con73600 = tf.keras.layers.Input(shape=([3, 1]))
in0Ave26680 = tf.keras.layers.Input(shape=([1, 2]))
in1Ave26680 = tf.keras.layers.Input(shape=([1, 2]))

Max33492 = keras.layers.MaxPool1D(pool_size=(1), strides=(1), padding='valid', name = 'Max33492', )(in0Max33492)
Zer29871 = keras.layers.ZeroPadding1D(padding=((1, 0)), name = 'Zer29871', )(Max33492)
Con70495 = keras.layers.Concatenate(axis=2, name = 'Con70495', )([Zer29871,in0Con70495])
Max73621 = keras.layers.Maximum(name = 'Max73621', )([in0Max73621,in1Max73621])
Dot90747 = keras.layers.Dot(axes=(1, 1), name = 'Dot90747', )([Con70495,Max73621])
Lea27157 = keras.layers.LeakyReLU(alpha=3.8337752163568406, name = 'Lea27157', )(Dot90747)
Con73600 = keras.layers.Concatenate(axis=2, name = 'Con73600', )([Lea27157,in0Con73600])
Ave26680 = keras.layers.Average(name = 'Ave26680', )([in0Ave26680,in1Ave26680])
Zer39833 = keras.layers.ZeroPadding1D(padding=((2, 0)), name = 'Zer39833', )(Ave26680)
Add16848 = keras.layers.Add(name = 'Add16848', )([Con73600,Zer39833])
model = tf.keras.models.Model(inputs=[in0Max33492,in0Con70495,in0Max73621,in1Max73621,in0Con73600,in0Ave26680,in1Ave26680], outputs=Add16848)
in0Max33492 = tf.constant([[[1.8981, 1.0682]]])
in0Con70495 = tf.constant([[[0.9252], [0.724]]])
in0Max73621 = tf.constant([[[0.3536], [0.4597]]])
in1Max73621 = tf.constant([[[0.0514], [0.2936]]])
in0Con73600 = tf.constant([[[0.773], [0.5192], [0.8984]]])
in0Ave26680 = tf.constant([[[0.6934, 0.9932]]])
in1Ave26680 = tf.constant([[[0.2627, 0.0879]]])
print (np.array2string(model.predict([in0Max33492,in0Con70495,in0Max73621,in1Max73621,in0Con73600,in0Ave26680,in1Ave26680],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Add16848.png')

LMax33492 = max_pool1D_layer([[[1.8981, 1.0682]]], 1, 1, false, Max33492), 
LZer29871 = zero_padding1D_layer(Max33492, 1, 0, Zer29871), 
LCon70495 = concatenate_layer([Zer29871,[[[0.9252], [0.724]]]], 2, Con70495), 
LMax73621 = maximum_layer([[[[0.3536], [0.4597]]], [[[0.0514], [0.2936]]]], Max73621), 
LDot90747 = dot_layer(Con70495,Max73621, 1, 1, Dot90747), 
LLea27157 = leaky_relu_layer(Dot90747, 3.8337752163568406, Lea27157), 
LCon73600 = concatenate_layer([Lea27157,[[[0.773], [0.5192], [0.8984]]]], 2, Con73600), 
LAve26680 = average_layer([[[[0.6934, 0.9932]]], [[[0.2627, 0.0879]]]], Ave26680), 
LZer39833 = zero_padding1D_layer(Ave26680, 2, 0, Zer39833), 
LAdd16848 = add_layer([Con73600,Zer39833], Add16848), 
exec_layers([LMax33492,LZer29871,LCon70495,LMax73621,LDot90747,LLea27157,LCon73600,LAve26680,LZer39833,LAdd16848],["Max33492","Zer29871","Con70495","Max73621","Dot90747","Lea27157","Con73600","Ave26680","Zer39833","Add16848"],Add16848,"Add16848")

Actual (Unparsed): [[[0.8725566, 0.7730000], [0.4910515, 0.5192000], [1.1380235, 1.4389500]]]

Expected (Unparsed): [[[0.87255657,0.773],[0.49105154,0.5192],[1.13802352,1.43895]]]

Actual:   [[[0.8726, 0.773], [0.4911, 0.5192], [1.1381, 1.439]]]

Expected: [[[0.8726, 0.773], [0.4911, 0.5192], [1.1381, 1.439]]]