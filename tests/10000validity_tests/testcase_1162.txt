import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0LST82549 = tf.keras.layers.Input(shape=([3, 3]))
in0Add22506 = tf.keras.layers.Input(shape=([2, 2]))
in1Add22506 = tf.keras.layers.Input(shape=([2, 2]))
in0Con90627 = tf.keras.layers.Input(shape=([8, 1]))

LST82549 = keras.layers.LSTM(1,recurrent_activation='sigmoid', name = 'LST82549', )(in0LST82549)
Res29036 = keras.layers.Reshape((1, 1), name = 'Res29036', )(LST82549)
Res27804 = keras.layers.Reshape((1, 1, 1), name = 'Res27804', )(Res29036)
Res30652 = keras.layers.Reshape((1, 1, 1, 1), name = 'Res30652', )(Res27804)
Con34787 = keras.layers.Conv3D(3, (1, 1, 1),strides=(3, 1, 7), padding='same', dilation_rate=(1, 1, 1), name = 'Con34787', )(Res30652)
Res97275 = keras.layers.Reshape((1, 1, 3), name = 'Res97275', )(Con34787)
Res20625 = keras.layers.Reshape((1, 3), name = 'Res20625', )(Res97275)
Zer98656 = keras.layers.ZeroPadding1D(padding=((7, 0)), name = 'Zer98656', )(Res20625)
Bat60182 = keras.layers.BatchNormalization(axis=2, epsilon=0.9556479657113653,  name = 'Bat60182', )(Zer98656)
Add22506 = keras.layers.Add(name = 'Add22506', )([in0Add22506,in1Add22506])
Zer29000 = keras.layers.ZeroPadding1D(padding=((6, 0)), name = 'Zer29000', )(Add22506)
Con90627 = keras.layers.Concatenate(axis=2, name = 'Con90627', )([Zer29000,in0Con90627])
Min79900 = keras.layers.Minimum(name = 'Min79900', )([Bat60182,Con90627])
model = tf.keras.models.Model(inputs=[in0LST82549,in0Add22506,in1Add22506,in0Con90627], outputs=Min79900)
w = model.get_layer('LST82549').get_weights() 
w[0] = np.array([[4, 10, 10, 6], [4, 8, 7, 1], [5, 1, 5, 4]])
w[1] = np.array([[8, 9, 2, 8]])
w[2] = np.array([3, 6, 8, 5])
model.get_layer('LST82549').set_weights(w) 
w = model.get_layer('Con34787').get_weights() 
w[0] = np.array([[[[[0.4933, 0.5395, 0.4176]]]]])
w[1] = np.array([0, 0, 0])
model.get_layer('Con34787').set_weights(w) 
w = model.get_layer('Bat60182').get_weights() 
w[0] = np.array([0.9837, 0.2635, 0.8476])
w[1] = np.array([0.6745, 0.8631, 0.7585])
w[2] = np.array([0.7844, 0.782, 0.3906])
w[3] = np.array([0.0421, 0.2452, 0.91])
model.get_layer('Bat60182').set_weights(w) 
in0LST82549 = tf.constant([[[8, 7, 6], [7, 9, 2], [9, 4, 5]]])
in0Add22506 = tf.constant([[[0.359, 0.9071], [0.1076, 0.0899]]])
in1Add22506 = tf.constant([[[0.1684, 0.3145], [0.1902, 0.3663]]])
in0Con90627 = tf.constant([[[0.6026], [0.1933], [0.9548], [0.8551], [0.7878], [0.0151], [0.1191], [0.7203]]])
print (np.array2string(model.predict([in0LST82549,in0Add22506,in1Add22506,in0Con90627],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Min79900.png')

LLST82549 = lstm_layer([[[8, 7, 6], [7, 9, 2], [9, 4, 5]]],[[4, 10, 10, 6], [4, 8, 7, 1], [5, 1, 5, 4]],[[8, 9, 2, 8]],[3, 6, 8, 5], LST82549), 
LRes29036 = reshape_layer(LST82549, [1, 1], Res29036), 
LRes27804 = reshape_layer(Res29036, [1, 1, 1], Res27804), 
LRes30652 = reshape_layer(Res27804, [1, 1, 1, 1], Res30652), 
LCon34787 = conv3D_layer(Res30652, 1, 1, 1,[[[[[0.4933, 0.5395, 0.4176]]]]],[0, 0, 0], 3, 1, 7, true, 1, 1, 1, Con34787), 
LRes97275 = reshape_layer(Con34787, [1, 1, 3], Res97275), 
LRes20625 = reshape_layer(Res97275, [1, 3], Res20625), 
LZer98656 = zero_padding1D_layer(Res20625, 7, 0, Zer98656), 
LBat60182 = batch_normalization_layer(Zer98656, 2, 0.9556479657113653, [0.9837, 0.2635, 0.8476], [0.6745, 0.8631, 0.7585], [0.7844, 0.782, 0.3906], [0.0421, 0.2452, 0.91], Bat60182), 
LAdd22506 = add_layer([[[[0.359, 0.9071], [0.1076, 0.0899]]], [[[0.1684, 0.3145], [0.1902, 0.3663]]]], Add22506), 
LZer29000 = zero_padding1D_layer(Add22506, 6, 0, Zer29000), 
LCon90627 = concatenate_layer([Zer29000,[[[0.6026], [0.1933], [0.9548], [0.8551], [0.7878], [0.0151], [0.1191], [0.7203]]]], 2, Con90627), 
LMin79900 = minimum_layer([Bat60182,Con90627], Min79900), 
exec_layers([LLST82549,LRes29036,LRes27804,LRes30652,LCon34787,LRes97275,LRes20625,LZer98656,LBat60182,LAdd22506,LZer29000,LCon90627,LMin79900],["LST82549","Res29036","Res27804","Res30652","Con34787","Res97275","Res20625","Zer98656","Bat60182","Add22506","Zer29000","Con90627","Min79900"],Min79900,"Min79900")

Actual (Unparsed): [[[-0.0979846, 0.0000000, 0.5161136], [-0.0979846, 0.0000000, 0.1933000], [-0.0979846, 0.0000000, 0.5161136], [-0.0979846, 0.0000000, 0.5161136], [-0.0979846, 0.0000000, 0.5161136], [-0.0979846, 0.0000000, 0.0151000], [-0.0979846, 0.6750630, 0.1191000], [0.2978000, 0.4562000, 0.7203000]]]

Expected (Unparsed): [[[-0.09798460117911745,0,0.5161135502822021],[-0.09798460117911745,0,0.1933],[-0.09798460117911745,0,0.5161135502822021],[-0.09798460117911745,0,0.5161135502822021],[-0.09798460117911745,0,0.5161135502822021],[-0.09798460117911745,0,0.0151],[-0.09798460117911745,0.6750629802748331,0.1191],[0.2978,0.4562,0.7203]]]

Actual:   [[[-0.0979, 0, 0.5162], [-0.0979, 0, 0.1933], [-0.0979, 0, 0.5162], [-0.0979, 0, 0.5162], [-0.0979, 0, 0.5162], [-0.0979, 0, 0.0151], [-0.0979, 0.6751, 0.1191], [0.2978, 0.4562, 0.7203]]]

Expected: [[[-0.0979, 0, 0.5162], [-0.0979, 0, 0.1933], [-0.0979, 0, 0.5162], [-0.0979, 0, 0.5162], [-0.0979, 0, 0.5162], [-0.0979, 0, 0.0151], [-0.0979, 0.6751, 0.1191], [0.2978, 0.4562, 0.7203]]]