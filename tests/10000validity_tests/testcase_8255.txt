import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Sub33547 = tf.keras.layers.Input(shape=([3, 2, 3]))
in1Sub33547 = tf.keras.layers.Input(shape=([3, 2, 3]))

Sub33547 = keras.layers.Subtract(name = 'Sub33547', )([in0Sub33547,in1Sub33547])
Con81581 = keras.layers.Conv2DTranspose(3, (3, 1),strides=(12, 7), padding='same', name = 'Con81581', )(Sub33547)
Bat73605 = keras.layers.BatchNormalization(axis=2, epsilon=0.5269683073336539,  name = 'Bat73605', )(Con81581)
Res10412 = keras.layers.Reshape((36, 42), name = 'Res10412', )(Bat73605)
Glo54222 = keras.layers.GlobalAveragePooling1D(name = 'Glo54222', )(Res10412)
model = tf.keras.models.Model(inputs=[in0Sub33547,in1Sub33547], outputs=Glo54222)
w = model.get_layer('Con81581').get_weights() 
w[0] = np.array([[[[0.4384, 0.077, 0.1724], [0.25, 0.6139, 0.5786], [0.8263, 0.9876, 0.9052]]], [[[0.9641, 0.302, 0.492], [0.8483, 0.9815, 0.1611], [0.1156, 0.6542, 0.3612]]], [[[0.8795, 0.3414, 0.5047], [0.708, 0.0658, 0.6009], [0.1338, 0.1254, 0.6393]]]])
w[1] = np.array([0, 0, 0])
model.get_layer('Con81581').set_weights(w) 
w = model.get_layer('Bat73605').get_weights() 
w[0] = np.array([0.6476, 0.8594, 0.7162, 0.0724, 0.977, 0.9972, 0.6108, 0.4897, 0.8493, 0.5252, 0.1928, 0.6924, 0.0401, 0.5394])
w[1] = np.array([0.9156, 0.5379, 0.6957, 0.4998, 0.364, 0.0573, 0.4105, 0.2741, 0.7474, 0.8929, 0.1462, 0.0238, 0.5757, 0.948])
w[2] = np.array([0.2964, 0.9352, 0.1783, 0.6703, 0.9505, 0.7385, 0.1765, 0.1632, 0.813, 0.8084, 0.6249, 0.4915, 0.2947, 0.3919])
w[3] = np.array([0.3015, 0.1349, 0.0051, 0.1725, 0.5452, 0.5309, 0.2527, 0.9476, 0.1122, 0.0431, 0.1828, 0.6663, 0.7585, 0.4938])
model.get_layer('Bat73605').set_weights(w) 
in0Sub33547 = tf.constant([[[[0.2618, 0.3171, 0.908], [0.1983, 0.4594, 0.9874]], [[0.6753, 0.2145, 0.6361], [0.0644, 0.6763, 0.8225]], [[0.8673, 0.4219, 0.2546], [0.5875, 0.3504, 0.0312]]]])
in1Sub33547 = tf.constant([[[[0.5758, 0.9506, 0.3634], [0.7267, 0.8284, 0.7595]], [[0.3877, 0.2237, 0.4605], [0.6477, 0.6713, 0.41]], [[0.4665, 0.8987, 0.1444], [0.3905, 0.3251, 0.9691]]]])
print (np.array2string(model.predict([in0Sub33547,in1Sub33547],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Glo54222.png')

LSub33547 = subtract_layer([[[[0.2618, 0.3171, 0.908], [0.1983, 0.4594, 0.9874]], [[0.6753, 0.2145, 0.6361], [0.0644, 0.6763, 0.8225]], [[0.8673, 0.4219, 0.2546], [0.5875, 0.3504, 0.0312]]]], [[[[0.5758, 0.9506, 0.3634], [0.7267, 0.8284, 0.7595]], [[0.3877, 0.2237, 0.4605], [0.6477, 0.6713, 0.41]], [[0.4665, 0.8987, 0.1444], [0.3905, 0.3251, 0.9691]]]], Sub33547), 
LCon81581 = conv2D_transpose_layer(Sub33547, 3, 1,[[[[0.4384, 0.077, 0.1724], [0.25, 0.6139, 0.5786], [0.8263, 0.9876, 0.9052]]], [[[0.9641, 0.302, 0.492], [0.8483, 0.9815, 0.1611], [0.1156, 0.6542, 0.3612]]], [[[0.8795, 0.3414, 0.5047], [0.708, 0.0658, 0.6009], [0.1338, 0.1254, 0.6393]]]],[0, 0, 0], 12, 7, true, Con81581), 
LBat73605 = batch_normalization_layer(Con81581, 2, 0.5269683073336539, [0.6476, 0.8594, 0.7162, 0.0724, 0.977, 0.9972, 0.6108, 0.4897, 0.8493, 0.5252, 0.1928, 0.6924, 0.0401, 0.5394], [0.9156, 0.5379, 0.6957, 0.4998, 0.364, 0.0573, 0.4105, 0.2741, 0.7474, 0.8929, 0.1462, 0.0238, 0.5757, 0.948], [0.2964, 0.9352, 0.1783, 0.6703, 0.9505, 0.7385, 0.1765, 0.1632, 0.813, 0.8084, 0.6249, 0.4915, 0.2947, 0.3919], [0.3015, 0.1349, 0.0051, 0.1725, 0.5452, 0.5309, 0.2527, 0.9476, 0.1122, 0.0431, 0.1828, 0.6663, 0.7585, 0.4938], Bat73605), 
LRes10412 = reshape_layer(Bat73605, [36, 42], Res10412), 
LGlo54222 = global_average_pooling1D_layer(Res10412, Glo54222), 
exec_layers([LSub33547,LCon81581,LBat73605,LRes10412,LGlo54222],["Sub33547","Con81581","Bat73605","Res10412","Glo54222"],Glo54222,"Glo54222")

Actual (Unparsed): [[0.7248478, 0.7033269, 0.7048498, -0.4500024, -0.4500024, -0.4500024, 0.5206339, 0.5206339, 0.5206339, 0.4417738, 0.4417738, 0.4417738, -0.5328405, -0.5328405, -0.5328405, -0.6587065, -0.6587065, -0.6587065, 0.2884076, 0.2884076, 0.2884076, 0.1782742, 0.1790074, 0.1842081, -0.1162625, -0.1162625, -0.1162625, 0.3305748, 0.3305748, 0.3305748, 0.0031924, 0.0031924, 0.0031924, -0.2877384, -0.2877384, -0.2877384, 0.5652770, 0.5652770, 0.5652770, 0.7387706, 0.7387706, 0.7387706]]

Expected (Unparsed): [[0.7248477847014839,0.7033269058330429,0.7048497991081111,-0.4500024318606886,-0.4500024318606886,-0.4500024318606886,0.5206339373104956,0.5206339373104956,0.5206339373104956,0.4417738488674785,0.4417738488674785,0.4417738488674785,-0.5328404793391601,-0.5328404793391601,-0.5328404793391601,-0.658706502343087,-0.658706502343087,-0.658706502343087,0.2884075531173137,0.2884075531173137,0.2884075531173137,0.17827423852367918,0.17900743762792154,0.18420807734936917,-0.11626248067575329,-0.11626248067575329,-0.11626248067575329,0.3305747647419454,0.3305747647419454,0.3305747647419454,0.0031923557563452443,0.0031923557563452443,0.0031923557563452443,-0.28773835925067465,-0.28773835925067465,-0.28773835925067465,0.5652769739884618,0.5652769739884618,0.5652769739884618,0.7387706447495755,0.7387706447495755,0.7387706447495755]]

Actual:   [[0.7249, 0.7034, 0.7049, -0.45, -0.45, -0.45, 0.5207, 0.5207, 0.5207, 0.4418, 0.4418, 0.4418, -0.5328, -0.5328, -0.5328, -0.6587, -0.6587, -0.6587, 0.2885, 0.2885, 0.2885, 0.1783, 0.1791, 0.1843, -0.1162, -0.1162, -0.1162, 0.3306, 0.3306, 0.3306, 0.0032, 0.0032, 0.0032, -0.2877, -0.2877, -0.2877, 0.5653, 0.5653, 0.5653, 0.7388, 0.7388, 0.7388]]

Expected: [[0.7249, 0.7034, 0.7049, -0.45, -0.45, -0.45, 0.5207, 0.5207, 0.5207, 0.4418, 0.4418, 0.4418, -0.5328, -0.5328, -0.5328, -0.6587, -0.6587, -0.6587, 0.2885, 0.2885, 0.2885, 0.1783, 0.1791, 0.1843, -0.1162, -0.1162, -0.1162, 0.3306, 0.3306, 0.3306, 0.0032, 0.0032, 0.0032, -0.2877, -0.2877, -0.2877, 0.5653, 0.5653, 0.5653, 0.7388, 0.7388, 0.7388]]