import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Ave64961 = tf.keras.layers.Input(shape=([1, 1]))
in1Ave64961 = tf.keras.layers.Input(shape=([1, 1]))

Ave64961 = keras.layers.Average(name = 'Ave64961', )([in0Ave64961,in1Ave64961])
Den58444 = keras.layers.Dense(4,name = 'Den58444', )(Ave64961)
Sim83257 = keras.layers.SimpleRNN(3,name = 'Sim83257', )(Den58444)
Bat55552 = keras.layers.BatchNormalization(axis=1, epsilon=0.9533392665054806,  name = 'Bat55552', )(Sim83257)
ELU83007 = keras.layers.ELU(alpha=-9.962095097553302, name = 'ELU83007', )(Bat55552)
ReL42238 = keras.layers.ReLU(max_value=4.930105980149016, negative_slope=3.1576903077848724, threshold=9.184772975891892, name = 'ReL42238', )(ELU83007)
model = tf.keras.models.Model(inputs=[in0Ave64961,in1Ave64961], outputs=ReL42238)
w = model.get_layer('Den58444').get_weights() 
w[0] = np.array([[0.4708, 0.7879, 0.1688, 0.3207]])
w[1] = np.array([0.2017, 0.2229, 0.7881, 0.8113])
model.get_layer('Den58444').set_weights(w) 
w = model.get_layer('Sim83257').get_weights() 
w[0] = np.array([[5, 3, 10], [2, 7, 1], [4, 8, 3], [9, 1, 6]])
w[1] = np.array([[3, 6, 4], [1, 8, 1], [6, 10, 10]])
w[2] = np.array([2, 3, 6])
model.get_layer('Sim83257').set_weights(w) 
w = model.get_layer('Bat55552').get_weights() 
w[0] = np.array([0.2891, 0.6993, 0.5878])
w[1] = np.array([0.581, 0.1456, 0.7804])
w[2] = np.array([0.1114, 0.1159, 0.9783])
w[3] = np.array([0.9904, 0.0194, 0.6643])
model.get_layer('Bat55552').set_weights(w) 
in0Ave64961 = tf.constant([[[0.5906]]])
in1Ave64961 = tf.constant([[[0.4532]]])
print (np.array2string(model.predict([in0Ave64961,in1Ave64961],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='ReL42238.png')

LAve64961 = average_layer([[[[0.5906]]], [[[0.4532]]]], Ave64961), 
LDen58444 = dense_layer(Ave64961, [[0.4708, 0.7879, 0.1688, 0.3207]],[0.2017, 0.2229, 0.7881, 0.8113], Den58444), 
LSim83257 = simple_rnn_layer(Den58444,[[5, 3, 10], [2, 7, 1], [4, 8, 3], [9, 1, 6]],[[3, 6, 4], [1, 8, 1], [6, 10, 10]],[2, 3, 6], Sim83257), 
LBat55552 = batch_normalization_layer(Sim83257, 1, 0.9533392665054806, [0.2891, 0.6993, 0.5878], [0.581, 0.1456, 0.7804], [0.1114, 0.1159, 0.9783], [0.9904, 0.0194, 0.6643], Bat55552), 
LELU83007 = elu_layer(Bat55552, -9.962095097553302, ELU83007), 
LReL42238 = relu_layer(ELU83007, 4.930105980149016, 3.1576903077848724, 9.184772975891892, ReL42238), 
exec_layers([LAve64961,LDen58444,LSim83257,LBat55552,LELU83007,LReL42238],["Ave64961","Den58444","Sim83257","Bat55552","ELU83007","ReL42238"],ReL42238,"ReL42238")

Actual (Unparsed): [[-26.5862087, -26.5634968, -26.5067393]]

Expected (Unparsed): [[-26.58620871964415,-26.563496757763925,-26.506739280524215]]

Actual:   [[-26.5862, -26.5634, -26.5067]]

Expected: [[-26.5862, -26.5634, -26.5067]]