import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Min89165 = tf.keras.layers.Input(shape=([1, 1, 1]))
in1Min89165 = tf.keras.layers.Input(shape=([1, 1, 1]))
in0Bat43769 = tf.keras.layers.Input(shape=([4, 2, 1]))
in0Sub33635 = tf.keras.layers.Input(shape=([3, 2]))
in1Sub33635 = tf.keras.layers.Input(shape=([3, 2]))

Min89165 = keras.layers.Minimum(name = 'Min89165', )([in0Min89165,in1Min89165])
Lea67281 = keras.layers.LeakyReLU(alpha=3.0135425960399513, name = 'Lea67281', )(Min89165)
Zer14524 = keras.layers.ZeroPadding2D(padding=((3, 0), (1, 0)), name = 'Zer14524', )(Lea67281)
Bat43769 = keras.layers.BatchNormalization(axis=1, epsilon=0.9418228531528211,  name = 'Bat43769', )(in0Bat43769)
Sub11283 = keras.layers.Subtract(name = 'Sub11283', )([Zer14524,Bat43769])
Res63360 = keras.layers.Reshape((4, 2), name = 'Res63360', )(Sub11283)
Sub33635 = keras.layers.Subtract(name = 'Sub33635', )([in0Sub33635,in1Sub33635])
PRe85774 = keras.layers.PReLU(name = 'PRe85774', )(Sub33635)
Zer69633 = keras.layers.ZeroPadding1D(padding=((1, 0)), name = 'Zer69633', )(PRe85774)
Add281 = keras.layers.Add(name = 'Add281', )([Res63360,Zer69633])
model = tf.keras.models.Model(inputs=[in0Min89165,in1Min89165,in0Bat43769,in0Sub33635,in1Sub33635], outputs=Add281)
w = model.get_layer('Bat43769').get_weights() 
w[0] = np.array([0.4023, 0.9881, 0.9496, 0.0728])
w[1] = np.array([0.6853, 0.0197, 0.8164, 0.4099])
w[2] = np.array([0.2258, 0.4637, 0.2126, 0.4994])
w[3] = np.array([0.7444, 0.5329, 0.35, 0.9234])
model.get_layer('Bat43769').set_weights(w) 
w = model.get_layer('PRe85774').get_weights() 
w[0] = np.array([[0.5025, 0.1249], [0.6757, 0.8598], [0.1131, 0.4109]])
model.get_layer('PRe85774').set_weights(w) 
in0Min89165 = tf.constant([[[[0.2757]]]])
in1Min89165 = tf.constant([[[[0.2138]]]])
in0Bat43769 = tf.constant([[[[1.1489], [1.9189]], [[1.8771], [1.3523]], [[1.0044], [1.0795]], [[1.462], [1.8251]]]])
in0Sub33635 = tf.constant([[[0.7917, 0.3999], [0.3979, 0.3418], [0.6587, 0.6931]]])
in1Sub33635 = tf.constant([[[0.6794, 0.2681], [0.0862, 0.7325], [0.1716, 0.2734]]])
print (np.array2string(model.predict([in0Min89165,in1Min89165,in0Bat43769,in0Sub33635,in1Sub33635],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Add281.png')

LMin89165 = minimum_layer([[[[[0.2757]]]], [[[[0.2138]]]]], Min89165), 
LLea67281 = leaky_relu_layer(Min89165, 3.0135425960399513, Lea67281), 
LZer14524 = zero_padding2D_layer(Lea67281, 3, 0, 1, 0, Zer14524), 
LBat43769 = batch_normalization_layer([[[[1.1489], [1.9189]], [[1.8771], [1.3523]], [[1.0044], [1.0795]], [[1.462], [1.8251]]]], 1, 0.9418228531528211, [0.4023, 0.9881, 0.9496, 0.0728], [0.6853, 0.0197, 0.8164, 0.4099], [0.2258, 0.4637, 0.2126, 0.4994], [0.7444, 0.5329, 0.35, 0.9234], Bat43769), 
LSub11283 = subtract_layer(Zer14524,Bat43769, Sub11283), 
LRes63360 = reshape_layer(Sub11283, [4, 2], Res63360), 
LSub33635 = subtract_layer([[[0.7917, 0.3999], [0.3979, 0.3418], [0.6587, 0.6931]]], [[[0.6794, 0.2681], [0.0862, 0.7325], [0.1716, 0.2734]]], Sub33635), 
LPRe85774 = prelu_layer(Sub33635, [[0.5025, 0.1249], [0.6757, 0.8598], [0.1131, 0.4109]], PRe85774), 
LZer69633 = zero_padding1D_layer(PRe85774, 1, 0, Zer69633), 
LAdd281 = add_layer([Res63360,Zer69633], Add281), 
exec_layers([LMin89165,LLea67281,LZer14524,LBat43769,LSub11283,LRes63360,LSub33635,LPRe85774,LZer69633,LAdd281],["Min89165","Lea67281","Zer14524","Bat43769","Sub11283","Res63360","Sub33635","PRe85774","Zer69633","Add281"],Add281,"Add281")

Actual (Unparsed): [[[-0.9712837, -1.2098358], [-1.0574343, -0.6109229], [-1.1662379, -1.8766067], [0.0258888, 0.1529338]]]

Expected (Unparsed): [[[-0.971283711453288,-1.2098358269543514],[-1.0574342672140369,-0.6109228172112589],[-1.1662378869970267,-1.8766067524447114],[0.025888835300911905,0.15293387591774255]]]

Actual:   [[[-0.9712, -1.2098], [-1.0574, -0.6109], [-1.1662, -1.8766], [0.0259, 0.153]]]

Expected: [[[-0.9712, -1.2098], [-1.0574, -0.6109], [-1.1662, -1.8766], [0.0259, 0.153]]]