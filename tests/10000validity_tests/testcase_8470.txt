import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Bat9678 = tf.keras.layers.Input(shape=([4, 2, 1]))

Bat9678 = keras.layers.BatchNormalization(axis=3, epsilon=0.10228546085401144,  name = 'Bat9678', )(in0Bat9678)
Lea1520 = keras.layers.LeakyReLU(alpha=3.351858475989384, name = 'Lea1520', )(Bat9678)
model = tf.keras.models.Model(inputs=[in0Bat9678], outputs=Lea1520)
w = model.get_layer('Bat9678').get_weights() 
w[0] = np.array([0.714])
w[1] = np.array([0.8544])
w[2] = np.array([0.0194])
w[3] = np.array([0.4165])
model.get_layer('Bat9678').set_weights(w) 
in0Bat9678 = tf.constant([[[[1.1575], [1.1707]], [[1.6618], [1.8942]], [[1.6999], [1.0694]], [[1.6528], [1.5149]]]])
print (np.array2string(model.predict([in0Bat9678],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Lea1520.png')

LBat9678 = batch_normalization_layer([[[[1.1575], [1.1707]], [[1.6618], [1.8942]], [[1.6999], [1.0694]], [[1.6528], [1.5149]]]], 3, 0.10228546085401144, [0.714], [0.8544], [0.0194], [0.4165], Bat9678), 
LLea1520 = leaky_relu_layer(Bat9678, 3.351858475989384, Lea1520), 
exec_layers([LBat9678,LLea1520],["Bat9678","Lea1520"],Lea1520,"Lea1520")

Actual (Unparsed): [[[[1.9825965], [1.9956816]], [[2.4825082], [2.7128858]], [[2.5202766], [1.8952631]], [[2.4735864], [2.3368864]]]]

Expected (Unparsed): [[[[1.9825964710972241],[1.9956816072174979]],[[2.482508148783131],[2.71288584835522]],[[2.5202766098575564],[1.8952631004763074]],[[2.4735864650647628],[2.3368864445355406]]]]

Actual:   [[[[1.9826], [1.9957]], [[2.4826], [2.7129]], [[2.5203], [1.8953]], [[2.4736], [2.3369]]]]

Expected: [[[[1.9826], [1.9957]], [[2.4826], [2.7129]], [[2.5203], [1.8953]], [[2.4736], [2.3369]]]]