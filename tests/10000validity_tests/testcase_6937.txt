import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Mul16736 = tf.keras.layers.Input(shape=([2, 1, 1, 2]))
in1Mul16736 = tf.keras.layers.Input(shape=([2, 1, 1, 2]))
in0Con17110 = tf.keras.layers.Input(shape=([3, 9, 2]))
in0Con55400 = tf.keras.layers.Input(shape=([1, 1]))
in0Con13535 = tf.keras.layers.Input(shape=([1, 12]))
in0Up_70226 = tf.keras.layers.Input(shape=([1, 4, 2]))
in0Con14356 = tf.keras.layers.Input(shape=([3, 9, 3]))
in0Con46493 = tf.keras.layers.Input(shape=([2, 1, 1]))

Mul16736 = keras.layers.Multiply(name = 'Mul16736', )([in0Mul16736,in1Mul16736])
Res79509 = keras.layers.Reshape((2, 1, 2), name = 'Res79509', )(Mul16736)
Zer3050 = keras.layers.ZeroPadding2D(padding=((1, 0), (8, 0)), name = 'Zer3050', )(Res79509)
Con17110 = keras.layers.Concatenate(axis=3, name = 'Con17110', )([Zer3050,in0Con17110])
Con55400 = keras.layers.Conv1D(4, (1),strides=(1), padding='same', dilation_rate=(1), name = 'Con55400', )(in0Con55400)
Con13535 = keras.layers.Concatenate(axis=2, name = 'Con13535', )([Con55400,in0Con13535])
Up_70226 = keras.layers.UpSampling2D(size=(2, 2), name = 'Up_70226', )(in0Up_70226)
Res67395 = keras.layers.Reshape((2, 16), name = 'Res67395', )(Up_70226)
Dot41289 = keras.layers.Dot(axes=(2, 2), name = 'Dot41289', )([Con13535,Res67395])
Res61495 = keras.layers.Reshape((1, 2, 1), name = 'Res61495', )(Dot41289)
Zer94233 = keras.layers.ZeroPadding2D(padding=((2, 0), (7, 0)), name = 'Zer94233', )(Res61495)
Con14356 = keras.layers.Concatenate(axis=3, name = 'Con14356', )([Zer94233,in0Con14356])
Con46493 = keras.layers.Conv2DTranspose(4, (2, 1),strides=(1, 9), padding='valid', name = 'Con46493', )(in0Con46493)
Bat35645 = keras.layers.BatchNormalization(axis=1, epsilon=0.589071161753352,  name = 'Bat35645', )(Con46493)
Add439 = keras.layers.Add(name = 'Add439', )([Con14356,Bat35645])
Max49604 = keras.layers.Maximum(name = 'Max49604', )([Con17110,Add439])
model = tf.keras.models.Model(inputs=[in0Mul16736,in1Mul16736,in0Con17110,in0Con55400,in0Con13535,in0Up_70226,in0Con14356,in0Con46493], outputs=Max49604)
w = model.get_layer('Con55400').get_weights() 
w[0] = np.array([[[0.023, 0.2794, 0.0401, 0.8298]]])
w[1] = np.array([0, 0, 0, 0])
model.get_layer('Con55400').set_weights(w) 
w = model.get_layer('Con46493').get_weights() 
w[0] = np.array([[[[0.2625], [0.7153], [0.9632], [0.0302]]], [[[0.8488], [0.4356], [0.6949], [0.9099]]]])
w[1] = np.array([0, 0, 0, 0])
model.get_layer('Con46493').set_weights(w) 
w = model.get_layer('Bat35645').get_weights() 
w[0] = np.array([0.5231, 0.6001, 0.4639])
w[1] = np.array([0.3197, 0.4611, 0.087])
w[2] = np.array([0.8215, 0.084, 0.2902])
w[3] = np.array([0.7745, 0.6873, 0.2544])
model.get_layer('Bat35645').set_weights(w) 
in0Mul16736 = tf.constant([[[[[0.2715, 0.5456]]], [[[0.0399, 0.442]]]]])
in1Mul16736 = tf.constant([[[[[0.9077, 0.8145]]], [[[0.7752, 0.2255]]]]])
in0Con17110 = tf.constant([[[[0.7783, 0.6029], [0.3921, 0.1838], [0.6717, 0.2166], [0.9482, 0.4236], [0.1507, 0.0302], [0.7761, 0.3448], [0.3726, 0.0417], [0.578, 0.3403], [0.1327, 0.6776]], [[0.7126, 0.3706], [0.2047, 0.9292], [0.9897, 0.0359], [0.4725, 0.0461], [0.1196, 0.1428], [0.4755, 0.8284], [0.4137, 0.2386], [0.377, 0.1802], [0.7345, 0.8465]], [[0.0404, 0.7331], [0.885, 0.4212], [0.8523, 0.033], [0.5953, 0.3709], [0.1663, 0.992], [0.0161, 0.8395], [0.7569, 0.0519], [0.1604, 0.3296], [0.4729, 0.7805]]]])
in0Con55400 = tf.constant([[[0.1857]]])
in0Con13535 = tf.constant([[[0.8734, 0.7599, 0.5857, 0.5319, 0.3311, 0.5957, 0.6405, 0.0048, 0.3017, 0.3791, 0.5723, 0.1275]]])
in0Up_70226 = tf.constant([[[[1.1044, 1.8197], [1.0147, 1.4396], [1.649, 1.4148], [1.6661, 1.1968]]]])
in0Con14356 = tf.constant([[[[0.6453, 0.3133, 0.1855], [0.4181, 0.7843, 0.3423], [0.3014, 0.6619, 0.4513], [0.3198, 0.4313, 0.4565], [0.544, 0.8562, 0.0009], [0.3439, 0.6341, 0.9742], [0.6697, 0.9308, 0.5867], [0.5432, 0.5929, 0.8433], [0.4847, 0.2484, 0.847]], [[0.6626, 0.309, 0.9495], [0.1629, 0.0002, 0.2245], [0.7167, 0.6543, 0.0792], [0.2519, 0.0557, 0.7977], [0.6716, 0.5671, 0.8319], [0.5238, 0.0086, 0.6646], [0.1878, 0.9155, 0.328], [0.1438, 0.3093, 0.669], [0.0542, 0.9338, 0.2429]], [[0.1746, 0.2136, 0.3596], [0.092, 0.7993, 0.5817], [0.5699, 0.8249, 0.7125], [0.2316, 0.4958, 0.6202], [0.386, 0.6048, 0.6926], [0.14, 0.7215, 0.9644], [0.1255, 0.3768, 0.61], [0.1044, 0.2059, 0.7485], [0.6132, 0.4883, 0.1585]]]])
in0Con46493 = tf.constant([[[[0.29]], [[0.9906]]]])
print (np.array2string(model.predict([in0Mul16736,in1Mul16736,in0Con17110,in0Con55400,in0Con13535,in0Up_70226,in0Con14356,in0Con46493],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Max49604.png')

LMul16736 = multiply_layer([[[[[[0.2715, 0.5456]]], [[[0.0399, 0.442]]]]], [[[[[0.9077, 0.8145]]], [[[0.7752, 0.2255]]]]]], Mul16736), 
LRes79509 = reshape_layer(Mul16736, [2, 1, 2], Res79509), 
LZer3050 = zero_padding2D_layer(Res79509, 1, 0, 8, 0, Zer3050), 
LCon17110 = concatenate_layer([Zer3050,[[[[0.7783, 0.6029], [0.3921, 0.1838], [0.6717, 0.2166], [0.9482, 0.4236], [0.1507, 0.0302], [0.7761, 0.3448], [0.3726, 0.0417], [0.578, 0.3403], [0.1327, 0.6776]], [[0.7126, 0.3706], [0.2047, 0.9292], [0.9897, 0.0359], [0.4725, 0.0461], [0.1196, 0.1428], [0.4755, 0.8284], [0.4137, 0.2386], [0.377, 0.1802], [0.7345, 0.8465]], [[0.0404, 0.7331], [0.885, 0.4212], [0.8523, 0.033], [0.5953, 0.3709], [0.1663, 0.992], [0.0161, 0.8395], [0.7569, 0.0519], [0.1604, 0.3296], [0.4729, 0.7805]]]]], 3, Con17110), 
LCon55400 = conv1D_layer([[[0.1857]]], 1,[[[0.023, 0.2794, 0.0401, 0.8298]]],[0, 0, 0, 0], 1, true, 1, Con55400), 
LCon13535 = concatenate_layer([Con55400,[[[0.8734, 0.7599, 0.5857, 0.5319, 0.3311, 0.5957, 0.6405, 0.0048, 0.3017, 0.3791, 0.5723, 0.1275]]]], 2, Con13535), 
LUp_70226 = up_sampling2D_layer([[[[1.1044, 1.8197], [1.0147, 1.4396], [1.649, 1.4148], [1.6661, 1.1968]]]], 2, 2, Up_70226), 
LRes67395 = reshape_layer(Up_70226, [2, 16], Res67395), 
LDot41289 = dot_layer(Con13535,Res67395, 2, 2, Dot41289), 
LRes61495 = reshape_layer(Dot41289, [1, 2, 1], Res61495), 
LZer94233 = zero_padding2D_layer(Res61495, 2, 0, 7, 0, Zer94233), 
LCon14356 = concatenate_layer([Zer94233,[[[[0.6453, 0.3133, 0.1855], [0.4181, 0.7843, 0.3423], [0.3014, 0.6619, 0.4513], [0.3198, 0.4313, 0.4565], [0.544, 0.8562, 0.0009], [0.3439, 0.6341, 0.9742], [0.6697, 0.9308, 0.5867], [0.5432, 0.5929, 0.8433], [0.4847, 0.2484, 0.847]], [[0.6626, 0.309, 0.9495], [0.1629, 0.0002, 0.2245], [0.7167, 0.6543, 0.0792], [0.2519, 0.0557, 0.7977], [0.6716, 0.5671, 0.8319], [0.5238, 0.0086, 0.6646], [0.1878, 0.9155, 0.328], [0.1438, 0.3093, 0.669], [0.0542, 0.9338, 0.2429]], [[0.1746, 0.2136, 0.3596], [0.092, 0.7993, 0.5817], [0.5699, 0.8249, 0.7125], [0.2316, 0.4958, 0.6202], [0.386, 0.6048, 0.6926], [0.14, 0.7215, 0.9644], [0.1255, 0.3768, 0.61], [0.1044, 0.2059, 0.7485], [0.6132, 0.4883, 0.1585]]]]], 3, Con14356), 
LCon46493 = conv2D_transpose_layer([[[[0.29]], [[0.9906]]]], 2, 1,[[[[0.2625], [0.7153], [0.9632], [0.0302]]], [[[0.8488], [0.4356], [0.6949], [0.9099]]]],[0, 0, 0, 0], 1, 9, false, Con46493), 
LBat35645 = batch_normalization_layer(Con46493, 1, 0.589071161753352, [0.5231, 0.6001, 0.4639], [0.3197, 0.4611, 0.087], [0.8215, 0.084, 0.2902], [0.7745, 0.6873, 0.2544], Bat35645), 
LAdd439 = add_layer([Con14356,Bat35645], Add439), 
LMax49604 = maximum_layer([Con17110,Add439], Max49604), 
exec_layers([LMul16736,LRes79509,LZer3050,LCon17110,LCon55400,LCon13535,LUp_70226,LRes67395,LDot41289,LRes61495,LZer94233,LCon14356,LCon46493,LBat35645,LAdd439,LMax49604],["Mul16736","Res79509","Zer3050","Con17110","Con55400","Con13535","Up_70226","Res67395","Dot41289","Res61495","Zer94233","Con14356","Con46493","Bat35645","Add439","Max49604"],Max49604,"Max49604")

Actual (Unparsed): [[[[0.0000000, 0.6899201, 0.7783000, 0.6029000], [0.0000000, 0.3697953, 0.7359953, 0.2939953], [0.0000000, 0.2530953, 0.6717000, 0.4029953], [0.0000000, 0.2714953, 0.9482000, 0.4236000], [0.0000000, 0.4956953, 0.8078953, 0.0302000], [0.0000000, 0.2955953, 0.7761000, 0.9258953], [0.0000000, 0.6213953, 0.8824953, 0.5383953], [0.0000000, 0.4948953, 0.5780000, 0.7949953], [0.0000000, 0.4363953, 0.2000953, 0.7986953]], [[0.6853526, 1.5225571, 1.3393394, 1.5220331], [0.4164816, 0.5793816, 0.4166816, 0.9292000], [0.4164816, 1.1331816, 1.0707815, 0.4956816], [0.4164816, 0.6683815, 0.4725000, 1.2141815], [0.4164816, 1.0880815, 0.9835815, 1.2483816], [0.4164816, 0.9402816, 0.4755000, 1.0810816], [0.4164816, 0.6042816, 1.3319815, 0.7444816], [0.4164816, 0.5602816, 0.7257816, 1.0854816], [0.4164816, 0.4706816, 1.3502815, 0.8465000]], [[0.3651264, 0.3329753, 0.5017201, 0.7552988], [0.0000000, 0.0324160, 0.8850000, 0.5221160], [0.0000000, 0.5103159, 0.8523000, 0.6529159], [0.0000000, 0.1720160, 0.5953000, 0.5606159], [0.0000000, 0.3264160, 0.5452160, 0.9920000], [0.0000000, 0.0804160, 0.6619159, 0.9048160], [0.0000000, 0.0659160, 0.7569000, 0.5504160], [8.1826262, 0.0448160, 0.1604000, 0.6889160], [8.1826262, 0.5536160, 0.4729000, 0.7805000]]]]

Expected (Unparsed): [[[[0,0.6899201706963654,0.7783,0.6029],[0,0.36979528424129793,0.7359952842412979,0.2939952842412979],[0,0.2530952842412979,0.6717,0.4029952842412979],[0,0.2714952842412979,0.9482,0.4236],[0,0.49569528424129794,0.8078952842412979,0.0302],[0,0.2955952842412979,0.7761,0.9258952842412979],[0,0.6213952842412979,0.8824952842412979,0.5383952842412979],[0,0.4948952842412979,0.578,0.794995284241298],[0,0.4363952842412979,0.2000952842412979,0.7986952842412979]],[[0.6853525653191657,1.522557114990302,1.3393394106787175,1.5220330342087869],[0.4164815559623579,0.5793815559623579,0.4166815559623579,0.9292],[0.4164815559623579,1.133181555962358,1.070781555962358,0.4956815559623579],[0.4164815559623579,0.6683815559623579,0.4725,1.2141815559623579],[0.4164815559623579,1.0880815559623578,0.983581555962358,1.248381555962358],[0.4164815559623579,0.940281555962358,0.4755,1.0810815559623579],[0.4164815559623579,0.6042815559623579,1.3319815559623578,0.7444815559623579],[0.4164815559623579,0.560281555962358,0.7257815559623579,1.085481555962358],[0.4164815559623579,0.47068155596235794,1.3502815559623578,0.8465]],[[0.36512641948414026,0.33297529052766983,0.5017201301656485,0.7552987669653608],[0,0.0324159687139271,0.885,0.5221159687139271],[0,0.510315968713927,0.8523,0.6529159687139271],[0,0.1720159687139271,0.5953,0.560615968713927],[0,0.3264159687139271,0.5452159687139271,0.992],[0,0.08041596871392712,0.6619159687139271,0.9048159687139271],[0,0.0659159687139271,0.7569,0.5504159687139271],[8.182626060729927,0.04481596871392711,0.1604,0.6889159687139271],[8.182626060729927,0.553615968713927,0.4729,0.7805]]]]

Actual:   [[[[0, 0.69, 0.7783, 0.6029], [0, 0.3698, 0.736, 0.294], [0, 0.2531, 0.6717, 0.403], [0, 0.2715, 0.9482, 0.4236], [0, 0.4957, 0.8079, 0.0302], [0, 0.2956, 0.7761, 0.9259], [0, 0.6214, 0.8825, 0.5384], [0, 0.4949, 0.578, 0.795], [0, 0.4364, 0.2001, 0.7987]], [[0.6854, 1.5226, 1.3394, 1.5221], [0.4165, 0.5794, 0.4167, 0.9292], [0.4165, 1.1332, 1.0708, 0.4957], [0.4165, 0.6684, 0.4725, 1.2142], [0.4165, 1.0881, 0.9836, 1.2484], [0.4165, 0.9403, 0.4755, 1.0811], [0.4165, 0.6043, 1.332, 0.7445], [0.4165, 0.5603, 0.7258, 1.0855], [0.4165, 0.4707, 1.3503, 0.8465]], [[0.3652, 0.333, 0.5018, 0.7553], [0, 0.0325, 0.885, 0.5222], [0, 0.5104, 0.8523, 0.653], [0, 0.1721, 0.5953, 0.5607], [0, 0.3265, 0.5453, 0.992], [0, 0.0805, 0.662, 0.9049], [0, 0.066, 0.7569, 0.5505], [8.1827, 0.0449, 0.1604, 0.689], [8.1827, 0.5537, 0.4729, 0.7805]]]]

Expected: [[[[0, 0.69, 0.7783, 0.6029], [0, 0.3698, 0.736, 0.294], [0, 0.2531, 0.6717, 0.403], [0, 0.2715, 0.9482, 0.4236], [0, 0.4957, 0.8079, 0.0302], [0, 0.2956, 0.7761, 0.9259], [0, 0.6214, 0.8825, 0.5384], [0, 0.4949, 0.578, 0.795], [0, 0.4364, 0.2001, 0.7987]], [[0.6854, 1.5226, 1.3394, 1.5221], [0.4165, 0.5794, 0.4167, 0.9292], [0.4165, 1.1332, 1.0708, 0.4957], [0.4165, 0.6684, 0.4725, 1.2142], [0.4165, 1.0881, 0.9836, 1.2484], [0.4165, 0.9403, 0.4755, 1.0811], [0.4165, 0.6043, 1.332, 0.7445], [0.4165, 0.5603, 0.7258, 1.0855], [0.4165, 0.4707, 1.3503, 0.8465]], [[0.3652, 0.333, 0.5018, 0.7553], [0, 0.0325, 0.885, 0.5222], [0, 0.5104, 0.8523, 0.653], [0, 0.1721, 0.5953, 0.5607], [0, 0.3265, 0.5453, 0.992], [0, 0.0805, 0.662, 0.9049], [0, 0.066, 0.7569, 0.5505], [8.1827, 0.0449, 0.1604, 0.689], [8.1827, 0.5537, 0.4729, 0.7805]]]]