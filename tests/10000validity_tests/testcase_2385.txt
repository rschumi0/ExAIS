import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Zer80752 = tf.keras.layers.Input(shape=([4, 3]))

Zer80752 = keras.layers.ZeroPadding1D(padding=((1, 1)), name = 'Zer80752', )(in0Zer80752)
Bat61752 = keras.layers.BatchNormalization(axis=1, epsilon=0.2591858904819735,  name = 'Bat61752', )(Zer80752)
model = tf.keras.models.Model(inputs=[in0Zer80752], outputs=Bat61752)
w = model.get_layer('Bat61752').get_weights() 
w[0] = np.array([0.3562, 0.375, 0.2379, 0.6293, 0.4296, 0.0861])
w[1] = np.array([0.1464, 0.5572, 0.4521, 0.2628, 0.2191, 0.8032])
w[2] = np.array([0.0929, 0.5552, 0.7131, 0.8627, 0.7348, 0.7044])
w[3] = np.array([0.3444, 0.7001, 0.2947, 0.5508, 0.8517, 0.1796])
model.get_layer('Bat61752').set_weights(w) 
in0Zer80752 = tf.constant([[[1.4935, 1.4988, 1.0577], [1.9651, 1.4845, 1.951], [1.6132, 1.6704, 1.9831], [1.6857, 1.0593, 1.322]]])
print (np.array2string(model.predict([in0Zer80752],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Bat61752.png')

LZer80752 = zero_padding1D_layer([[[1.4935, 1.4988, 1.0577], [1.9651, 1.4845, 1.951], [1.6132, 1.6704, 1.9831], [1.6857, 1.0593, 1.322]]], 1, 1, Zer80752), 
LBat61752 = batch_normalization_layer(Zer80752, 1, 0.2591858904819735, [0.3562, 0.375, 0.2379, 0.6293, 0.4296, 0.0861], [0.1464, 0.5572, 0.4521, 0.2628, 0.2191, 0.8032], [0.0929, 0.5552, 0.7131, 0.8627, 0.7348, 0.7044], [0.3444, 0.7001, 0.2947, 0.5508, 0.8517, 0.1796], Bat61752), 
exec_layers([LZer80752,LBat61752],["Zer80752","Bat61752"],Bat61752,"Bat61752")

Actual (Unparsed): [[[0.1038068, 0.1038068, 0.1038068], [0.9164518, 0.9184811, 0.7495948], [0.8523106, 0.6986835, 0.8478035], [0.7875708, 0.8275667, 1.0462154], [0.6066827, 0.3513648, 0.4584402], [0.7116420, 0.7116420, 0.7116420]]]

Expected (Unparsed): [[[0.10380681765240485,0.10380681765240485,0.10380681765240485],[0.9164518023718273,0.9184810409443209,0.7495947891845287],[0.8523106300502743,0.6986834504958318,0.8478034656064173],[0.787570848330857,0.8275667077905842,1.0462154010258393],[0.6066827097312757,0.3513647905224513,0.4584401694754497],[0.7116420264815644,0.7116420264815644,0.7116420264815644]]]

Actual:   [[[0.1039, 0.1039, 0.1039], [0.9165, 0.9185, 0.7496], [0.8524, 0.6987, 0.8479], [0.7876, 0.8276, 1.0463], [0.6067, 0.3514, 0.4585], [0.7117, 0.7117, 0.7117]]]

Expected: [[[0.1039, 0.1039, 0.1039], [0.9165, 0.9185, 0.7496], [0.8524, 0.6987, 0.8479], [0.7876, 0.8276, 1.0463], [0.6067, 0.3514, 0.4585], [0.7117, 0.7117, 0.7117]]]