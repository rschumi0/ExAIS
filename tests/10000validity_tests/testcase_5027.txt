import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Max41533 = tf.keras.layers.Input(shape=([2, 1, 2]))
in1Max41533 = tf.keras.layers.Input(shape=([2, 1, 2]))
in0Con25381 = tf.keras.layers.Input(shape=([4, 3, 4, 2]))
in0Con63286 = tf.keras.layers.Input(shape=([1, 2, 1, 1]))

Max41533 = keras.layers.Maximum(name = 'Max41533', )([in0Max41533,in1Max41533])
Res36197 = keras.layers.Reshape((2, 1, 2, 1), name = 'Res36197', )(Max41533)
Zer18200 = keras.layers.ZeroPadding3D(padding=((1, 1), (1, 1), (1, 1)), name = 'Zer18200', )(Res36197)
Con25381 = keras.layers.Concatenate(axis=4, name = 'Con25381', )([Zer18200,in0Con25381])
Con63286 = keras.layers.Conv3D(3, (1, 1, 1),strides=(1, 1, 1), padding='valid', dilation_rate=(1, 1, 1), name = 'Con63286', )(in0Con63286)
Zer15011 = keras.layers.ZeroPadding3D(padding=((3, 0), (1, 0), (3, 0)), name = 'Zer15011', )(Con63286)
Add37151 = keras.layers.Add(name = 'Add37151', )([Con25381,Zer15011])
Fla86566 = keras.layers.Flatten(name = 'Fla86566', )(Add37151)
model = tf.keras.models.Model(inputs=[in0Max41533,in1Max41533,in0Con25381,in0Con63286], outputs=Fla86566)
w = model.get_layer('Con63286').get_weights() 
w[0] = np.array([[[[[0.7063, 0.702, 0.4286]]]]])
w[1] = np.array([0, 0, 0])
model.get_layer('Con63286').set_weights(w) 
in0Max41533 = tf.constant([[[[0.0083, 0.9396]], [[0.1233, 0.9681]]]])
in1Max41533 = tf.constant([[[[0.1064, 0.5472]], [[0.1143, 0.3611]]]])
in0Con25381 = tf.constant([[[[[0.1239, 0.4324], [0.915, 0.8002], [0.5819, 0.7867], [0.9861, 0.5284]], [[0.9223, 0.5379], [0.9242, 0.9842], [0.982, 0.146], [0.6177, 0.3261]], [[0.8597, 0.5044], [0.6172, 0.9898], [0.7708, 0.9228], [0.7448, 0.6634]]], [[[0.1829, 0.5117], [0.6625, 0.3739], [0.7209, 0.5746], [0.1535, 0.071]], [[0.9254, 0.5569], [0.7274, 0.9318], [0.3262, 0.079], [0.3029, 0.9758]], [[0.4463, 0.5916], [0.1453, 0.4816], [0.2942, 0.2292], [0.9383, 0.1261]]], [[[0.6188, 0.5091], [0.8575, 0.7276], [0.7047, 0.6275], [0.7772, 0.9719]], [[0.7655, 0.2369], [0.2911, 0.5179], [0.0589, 0.7531], [0.0564, 0.4425]], [[0.689, 0.4387], [0.0823, 0.0646], [0.6905, 0.7356], [0.9723, 0.8122]]], [[[0.3941, 0.5784], [0.8065, 0.941], [0.2619, 0.7115], [0.3016, 0.5965]], [[0.3513, 0.3517], [0.1228, 0.9236], [0.3179, 0.8938], [0.737, 0.6686]], [[0.3968, 0.7377], [0.9968, 0.9949], [0.1201, 0.732], [0.7225, 0.0396]]]]])
in0Con63286 = tf.constant([[[[[0.6303]], [[0.9321]]]]])
print (np.array2string(model.predict([in0Max41533,in1Max41533,in0Con25381,in0Con63286],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Fla86566.png')

LMax41533 = maximum_layer([[[[[0.0083, 0.9396]], [[0.1233, 0.9681]]]], [[[[0.1064, 0.5472]], [[0.1143, 0.3611]]]]], Max41533), 
LRes36197 = reshape_layer(Max41533, [2, 1, 2, 1], Res36197), 
LZer18200 = zero_padding3D_layer(Res36197, 1, 1, 1, 1, 1, 1, Zer18200), 
LCon25381 = concatenate_layer([Zer18200,[[[[[0.1239, 0.4324], [0.915, 0.8002], [0.5819, 0.7867], [0.9861, 0.5284]], [[0.9223, 0.5379], [0.9242, 0.9842], [0.982, 0.146], [0.6177, 0.3261]], [[0.8597, 0.5044], [0.6172, 0.9898], [0.7708, 0.9228], [0.7448, 0.6634]]], [[[0.1829, 0.5117], [0.6625, 0.3739], [0.7209, 0.5746], [0.1535, 0.071]], [[0.9254, 0.5569], [0.7274, 0.9318], [0.3262, 0.079], [0.3029, 0.9758]], [[0.4463, 0.5916], [0.1453, 0.4816], [0.2942, 0.2292], [0.9383, 0.1261]]], [[[0.6188, 0.5091], [0.8575, 0.7276], [0.7047, 0.6275], [0.7772, 0.9719]], [[0.7655, 0.2369], [0.2911, 0.5179], [0.0589, 0.7531], [0.0564, 0.4425]], [[0.689, 0.4387], [0.0823, 0.0646], [0.6905, 0.7356], [0.9723, 0.8122]]], [[[0.3941, 0.5784], [0.8065, 0.941], [0.2619, 0.7115], [0.3016, 0.5965]], [[0.3513, 0.3517], [0.1228, 0.9236], [0.3179, 0.8938], [0.737, 0.6686]], [[0.3968, 0.7377], [0.9968, 0.9949], [0.1201, 0.732], [0.7225, 0.0396]]]]]], 4, Con25381), 
LCon63286 = conv3D_layer([[[[[0.6303]], [[0.9321]]]]], 1, 1, 1,[[[[[0.7063, 0.702, 0.4286]]]]],[0, 0, 0], 1, 1, 1, false, 1, 1, 1, Con63286), 
LZer15011 = zero_padding3D_layer(Con63286, 3, 0, 1, 0, 3, 0, Zer15011), 
LAdd37151 = add_layer([Con25381,Zer15011], Add37151), 
LFla86566 = flatten_layer(Add37151, Fla86566), 
exec_layers([LMax41533,LRes36197,LZer18200,LCon25381,LCon63286,LZer15011,LAdd37151,LFla86566],["Max41533","Res36197","Zer18200","Con25381","Con63286","Zer15011","Add37151","Fla86566"],Fla86566,"Fla86566")

Actual (Unparsed): [[0.0000000, 0.1239000, 0.4324000, 0.0000000, 0.9150000, 0.8002000, 0.0000000, 0.5819000, 0.7867000, 0.0000000, 0.9861000, 0.5284000, 0.0000000, 0.9223000, 0.5379000, 0.0000000, 0.9242000, 0.9842000, 0.0000000, 0.9820000, 0.1460000, 0.0000000, 0.6177000, 0.3261000, 0.0000000, 0.8597000, 0.5044000, 0.0000000, 0.6172000, 0.9898000, 0.0000000, 0.7708000, 0.9228000, 0.0000000, 0.7448000, 0.6634000, 0.0000000, 0.1829000, 0.5117000, 0.0000000, 0.6625000, 0.3739000, 0.0000000, 0.7209000, 0.5746000, 0.0000000, 0.1535000, 0.0710000, 0.0000000, 0.9254000, 0.5569000, 0.1064000, 0.7274000, 0.9318000, 0.9396000, 0.3262000, 0.0790000, 0.0000000, 0.3029000, 0.9758000, 0.0000000, 0.4463000, 0.5916000, 0.0000000, 0.1453000, 0.4816000, 0.0000000, 0.2942000, 0.2292000, 0.0000000, 0.9383000, 0.1261000, 0.0000000, 0.6188000, 0.5091000, 0.0000000, 0.8575000, 0.7276000, 0.0000000, 0.7047000, 0.6275000, 0.0000000, 0.7772000, 0.9719000, 0.0000000, 0.7655000, 0.2369000, 0.1233000, 0.2911000, 0.5179000, 0.9681000, 0.0589000, 0.7531000, 0.0000000, 0.0564000, 0.4425000, 0.0000000, 0.6890000, 0.4387000, 0.0000000, 0.0823000, 0.0646000, 0.0000000, 0.6905000, 0.7356000, 0.0000000, 0.9723000, 0.8122000, 0.0000000, 0.3941000, 0.5784000, 0.0000000, 0.8065000, 0.9410000, 0.0000000, 0.2619000, 0.7115000, 0.0000000, 0.3016000, 0.5965000, 0.0000000, 0.3513000, 0.3517000, 0.0000000, 0.1228000, 0.9236000, 0.0000000, 0.3179000, 0.8938000, 0.4451809, 1.1794706, 0.9387466, 0.0000000, 0.3968000, 0.7377000, 0.0000000, 0.9968000, 0.9949000, 0.0000000, 0.1201000, 0.7320000, 0.6583422, 1.3768342, 0.4390981]]

Expected (Unparsed): [[0,0.1239,0.4324,0,0.915,0.8002,0,0.5819,0.7867,0,0.9861,0.5284,0,0.9223,0.5379,0,0.9242,0.9842,0,0.982,0.146,0,0.6177,0.3261,0,0.8597,0.5044,0,0.6172,0.9898,0,0.7708,0.9228,0,0.7448,0.6634,0,0.1829,0.5117,0,0.6625,0.3739,0,0.7209,0.5746,0,0.1535,0.071,0,0.9254,0.5569,0.1064,0.7274,0.9318,0.9396,0.3262,0.079,0,0.3029,0.9758,0,0.4463,0.5916,0,0.1453,0.4816,0,0.2942,0.2292,0,0.9383,0.1261,0,0.6188,0.5091,0,0.8575,0.7276,0,0.7047,0.6275,0,0.7772,0.9719,0,0.7655,0.2369,0.1233,0.2911,0.5179,0.9681,0.0589,0.7531,0,0.0564,0.4425,0,0.689,0.4387,0,0.0823,0.0646,0,0.6905,0.7356,0,0.9723,0.8122,0,0.3941,0.5784,0,0.8065,0.941,0,0.2619,0.7115,0,0.3016,0.5965,0,0.3513,0.3517,0,0.1228,0.9236,0,0.3179,0.8938,0.44518089,1.1794706,0.9387465799999999,0,0.3968,0.7377,0,0.9968,0.9949,0,0.1201,0.732,0.65834223,1.3768342,0.43909806]]

Actual:   [[0, 0.1239, 0.4324, 0, 0.915, 0.8002, 0, 0.5819, 0.7867, 0, 0.9861, 0.5284, 0, 0.9223, 0.5379, 0, 0.9242, 0.9842, 0, 0.982, 0.146, 0, 0.6177, 0.3261, 0, 0.8597, 0.5044, 0, 0.6172, 0.9898, 0, 0.7708, 0.9228, 0, 0.7448, 0.6634, 0, 0.1829, 0.5117, 0, 0.6625, 0.3739, 0, 0.7209, 0.5746, 0, 0.1535, 0.071, 0, 0.9254, 0.5569, 0.1064, 0.7274, 0.9318, 0.9396, 0.3262, 0.079, 0, 0.3029, 0.9758, 0, 0.4463, 0.5916, 0, 0.1453, 0.4816, 0, 0.2942, 0.2292, 0, 0.9383, 0.1261, 0, 0.6188, 0.5091, 0, 0.8575, 0.7276, 0, 0.7047, 0.6275, 0, 0.7772, 0.9719, 0, 0.7655, 0.2369, 0.1233, 0.2911, 0.5179, 0.9681, 0.0589, 0.7531, 0, 0.0564, 0.4425, 0, 0.689, 0.4387, 0, 0.0823, 0.0646, 0, 0.6905, 0.7356, 0, 0.9723, 0.8122, 0, 0.3941, 0.5784, 0, 0.8065, 0.941, 0, 0.2619, 0.7115, 0, 0.3016, 0.5965, 0, 0.3513, 0.3517, 0, 0.1228, 0.9236, 0, 0.3179, 0.8938, 0.4452, 1.1795, 0.9388, 0, 0.3968, 0.7377, 0, 0.9968, 0.9949, 0, 0.1201, 0.732, 0.6584, 1.3769, 0.4391]]

Expected: [[0, 0.1239, 0.4324, 0, 0.915, 0.8002, 0, 0.5819, 0.7867, 0, 0.9861, 0.5284, 0, 0.9223, 0.5379, 0, 0.9242, 0.9842, 0, 0.982, 0.146, 0, 0.6177, 0.3261, 0, 0.8597, 0.5044, 0, 0.6172, 0.9898, 0, 0.7708, 0.9228, 0, 0.7448, 0.6634, 0, 0.1829, 0.5117, 0, 0.6625, 0.3739, 0, 0.7209, 0.5746, 0, 0.1535, 0.071, 0, 0.9254, 0.5569, 0.1064, 0.7274, 0.9318, 0.9396, 0.3262, 0.079, 0, 0.3029, 0.9758, 0, 0.4463, 0.5916, 0, 0.1453, 0.4816, 0, 0.2942, 0.2292, 0, 0.9383, 0.1261, 0, 0.6188, 0.5091, 0, 0.8575, 0.7276, 0, 0.7047, 0.6275, 0, 0.7772, 0.9719, 0, 0.7655, 0.2369, 0.1233, 0.2911, 0.5179, 0.9681, 0.0589, 0.7531, 0, 0.0564, 0.4425, 0, 0.689, 0.4387, 0, 0.0823, 0.0646, 0, 0.6905, 0.7356, 0, 0.9723, 0.8122, 0, 0.3941, 0.5784, 0, 0.8065, 0.941, 0, 0.2619, 0.7115, 0, 0.3016, 0.5965, 0, 0.3513, 0.3517, 0, 0.1228, 0.9236, 0, 0.3179, 0.8938, 0.4452, 1.1795, 0.9388, 0, 0.3968, 0.7377, 0, 0.9968, 0.9949, 0, 0.1201, 0.732, 0.6584, 1.3769, 0.4391]]