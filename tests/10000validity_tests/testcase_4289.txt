import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Bat83609 = tf.keras.layers.Input(shape=([4]))
in0Up_62106 = tf.keras.layers.Input(shape=([1, 1]))
in0Dot1423 = tf.keras.layers.Input(shape=([2]))
in1Dot1423 = tf.keras.layers.Input(shape=([2]))
in0Con97086 = tf.keras.layers.Input(shape=([3]))

Bat83609 = keras.layers.BatchNormalization(axis=1, epsilon=0.22566771766551025,  name = 'Bat83609', )(in0Bat83609)
Res88908 = keras.layers.Reshape((4, 1), name = 'Res88908', )(Bat83609)
Up_62106 = keras.layers.UpSampling1D(size=(1), name = 'Up_62106', )(in0Up_62106)
Zer75498 = keras.layers.ZeroPadding1D(padding=((3, 0)), name = 'Zer75498', )(Up_62106)
Add99656 = keras.layers.Add(name = 'Add99656', )([Res88908,Zer75498])
Fla78235 = keras.layers.Flatten(name = 'Fla78235', )(Add99656)
Dot1423 = keras.layers.Dot(axes=(1, 1), name = 'Dot1423', )([in0Dot1423,in1Dot1423])
Con97086 = keras.layers.Concatenate(axis=1, name = 'Con97086', )([Dot1423,in0Con97086])
Max97804 = keras.layers.Maximum(name = 'Max97804', )([Fla78235,Con97086])
model = tf.keras.models.Model(inputs=[in0Bat83609,in0Up_62106,in0Dot1423,in1Dot1423,in0Con97086], outputs=Max97804)
w = model.get_layer('Bat83609').get_weights() 
w[0] = np.array([0.4094, 0.4126, 0.0108, 0.0109])
w[1] = np.array([0.3516, 0.3755, 0.8221, 0.0894])
w[2] = np.array([0.1218, 0.8666, 0.5051, 0.3378])
w[3] = np.array([0.1167, 0.1069, 0.7952, 0.6474])
model.get_layer('Bat83609').set_weights(w) 
in0Bat83609 = tf.constant([[1.736, 1.7339, 1.784, 1.7687]])
in0Up_62106 = tf.constant([[[1.0583]]])
in0Dot1423 = tf.constant([[0.2326, 0.4673]])
in1Dot1423 = tf.constant([[0.9327, 0.18]])
in0Con97086 = tf.constant([[0.825, 0.418, 0.4372]])
print (np.array2string(model.predict([in0Bat83609,in0Up_62106,in0Dot1423,in1Dot1423,in0Con97086],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Max97804.png')

LBat83609 = batch_normalization_layer([[1.736, 1.7339, 1.784, 1.7687]], 1, 0.22566771766551025, [0.4094, 0.4126, 0.0108, 0.0109], [0.3516, 0.3755, 0.8221, 0.0894], [0.1218, 0.8666, 0.5051, 0.3378], [0.1167, 0.1069, 0.7952, 0.6474], Bat83609), 
LRes88908 = reshape_layer(Bat83609, [4, 1], Res88908), 
LUp_62106 = up_sampling1D_layer([[[1.0583]]], 1, Up_62106), 
LZer75498 = zero_padding1D_layer(Up_62106, 3, 0, Zer75498), 
LAdd99656 = add_layer([Res88908,Zer75498], Add99656), 
LFla78235 = flatten_layer(Add99656, Fla78235), 
LDot1423 = dot_layer([[0.2326, 0.4673]], [[0.9327, 0.18]], 1, 1, Dot1423), 
LCon97086 = concatenate_layer([Dot1423,[[0.825, 0.418, 0.4372]]], 1, Con97086), 
LMax97804 = maximum_layer([Fla78235,Con97086], Max97804), 
exec_layers([LBat83609,LRes88908,LUp_62106,LZer75498,LAdd99656,LFla78235,LDot1423,LCon97086,LMax97804],["Bat83609","Res88908","Up_62106","Zer75498","Add99656","Fla78235","Dot1423","Con97086","Max97804"],Max97804,"Max97804")

Actual (Unparsed): [[1.4810286, 0.9960239, 0.8357702, 1.1643922]]

Expected (Unparsed): [[1.4810285908244705,0.9960239166630009,0.8357702232633821,1.1643921322354276]]

Actual:   [[1.4811, 0.9961, 0.8358, 1.1644]]

Expected: [[1.4811, 0.9961, 0.8358, 1.1644]]