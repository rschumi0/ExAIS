import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Dot7066 = tf.keras.layers.Input(shape=([3, 2]))
in1Dot7066 = tf.keras.layers.Input(shape=([3, 2]))
in0Glo34704 = tf.keras.layers.Input(shape=([2, 1, 1, 1]))
in0Con78372 = tf.keras.layers.Input(shape=([1]))

Dot7066 = keras.layers.Dot(axes=(2, 2), name = 'Dot7066', )([in0Dot7066,in1Dot7066])
Res13340 = keras.layers.Reshape((3, 3, 1), name = 'Res13340', )(Dot7066)
Dep43696 = keras.layers.DepthwiseConv2D((1, 3),strides=(2, 2), padding='valid', name = 'Dep43696', )(Res13340)
Res45121 = keras.layers.Reshape((2, 1), name = 'Res45121', )(Dep43696)
Fla32736 = keras.layers.Flatten(name = 'Fla32736', )(Res45121)
Glo34704 = keras.layers.GlobalAveragePooling3D(name = 'Glo34704', )(in0Glo34704)
Bat56964 = keras.layers.BatchNormalization(axis=1, epsilon=0.9965491347144683,  name = 'Bat56964', )(Glo34704)
Con78372 = keras.layers.Concatenate(axis=1, name = 'Con78372', )([Bat56964,in0Con78372])
Add59603 = keras.layers.Add(name = 'Add59603', )([Fla32736,Con78372])
Res89321 = keras.layers.Reshape((2, 1), name = 'Res89321', )(Add59603)
Res83644 = keras.layers.Reshape((2, 1, 1), name = 'Res83644', )(Res89321)
Res25788 = keras.layers.Reshape((2, 1, 1, 1), name = 'Res25788', )(Res83644)
Zer65685 = keras.layers.ZeroPadding3D(padding=((1, 1), (1, 1), (1, 1)), name = 'Zer65685', )(Res25788)
model = tf.keras.models.Model(inputs=[in0Dot7066,in1Dot7066,in0Glo34704,in0Con78372], outputs=Zer65685)
w = model.get_layer('Dep43696').get_weights() 
w[0] = np.array([[[[0.8504]], [[0.8145]], [[0.5534]]]])
w[1] = np.array([0])
model.get_layer('Dep43696').set_weights(w) 
w = model.get_layer('Bat56964').get_weights() 
w[0] = np.array([0.2132])
w[1] = np.array([0.073])
w[2] = np.array([0.8382])
w[3] = np.array([0.7745])
model.get_layer('Bat56964').set_weights(w) 
in0Dot7066 = tf.constant([[[0.9348, 0.5194], [0.7065, 0.3611], [0.2379, 0.0782]]])
in1Dot7066 = tf.constant([[[0.6837, 0.0459], [0.6804, 0.0165], [0.4886, 0.918]]])
in0Glo34704 = tf.constant([[[[[1.9554]]], [[[1.2211]]]]])
in0Con78372 = tf.constant([[0.8686]])
print (np.array2string(model.predict([in0Dot7066,in1Dot7066,in0Glo34704,in0Con78372],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Zer65685.png')

LDot7066 = dot_layer([[[0.9348, 0.5194], [0.7065, 0.3611], [0.2379, 0.0782]]], [[[0.6837, 0.0459], [0.6804, 0.0165], [0.4886, 0.918]]], 2, 2, Dot7066), 
LRes13340 = reshape_layer(Dot7066, [3, 3, 1], Res13340), 
LDep43696 = depthwise_conv2D_layer(Res13340, 1, 3,[[[[0.8504]], [[0.8145]], [[0.5534]]]],[0], 2, 2, false, Dep43696), 
LRes45121 = reshape_layer(Dep43696, [2, 1], Res45121), 
LFla32736 = flatten_layer(Res45121, Fla32736), 
LGlo34704 = global_average_pooling3D_layer([[[[[1.9554]]], [[[1.2211]]]]], Glo34704), 
LBat56964 = batch_normalization_layer(Glo34704, 1, 0.9965491347144683, [0.2132], [0.073], [0.8382], [0.7745], Bat56964), 
LCon78372 = concatenate_layer([Bat56964,[[0.8686]]], 1, Con78372), 
LAdd59603 = add_layer([Fla32736,Con78372], Add59603), 
LRes89321 = reshape_layer(Add59603, [2, 1], Res89321), 
LRes83644 = reshape_layer(Res89321, [2, 1, 1], Res83644), 
LRes25788 = reshape_layer(Res83644, [2, 1, 1, 1], Res25788), 
LZer65685 = zero_padding3D_layer(Res25788, 1, 1, 1, 1, 1, 1, Zer65685), 
exec_layers([LDot7066,LRes13340,LDep43696,LRes45121,LFla32736,LGlo34704,LBat56964,LCon78372,LAdd59603,LRes89321,LRes83644,LRes25788,LZer65685],["Dot7066","Res13340","Dep43696","Res45121","Fla32736","Glo34704","Bat56964","Con78372","Add59603","Res89321","Res83644","Res25788","Zer65685"],Zer65685,"Zer65685")

Actual (Unparsed): [[[[[0.0000000], [0.0000000], [0.0000000]], [[0.0000000], [0.0000000], [0.0000000]], [[0.0000000], [0.0000000], [0.0000000]]], [[[0.0000000], [0.0000000], [0.0000000]], [[0.0000000], [1.7986058], [0.0000000]], [[0.0000000], [0.0000000], [0.0000000]]], [[[0.0000000], [0.0000000], [0.0000000]], [[0.0000000], [1.2469170], [0.0000000]], [[0.0000000], [0.0000000], [0.0000000]]], [[[0.0000000], [0.0000000], [0.0000000]], [[0.0000000], [0.0000000], [0.0000000]], [[0.0000000], [0.0000000], [0.0000000]]]]]

Expected (Unparsed): [[[[[0],[0],[0]],[[0],[0],[0]],[[0],[0],[0]]],[[[0],[0],[0]],[[0],[1.7986057032229548],[0]],[[0],[0],[0]]],[[[0],[0],[0]],[[0],[1.2469169501500001],[0]],[[0],[0],[0]]],[[[0],[0],[0]],[[0],[0],[0]],[[0],[0],[0]]]]]

Actual:   [[[[[0], [0], [0]], [[0], [0], [0]], [[0], [0], [0]]], [[[0], [0], [0]], [[0], [1.7987], [0]], [[0], [0], [0]]], [[[0], [0], [0]], [[0], [1.247], [0]], [[0], [0], [0]]], [[[0], [0], [0]], [[0], [0], [0]], [[0], [0], [0]]]]]

Expected: [[[[[0], [0], [0]], [[0], [0], [0]], [[0], [0], [0]]], [[[0], [0], [0]], [[0], [1.7987], [0]], [[0], [0], [0]]], [[[0], [0], [0]], [[0], [1.247], [0]], [[0], [0], [0]]], [[[0], [0], [0]], [[0], [0], [0]], [[0], [0], [0]]]]]