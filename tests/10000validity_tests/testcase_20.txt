import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Mul96399 = tf.keras.layers.Input(shape=([1, 2]))
in1Mul96399 = tf.keras.layers.Input(shape=([1, 2]))
in0Con30794 = tf.keras.layers.Input(shape=([2, 3, 1]))
in0Add74930 = tf.keras.layers.Input(shape=([2, 1, 2]))
in1Add74930 = tf.keras.layers.Input(shape=([2, 1, 2]))

Mul96399 = keras.layers.Multiply(name = 'Mul96399', )([in0Mul96399,in1Mul96399])
Res58677 = keras.layers.Reshape((1, 2, 1), name = 'Res58677', )(Mul96399)
Zer78532 = keras.layers.ZeroPadding2D(padding=((1, 0), (1, 0)), name = 'Zer78532', )(Res58677)
Con30794 = keras.layers.Concatenate(axis=3, name = 'Con30794', )([Zer78532,in0Con30794])
Add74930 = keras.layers.Add(name = 'Add74930', )([in0Add74930,in1Add74930])
Zer51519 = keras.layers.ZeroPadding2D(padding=((0, 0), (2, 0)), name = 'Zer51519', )(Add74930)
Mul64881 = keras.layers.Multiply(name = 'Mul64881', )([Con30794,Zer51519])
Res97237 = keras.layers.Reshape((2, 6), name = 'Res97237', )(Mul64881)
Fla1005 = keras.layers.Flatten(name = 'Fla1005', )(Res97237)
Res97818 = keras.layers.Reshape((12, 1), name = 'Res97818', )(Fla1005)
Res80666 = keras.layers.Reshape((12, 1, 1), name = 'Res80666', )(Res97818)
Zer87861 = keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name = 'Zer87861', )(Res80666)
Den29758 = keras.layers.Dense(4,name = 'Den29758', )(Zer87861)
model = tf.keras.models.Model(inputs=[in0Mul96399,in1Mul96399,in0Con30794,in0Add74930,in1Add74930], outputs=Den29758)
w = model.get_layer('Den29758').get_weights() 
w[0] = np.array([[0.4267, 0.8316, 0.3872, 0.1266]])
w[1] = np.array([0.3825, 0.3075, 0.166, 0.4338])
model.get_layer('Den29758').set_weights(w) 
in0Mul96399 = tf.constant([[[0.3604, 0.3763]]])
in1Mul96399 = tf.constant([[[0.9781, 0.9655]]])
in0Con30794 = tf.constant([[[[0.2017], [0.4788], [0.4629]], [[0.39], [0.9173], [0.6327]]]])
in0Add74930 = tf.constant([[[[0.1557, 0.6812]], [[0.0015, 0.027]]]])
in1Add74930 = tf.constant([[[[0.2184, 0.1381]], [[0.5933, 0.2178]]]])
print (np.array2string(model.predict([in0Mul96399,in1Mul96399,in0Con30794,in0Add74930,in1Add74930],steps=1), separator=', '))
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='Den29758.png')

LMul96399 = multiply_layer([[[[0.3604, 0.3763]]], [[[0.9781, 0.9655]]]], Mul96399), 
LRes58677 = reshape_layer(Mul96399, [1, 2, 1], Res58677), 
LZer78532 = zero_padding2D_layer(Res58677, 1, 0, 1, 0, Zer78532), 
LCon30794 = concatenate_layer([Zer78532,[[[[0.2017], [0.4788], [0.4629]], [[0.39], [0.9173], [0.6327]]]]], 3, Con30794), 
LAdd74930 = add_layer([[[[[0.1557, 0.6812]], [[0.0015, 0.027]]]], [[[[0.2184, 0.1381]], [[0.5933, 0.2178]]]]], Add74930), 
LZer51519 = zero_padding2D_layer(Add74930, 0, 0, 2, 0, Zer51519), 
LMul64881 = multiply_layer([Con30794,Zer51519], Mul64881), 
LRes97237 = reshape_layer(Mul64881, [2, 6], Res97237), 
LFla1005 = flatten_layer(Res97237, Fla1005), 
LRes97818 = reshape_layer(Fla1005, [12, 1], Res97818), 
LRes80666 = reshape_layer(Res97818, [12, 1, 1], Res80666), 
LZer87861 = zero_padding2D_layer(Res80666, 1, 1, 1, 1, Zer87861), 
LDen29758 = dense_layer(Zer87861, [[0.4267, 0.8316, 0.3872, 0.1266]],[0.3825, 0.3075, 0.166, 0.4338], Den29758), 
exec_layers([LMul96399,LRes58677,LZer78532,LCon30794,LAdd74930,LZer51519,LMul64881,LRes97237,LFla1005,LRes97818,LRes80666,LZer87861,LDen29758],["Mul96399","Res58677","Zer78532","Con30794","Add74930","Zer51519","Mul64881","Res97237","Fla1005","Res97818","Res80666","Zer87861","Den29758"],Den29758,"Den29758")

Actual (Unparsed): [[[[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.5443277, 0.6228876, 0.3128471, 0.4818136], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.4747104, 0.4872099, 0.2496744, 0.4611584], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.4485894, 0.4363023, 0.2259715, 0.4534084], [0.3825000, 0.3075000, 0.1660000, 0.4338000]], [[0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000], [0.3825000, 0.3075000, 0.1660000, 0.4338000]]]]

Expected (Unparsed): [[[[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.544327668999,0.622887601452,0.31284713718400003,0.481813552602],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.47471044101847404,0.487209872863752,0.249674438158784,0.46115842941865204],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.448589412432,0.436302332736,0.225971456512,0.453408435936],[0.3825,0.3075,0.166,0.4338]],[[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338],[0.3825,0.3075,0.166,0.4338]]]]

Actual:   [[[[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.5444, 0.6229, 0.3129, 0.4819], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.4748, 0.4873, 0.2497, 0.4612], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.4486, 0.4364, 0.226, 0.4535], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]]]]

Expected: [[[[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.5444, 0.6229, 0.3129, 0.4819], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.4748, 0.4873, 0.2497, 0.4612], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.4486, 0.4364, 0.226, 0.4535], [0.3825, 0.3075, 0.166, 0.4338]], [[0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338], [0.3825, 0.3075, 0.166, 0.4338]]]]