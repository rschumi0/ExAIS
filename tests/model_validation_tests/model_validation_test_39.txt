import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Con42696 = tf.keras.layers.Input(shape=([1, 2, 2]))
in0Max30754 = tf.keras.layers.Input(shape=([2, 1]))
in1Max30754 = tf.keras.layers.Input(shape=([2, 1]))
in0Add86390 = tf.keras.layers.Input(shape=([2, 1]))
in1Add86390 = tf.keras.layers.Input(shape=([2, 1]))

Con42696 = keras.layers.Conv2DTranspose(2, (1, 1),strides=(9, 1), padding='same', name = 'Con42696', )(in0Con42696)
Res51163 = keras.layers.Reshape((9, 4), name = 'Res51163', )(Con42696)
Zer17886 = keras.layers.ZeroPadding1D(padding=((1, 1)), name = 'Zer17886', )(Res51163)
Max30754 = keras.layers.Maximum(name = 'Max30754', )([in0Max30754,in1Max30754])
Zer51973 = keras.layers.ZeroPadding1D(padding=((9, 0)), name = 'Zer51973', )(Max30754)
Dot97703 = keras.layers.Dot(axes=(1, 1), name = 'Dot97703', )([Zer17886,Zer51973])
ReL161 = keras.layers.ReLU(max_value=9.753848674151294, negative_slope=2.3796791521013554, threshold=5.335851005564324, name = 'ReL161', )(Dot97703)
Add86390 = keras.layers.Add(name = 'Add86390', )([in0Add86390,in1Add86390])
Mul44886 = keras.layers.Multiply(name = 'Mul44886', )([ReL161,Add86390])
model = tf.keras.models.Model(inputs=[in0Con42696,in0Max30754,in1Max30754,in0Add86390,in1Add86390], outputs=Mul44886)
w = model.get_layer('Con42696').get_weights() 
w[0] = np.array([[[[0.1462, 0.1612], [0.4867, 0.7806]]]])
w[1] = np.array([0, 0])
model.get_layer('Con42696').set_weights(w) 
in0Con42696 = tf.constant([[[[0.0199, 0.0847], [0.8132, 0.3991]]]])
in0Max30754 = tf.constant([[[0.8931], [0.706]]])
in1Max30754 = tf.constant([[[0.3995], [0.4956]]])
in0Add86390 = tf.constant([[[0.4002], [0.2926]]])
in1Add86390 = tf.constant([[[0.3962], [0.2881]]])
print (np.array2string(model.predict([in0Con42696,in0Max30754,in1Max30754,in0Add86390,in1Add86390],steps=1), separator=', '))


LCon42696 = conv2D_transpose_layer([[[[0.0199, 0.0847], [0.8132, 0.3991]]]], 1, 1,[[[[0.1462, 0.1612], [0.4867, 0.7806]]]],[0, 0], 9, 1, true, Con42696), 
LRes51163 = reshape_layer(Con42696, [9, 4], Res51163), 
LZer17886 = zero_padding1D_layer(Res51163, 1, 1, Zer17886), 
LMax30754 = maximum_layer([[[[0.8931], [0.706]]], [[[0.3995], [0.4956]]]], Max30754), 
LZer51973 = zero_padding1D_layer(Max30754, 9, 0, Zer51973), 
LDot97703 = dot_layer(Zer17886,Zer51973, 1, 1, Dot97703), 
LReL161 = relu_layer(Dot97703, 9.753848674151294, 2.3796791521013554, 5.335851005564324, ReL161), 
LAdd86390 = add_layer([[[[0.4002], [0.2926]]], [[[0.3962], [0.2881]]]], Add86390), 
LMul44886 = multiply_layer([ReL161,Add86390], Mul44886), 
exec_layers([LCon42696,LRes51163,LZer17886,LMax30754,LZer51973,LDot97703,LReL161,LAdd86390,LMul44886],["Con42696","Res51163","Zer17886","Max30754","Zer51973","Dot97703","ReL161","Add86390","Mul44886"],Mul44886,"Mul44886")

Actual (Unparsed): 
 ValueError(ValueError: Operands could not be broadcast together with shapes (4, 1) (2, 1)

Expected (Unparsed): 
Mul44886: Inconsistent Input Shapes, Input Shape [1,4,1]

Actual:   

Expected: 