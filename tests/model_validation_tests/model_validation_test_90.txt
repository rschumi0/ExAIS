import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
np.set_printoptions(suppress=True,threshold=np.inf,formatter={'float_kind':'{:16.7f}'.format})
tf.keras.backend.set_floatx('float64')
in0Add88209 = tf.keras.layers.Input(shape=([2, 1, 2, 1]))
in1Add88209 = tf.keras.layers.Input(shape=([2, 1, 2, 1]))
in0Sub2925 = tf.keras.layers.Input(shape=([2, 2, 2]))
in1Sub2925 = tf.keras.layers.Input(shape=([2, 2, 2]))
in0Max92139 = tf.keras.layers.Input(shape=([1, 2, 2]))

Add88209 = keras.layers.Add(name = 'Add88209', )([in0Add88209,in1Add88209])
Res85182 = keras.layers.Reshape((2, 1, 2), name = 'Res85182', )(Add88209)
Res60075 = keras.layers.Reshape((2, 2), name = 'Res60075', )(Res85182)
Fla54864 = keras.layers.Flatten(name = 'Fla54864', )(Res60075)
Sub2925 = keras.layers.Subtract(name = 'Sub2925', )([in0Sub2925,in1Sub2925])
Max92139 = keras.layers.MaxPool2D(pool_size=(1, 2), name = 'Max92139', )(in0Max92139)
Zer36209 = keras.layers.ZeroPadding2D(padding=((1, 0), (1, 0)), name = 'Zer36209', )(Max92139)
Add27993 = keras.layers.Add(name = 'Add27993', )([Sub2925,Zer36209])
Glo23824 = keras.layers.GlobalAveragePooling2D(name = 'Glo23824', )(Add27993)
Min9640 = keras.layers.Minimum(name = 'Min9640', )([Fla54864,Glo23824])
model = tf.keras.models.Model(inputs=[in0Add88209,in1Add88209,in0Sub2925,in1Sub2925,in0Max92139], outputs=Min9640)
in0Add88209 = tf.constant([[[[[0.652], [0.9431]]], [[[0.8445], [0.4979]]]]])
in1Add88209 = tf.constant([[[[[0.1388], [0.6992]]], [[[0.1256], [0.4992]]]]])
in0Sub2925 = tf.constant([[[[0.7233, 0.6847], [0.279, 0.9981]], [[0.731, 0.5763], [0.1287, 0.8438]]]])
in1Sub2925 = tf.constant([[[[0.6196, 0.5542], [0.666, 0.782]], [[0.2019, 0.5095], [0.1572, 0.8451]]]])
in0Max92139 = tf.constant([[[[1.9917, 1.5709], [1.8492, 1.4924]]]])
print (np.array2string(model.predict([in0Add88209,in1Add88209,in0Sub2925,in1Sub2925,in0Max92139],steps=1), separator=', '))


LAdd88209 = add_layer([[[[[[0.652], [0.9431]]], [[[0.8445], [0.4979]]]]], [[[[[0.1388], [0.6992]]], [[[0.1256], [0.4992]]]]]], Add88209), 
LRes85182 = reshape_layer(Add88209, [2, 1, 2], Res85182), 
LRes60075 = reshape_layer(Res85182, [2, 2], Res60075), 
LFla54864 = flatten_layer(Res60075, Fla54864), 
LSub2925 = subtract_layer([[[[0.7233, 0.6847], [0.279, 0.9981]], [[0.731, 0.5763], [0.1287, 0.8438]]]], [[[[0.6196, 0.5542], [0.666, 0.782]], [[0.2019, 0.5095], [0.1572, 0.8451]]]], Sub2925), 
LMax92139 = max_pool2D_layer([[[[1.9917, 1.5709], [1.8492, 1.4924]]]], 1, 2, Max92139), 
LZer36209 = zero_padding2D_layer(Max92139, 1, 0, 1, 0, Zer36209), 
LAdd27993 = add_layer([Sub2925,Zer36209], Add27993), 
LGlo23824 = global_average_pooling2D_layer(Add27993, Glo23824), 
LMin9640 = minimum_layer([Fla54864,Glo23824], Min9640), 
exec_layers([LAdd88209,LRes85182,LRes60075,LFla54864,LSub2925,LMax92139,LZer36209,LAdd27993,LGlo23824,LMin9640],["Add88209","Res85182","Res60075","Fla54864","Sub2925","Max92139","Zer36209","Add27993","Glo23824","Min9640"],Min9640,"Min9640")

Actual (Unparsed): 
 ValueError(ValueError: Operands could not be broadcast together with shapes (4,) (2,)

Expected (Unparsed): 
Min9640: Inconsistent Input Shapes, Input Shape [1,4]

Actual:   

Expected: 